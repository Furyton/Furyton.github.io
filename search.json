[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi,\nIâ€™m an undergraduate student majoring in computer science\ninterested in machine learning, rec sys, NLP and fancy stuff.\ncontact me at furyton AT outlook DOT com ğŸ™ƒ"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html",
    "href": "posts/parallel-algorithm/pa-06.html",
    "title": "parallel algorithm course 06",
    "section": "",
    "text": "reiview\n\\(T_1(n)=O(n)\\)\n\\(T_\\infty(n)=O(log^k(n))\\)\nmodel: shared memory model"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#core-alg",
    "href": "posts/parallel-algorithm/pa-06.html#core-alg",
    "title": "parallel algorithm course 06",
    "section": "core alg",
    "text": "core alg\n\nmap \\(\\text{n}\\to \\text{n}\\)\nreduce \\(\\text{n} \\to 1\\)\nscan: compact (based on scan)\nlist ranking\nsort alg\nradix sort, sample sort"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#goal",
    "href": "posts/parallel-algorithm/pa-06.html#goal",
    "title": "parallel algorithm course 06",
    "section": "goal",
    "text": "goal\n\ncomplexity optimal\ndependency â€“\nsymetric â€“ (symetry: hard to find independency) using rand (find independent set) or odd-even"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#thinking",
    "href": "posts/parallel-algorithm/pa-06.html#thinking",
    "title": "parallel algorithm course 06",
    "section": "thinking",
    "text": "thinking\n\ncase to general\nforward-backward\nbit-wise thinking\nmatrix thinking"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#pattern",
    "href": "posts/parallel-algorithm/pa-06.html#pattern",
    "title": "parallel algorithm course 06",
    "section": "pattern",
    "text": "pattern\n\ndivide and conquer\nindependent set\nrandom\njump"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#openmp",
    "href": "posts/parallel-algorithm/pa-06.html#openmp",
    "title": "parallel algorithm course 06",
    "section": "OpenMP",
    "text": "OpenMP\n\npipeline: serial \\(\\to\\) independent \\(\\to\\) group \\(\\to\\) parallel\ndata racing, using syncronization\nPseudo random number generator"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-05.html",
    "href": "posts/parallel-algorithm/pa-05.html",
    "title": "parallel algorithm course 05",
    "section": "",
    "text": "omp task\narray pool\nuse array to simulate linked list\n\n\nfind prev using next\n\n\n\nfind index\nprev\njump\nrepeat:\npar-for i from 0 to n-1:\n    if NULL: continue\n    if prev[i] != empty:\n        rank[i] += rank[prev[i]] # initial all 1 except head is 0\n        prev[i] = prev[prev[i]] # use 2 copies to avoid data racing\n\\(T_1(n)=O(n\\cdot log(n))\\)\n\\(T_\\infty(n)=O(log(n))\\)\nWyllieâ€™s Alg: jump\n\n\n\nsimilar: binary lifting"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-05.html#improve-wyllies-alg",
    "href": "posts/parallel-algorithm/pa-05.html#improve-wyllies-alg",
    "title": "parallel algorithm course 05",
    "section": "improve Wyllieâ€™s Alg",
    "text": "improve Wyllieâ€™s Alg\n\\(O(n\\cdot log(n))\\rightarrow O(n)\\)\n\nstep 1: shrink \\(n\\rightarrow m\\)\nstep 2: call Wyllieâ€™s Alg \\(O(m\\cdot log(m))\\)\nstep 3: restore \\(m\\rightarrow n\\)\n\ntarget \\(m\\rightarrow \\frac{n}{log(n)}\\)\nindependent set \\(\\forall i \\in I,N(i) \\notin I\\)\nsymetry break\n\nrandom flip coin, keep or remove\nresolve confict\n\n# producing independent set\n\npar-for i=1..n\n    F[i] = RND(0 or 1)\npar-for i=1..n\n    if F[i]=F[N[i]]=1: # data racing. need 2 copies\n        F[i] = 0\n\\(T_1(n)=O(n)\\)\n\\(T_\\infty(n)=O(1)\\)\n\\(\\mathbb{E}|I|=\\frac{n}{4}\\)\n\\([n]/I\\rightarrow \\frac{3}{4}n\\) repeat \\(loglog(n)\\) times \\(\\frac{n}{log(n)}\\)\n\n\ninitialized with 1 except head\nrepeatedly remove an independent set \\(S\\) til #rest=\\(\\frac{n}{logn}\\) (change next[prev[i]] to next[i], add counter[i] to counter[next[i]] forall \\(i\\in S\\))\ncall Wyllieâ€™s Alg\nreverse\n\n\nquick sort: choose 1 pivot, sample 1\nv.s.\nsample sort: randomly choose \\(p-1\\) pivots, sample \\(k\\cdot p+1\\)\ncompact operator"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html",
    "href": "posts/parallel-algorithm/solution.html",
    "title": "midterm exam",
    "section": "",
    "text": "work complexity æ˜¯å½“å¤„ç†å™¨ä¸ªæ•°ä¸º 1 æ—¶ï¼Œå®Œæˆç®—æ³•æ‰€éœ€çš„åŸºç¡€æ“ä½œçš„ä¸ªæ•°ã€‚span complexity æ˜¯åœ¨æ•°æ®ä¾èµ– DAG å›¾ä¸­çš„æœ€é•¿è·¯å¾„çš„æ—¶é—´å¤æ‚åº¦ï¼Œå³å½“å¤„ç†å™¨ä¸ªæ•°ä¸º \\(\\infty\\) æ—¶ï¼ŒåŸºç¡€æ“ä½œçš„ä¸ªæ•°ã€‚\n\n\n\nBrentâ€™s Theorem:\n\\[\n\\max\\left\\{T_\\infty,\\frac{T_1}{p}\\right\\} \\le T_p \\le \\frac{T_1-T_\\infty}{p} + T_\\infty.\n\\]\nProof: å·¦ä¾§ä¸ç­‰å¼æ˜¾ç„¶ã€‚\nå¯¹äºå³ä¾§ä¸ç­‰å¼ï¼Œ\nå¯¹ DAG å›¾è¿›è¡Œåˆ†å±‚ï¼Œè®°ç¬¬ \\(i\\) å±‚çš„æ“ä½œæ•°ä¸º \\(m_i\\)ï¼Œå…± \\(n\\) å±‚ï¼Œåˆ™æœ‰ \\[\nT_1=\\sum_{i=1}^n m_i\n\\]\nå¯¹ç¬¬ \\(i\\) å±‚è¿›è¡Œå¹¶è¡Œï¼Œç”±äºè¿™ \\(m_i\\) ä¸ªæ“ä½œç‹¬ç«‹ï¼Œæ‰€ä»¥\n\\[\nT_p^i=\\left \\lceil \\frac{m_i}{p} \\right \\rceil \\le \\frac{m_i-1}{p} + 1.\n\\]\næ•…\n\\[\nT_p=\\sum_{i=1}^{n}T_p^i\\le \\sum_{i=1}^{n}\\left ( \\frac{m_i-1}{p}+1\\right)=\\frac{T_1-T_\\infty}{p} + T_\\infty\\blacksquare\n\\]\n\n\n\n\nå¯ç»“åˆæ€§å³ï¼Œ\\(\\forall a,b,c\\)ï¼Œæœ‰ \\(\\text{oper}(\\text{oper}(a,b),c)=\\text{oper}(a, \\text{oper}(b,c))\\)ã€‚\nç”±äºå¹¶è¡Œç¨‹åºåœ¨ scheduling æ—¶ï¼Œæ¯ä¸ª task çš„å¼€å§‹æ—¶é—´å’Œè¿è¡Œæ—¶é—´æˆ‘ä»¬æ— æ³•é¢„çŸ¥ï¼Œå¯¹äºä¸€ç³»åˆ—çš„æ“ä½œæ— æ³•ä¿è¯è¿ç®—çš„é¡ºåºï¼Œä»è€Œä¿è¯æ­£ç¡®æ€§ã€‚è‹¥æ“ä½œæ»¡è¶³å¯ç»“åˆæ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ merge ç­‰ç®—æ³•ä¿è¯æ“ä½œæ•°é¡ºåºçš„åŒæ—¶è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼›è‹¥æ“ä½œåˆæ»¡è¶³å¯äº¤æ¢æ€§ï¼Œåˆ™å¯ä»¥ç®€å•çš„ä½¿ç”¨ par-for ç­‰è¿›è¡Œå¹¶è¡Œï¼Œä¸”èƒ½ä¿è¯æ­£ç¡®æ€§ã€‚\n\n\n\n\ndifference\n\nscan æ“ä½œçš„å¯¹è±¡æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸€ä¸ªä½ç½®çš„ç­”æ¡ˆæ‰€å¯¹åº”çš„ä¾èµ–å…ƒç´ æ˜¯ predefinedã€‚list ranking æ“ä½œå¯¹è±¡æ˜¯ linked list æˆ– array poolï¼Œæ¯ä¸ªä½ç½®çš„ç­”æ¡ˆæ‰€ä¾èµ–çš„å…ƒç´ æ˜¯æœªçŸ¥çš„ï¼Œå¿…é¡»ç»è¿‡æŸç§ä¸²è¡Œçš„æŸ¥æ‰¾æ¥å¾—åˆ°å¯¹åº”çš„ä¾èµ–å…³ç³»ã€‚\nscan è®¡ç®—çš„æ˜¯ \\(a_i=\\bigoplus_{k=1}^i\\text{arr}_k\\)ï¼Œlist ranking è®¡ç®—çš„æ˜¯æ¯ä¸ª element çš„ rankingï¼Œå…¶ä¸­ \\(\\bigoplus\\) æ˜¯å¯ç»“åˆã€å¯äº¤æ¢çš„äºŒå…ƒè¿ç®—ã€‚\nlist ranking ä½¿ç”¨ jump çš„æ€æƒ³æ¥è§£å†³ï¼Œè€Œ scan æ²¡æœ‰ã€‚\n\nin common\n\nä¸¤ç§ç®—æ³•è§£å†³çš„é—®é¢˜éƒ½æ˜¯ \\(n\\to n\\)ã€‚\n\n\n\n\n\ndifference\n\nquicksort åªè¿›è¡Œä¸€æ¬¡é‡‡æ ·ï¼Œé€‰æ‹©ä¸€ä¸ª pivot elementï¼Œå°†æ•°ç»„åˆ†æˆä¸¤æ®µï¼Œè€Œ samplesort ä¼šè¿›è¡Œ \\(k\\cdot (p-1)\\) æ¬¡é‡‡æ ·ï¼Œé€‰å‡º p-1 ä¸ª pivot elementï¼Œå°†æ•°ç»„åˆ†æˆ p æ®µã€‚\n\nin common\n\néƒ½æ˜¯åŸºäº divide and conquer æ€æƒ³ï¼Œè§£å†³ sorting é—®é¢˜"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#openmp-1",
    "href": "posts/parallel-algorithm/solution.html#openmp-1",
    "title": "midterm exam",
    "section": "OpenMP 1",
    "text": "OpenMP 1\n\nQ6 False sharing and how to avoid\n\nç”±äº cache çš„å­˜åœ¨ï¼Œåœ¨ä¸€ä¸ª cpu ä¸Šçš„ cache ä¸­å¯èƒ½ä¼šæœ‰å…¶ä»– cpu æ‰€å¤„ç†çš„ä¸å½“å‰ cpu æ— å…³çš„æ•°æ® (ä¸€èˆ¬æ˜¯æ•°ç»„å…ƒç´ )ï¼Œå½“é‚£ä¸€éƒ¨åˆ†çš„æ•°æ®å‘ç”Ÿæ›´æ”¹æ—¶ï¼Œå½“å‰å¤„ç†å™¨çš„ cache éœ€è¦è¿›è¡ŒåŒæ­¥ï¼Œä»è€Œè¿›è¡Œä¸€ç³»åˆ—çš„æ²¡æœ‰æ„ä¹‰çš„ IO æ“ä½œï¼Œä»è€Œä¸¥é‡å½±å“å¹¶è¡Œæ€§èƒ½ã€‚\né¿å… false sharing\n\nå¯¹äºæ•°ç»„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¢åŠ  paddingï¼Œå¼ºåˆ¶å°†ä¸åŒ cpu cache é‡Œçš„å…ƒç´ éš”ç¦»å¼€ã€‚\né¿å…ä½¿ç”¨å…¨å±€æ•°ç»„ï¼Œå¯¹æ¯ä¸ª task ä½¿ç”¨ private çš„æ•°ç»„ï¼Œä½¿ç”¨ synchronization pragma æ¥è¿›è¡ŒåŒæ­¥ã€‚\n\n\n\n\nQ7 Why do synchronizations slow down your program\n\nè‹¥æœ‰å¾ˆå¤šçº¿ç¨‹åŒæ—¶éœ€è¦è¿›è¡ŒåŒæ­¥æ“ä½œï¼Œè¿›å…¥æ’é˜Ÿç­‰å¾…çŠ¶æ€ï¼Œé‚£ä¹ˆæ­¤æ—¶å¹¶è¡Œé€€åŒ–æˆäº†ä¸²è¡Œï¼Œä¼šé€ æˆæ€§èƒ½çš„ä¸‹é™ã€‚\nåŒæ­¥æ“ä½œæœ¬èº«ä¼šæœ‰ä¸€å®šçš„å¼€é”€ï¼Œå¦‚æ’é˜Ÿè°ƒåº¦ã€å¤„ç†å™¨ä¹‹é—´çš„é€šä¿¡ç­‰ã€‚\n\n\n\nQ8 When should we use critical, atomic or lock pragma\n\ncritical: å½“éœ€è¦é¿å… data racingï¼Œåªå…è®¸ä¸€ä¸ªçº¿ç¨‹è¿›å…¥ï¼Œè€Œä¸”å…³é”®åŒºè®¡ç®—çš„å¼€é”€ä¸æ˜¯éå¸¸å¤§çš„æ—¶å€™ä½¿ç”¨ã€‚\natomic: åªèƒ½åœ¨ä¸€äº›ç®€å•çš„è¿ç®—ä¸Šä½¿ç”¨(éœ€è¦ç¡¬ä»¶æ”¯æŒ)ï¼Œå¦‚ a++ ç­‰ã€‚\nlock: å½“éœ€è¦æ›´å¤æ‚çš„åŒæ­¥æˆ–æœ‰å¤æ‚çš„é”çš„å…³ç³»æ—¶ä½¿ç”¨ã€‚\n\n\n\nQ9 Give four reasons why the iterations are not in a static order for a parallel for loop\n\nparallel for çš„ schedule å‚æ•°é»˜è®¤æ˜¯ staticï¼Œå³åœ¨å¾ªç¯å¼€å§‹å‰ä¸ºæ¯ä¸ª thread åˆ†é…å¤§è‡´ç­‰é‡ä¸ª iterations1ã€‚å› æ­¤ï¼Œç”±äºä¸åŒ thread æ‰§è¡Œçš„ iterations chuncks ä¹‹é—´åœ¨è¿è¡Œæ—¶æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒä»¬æ‰§è¡Œçš„é¡ºåºæ˜¯ä¸ç¡®å®šçš„ã€‚\nç¨‹åºçš„è¿è¡Œä¼šå› ä¸º cpu çš„è°ƒåº¦ç­‰åŸå› ï¼Œå¯¼è‡´è¿è¡Œçš„æ—¶é—´ä¸å›ºå®šã€‚ä»è€Œå½±å“æ‰§è¡Œçš„é¡ºåºã€‚\nè®¡ç®—æœºå¹¶ä¸èƒ½ä¿è¯æä¾›å›ºå®šæ•°é‡çš„ cpuï¼Œå› æ­¤ OpenMP å¯¹å¹¶è¡Œ tasks çš„è°ƒåº¦å’Œç®¡ç†ä¹Ÿä¸å›ºå®šã€‚\nç”±äº OpenMP å’Œæ“ä½œç³»ç»Ÿçš„è°ƒåº¦ï¼Œthreads çš„æ‰§è¡Œé¡ºåºå¹¶ä¸ä¸€å®šæ˜¯ä»–ä»¬çš„åˆ›å»ºé¡ºåºï¼Œä¸”ä¸å›ºå®šã€‚\n\n1Â Openmp.org. [Online]. Available: https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-1.pdf. [Accessed: 22-Apr-2022].\n\n\n\n\n\nsolution\n\n\n\n(Compile, Run) \\(\\times\\) (Hardware, Software)\n\n\n\n\nQ10 Safe way to generate random numbers in parallel\nä½¿ç”¨å¹¶è¡Œç‰ˆ LCG ç®—æ³•ç”Ÿæˆéšæœºæ•°ã€‚\n\n// åŸ LCG ç®—æ³•\nint random() {\n    random_next = (MULTIPLIER * random_last + ADDEND) % PMOD;\n    random_last = random_next;\n\n    return random_next;\n}\n\nåœ¨åŸ LCG ç®—æ³•åŸºç¡€ä¸Šï¼Œä½¿ç”¨ Leap Frog Methodã€‚è®°ç”± LCG ç®—æ³•ç”Ÿæˆçš„éšæœºæ•°åºåˆ—ä¸º \\(a_0,a_1,\\dots\\)ï¼Œæœ‰ k ä¸ª threadã€‚å– \\(a_0,a_1,\\dots,a_{k-1}\\) ä½œä¸ºæ¯ä¸ª thread éšæœºæ•°åºåˆ—çš„ç§å­ï¼Œå¹¶ä¿®æ”¹ MULTIPLIER å’Œ ADDENDï¼Œä½¿å¾— \\(\\text{next}(a_i)=a_{i+k}\\)ï¼Œå³æ¯ä¸ª thread \\(t\\) æ‰€ä½¿ç”¨çš„éšæœºæ•°åºåˆ—ä¸º \\(a_{t::k}\\)ã€‚"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-2-comment-deletion",
    "href": "posts/parallel-algorithm/solution.html#algorithm-2-comment-deletion",
    "title": "midterm exam",
    "section": "Algorithm 2: comment deletion",
    "text": "Algorithm 2: comment deletion\nsolution: scan, fragsum"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-3-missing-element",
    "href": "posts/parallel-algorithm/solution.html#algorithm-3-missing-element",
    "title": "midterm exam",
    "section": "Algorithm 3: missing element",
    "text": "Algorithm 3: missing element\nsolution: xor"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-4-knn",
    "href": "posts/parallel-algorithm/solution.html#algorithm-4-knn",
    "title": "midterm exam",
    "section": "Algorithm 4: KNN",
    "text": "Algorithm 4: KNN\nsolution:\nfor i: 0 to N\n    for j: i+1 to N\n        d[i,j]=dis[A[i],A[j]] // heap[i].add, heap[j].add, lock\ndesigned schedule to balance work flow in each thread\nlock"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html",
    "href": "posts/parallel-algorithm/PA-08.html",
    "title": "parallel algorithm course 08",
    "section": "",
    "text": "openACC\nGPU not sharable\nopenacc.org\nopenacc best programming\nthree levels\ndata"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html#optimize",
    "href": "posts/parallel-algorithm/PA-08.html#optimize",
    "title": "parallel algorithm course 08",
    "section": "optimize",
    "text": "optimize\nUnified virtual memory, auto, slow\nremove the â€œmanagedâ€ suboption to the -ta compiler\ndata clauses\nallocate with copy\n\ncopyin {list}: cpu \\(\\to\\) gpu\ncopyout {list}: gpu \\(\\to\\) cpu\ncopy {list}\n\nallocate without copy\n\ncreate {list}\ndelete {list}\n\nonly copy\n\npresent {list}: only copy if present when copyin\n\npragma\n#pragam acc data copyin (a[0:nelem]) copyout(b[s/4:3*s/4])\nrun functions in each devices, and do copyin, copyout in the functions\nslow\nwe can copyin and copyout together\n\ndata\n\n#pragma acc data\n{\n    #pragma acc parallel loop\n    ...\n    #pragma acc parallel loop\n    ...\n}\ncopyin and copyout in each loops are done in one clause\n\nenter data: copyin, create\nexit data: copyout, delete, finalize(destroy a variable not regarding its reference)\nclass M{\n    M() {\n        v = new double[N];\n        #pragma acc enter data create(v)\n    }\n    ~M() {\n        #pragma acc exit data delete(v)\n        {\n            free(v);\n        }\n    }\n}\n\nrunning with explicit data management\ndata racing update between cpu and gpu\ndo_somthing_on_device();\n\n#pragma acc update host(a)\n\ndo_something_on_host();\n\n#pragma acc update device(a)\n\nreduce IO between cpu and GPU"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html#optimize-loops",
    "href": "posts/parallel-algorithm/PA-08.html#optimize-loops",
    "title": "parallel algorithm course 08",
    "section": "optimize loops",
    "text": "optimize loops\noptimize matvec loops\ndefault iteration is 128 for GPU\nthree level\nGang, workers, vectors\none SMP \\(\\to\\) 64 cores, 16 warps(1 warp = 32 threads), at most 2048 threads\n\nset vector size as 32 k\n\nworker = warp \n#pragma acc parallel vector_length(32)\n#pragma acc loop gang worker\nfor (int i = 0; i < n; i++)\n    #pragma acc loop vector // vector loop\n    for (int j = 0; j < i; j++)\nvector loop: has to specify vector_length or num_workers\ncollapse clause\n#pragma acc parallel loop collapse(2)\nfor (i)\n    for (j)\nequiv\n#pragma acc parallel loop\nfor (ij)\n\nthe tile clause\noperate on smaller blocks of the operation to exploit data locality\n\nstride-1 memory access\ncontinus access\na[0][i][j]"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-10.html",
    "href": "posts/parallel-algorithm/pa-10.html",
    "title": "parallel algorithm course 10",
    "section": "",
    "text": "final exam\nreview\n\nfundamental\n\n\nDAG\nwork-span analysis\n\nideal: \\(T_1(n)=T_s(n)\\), \\(T_\\infty(n)=\\Theta(\\log^k(n))\\)\n\nonly consider shared data memory model\n\nalgorithmï¼š\n\nmap: n->n\n\nassociative\n\nreduce:n -> 1\nscan: n -> n\n\ncompact: n -> m\n\nlist ranking: n -> n, input becomes linked list, GRAPH, BFS, DFS, find independent\nsorting, sample sort\n\ngoal:\n\noptimal complexity\nindependent ++, sync â€“, donâ€™t use sync too much, has cost, e.g.Â print message during parallel\ne.g.Â histgram, lock in each bucket\nsymetrics â€“, repeat occurance, break symetric, use randomize\n\nthinking:\n\ncase to general\nforward - backward, cover gap between input and output\nbit-wise, radix sort\nmatrix-wise, compute fibonacci sequence, matrix operation\n\npattern\n\ndivide & conquer\nindep set\nrandom\njump\nEulur tour\nBag of pennants (conform divide and conquer patterm)\n\n\nopenacc\nprofile driven programming\n\nanalyse\nparallel\noptimize\n\nincremental programming\nCPU -> GPU -> Unified memory -> data parallel -> loop -> blocking\nblocking:\n\nIO and compute parallel\nmulti-device\n\n\nAnalyse\n\nobserve profile\nbottle-neck, for loop\nimprove \\(\\frac{\\text{compute}}{\\text{mem}}\\) access ratio\n\nParallel\n\nalgorithm\n\nOptimize\n\ndata movement, manual management\nlarge matrix, sometimes we donâ€™t need load all the elements\nloop mapping: tell compile how loop maps the level of parallel, e.g.Â vetor length\nvector 32*n -> n warps\nblocking, æµæ°´çº¿å¹¶è¡Œ\n\ninput compute output\ncompute -> futher split -> multi device\n\ntile <- GPU 2-level cache usage\ncollapse\nmemory access pattern, keep continuous access\n\nGPU hardware\n\n\n\nSMP\n\n\n\n\nç®€ç­”é¢˜ X 2\nç®—æ³•é¢˜\n\næœŸä¸­å‰ X 1\næœŸä¸­å X 1\n\nç¼–ç¨‹é¢˜ X 1"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-02.html",
    "href": "posts/parallel-algorithm/PA-02.html",
    "title": "parallel algorithm course 02",
    "section": "",
    "text": "problem \\(T_1\\)\n-> reduce dependency ->\ntasks \\(T_\\infin\\)\nwork sharing\nblocks\nsynchronization (better to avoid)\nfalse sharing"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-02.html#sync",
    "href": "posts/parallel-algorithm/PA-02.html#sync",
    "title": "parallel algorithm course 02",
    "section": "sync",
    "text": "sync\nSingle program multiple data\nseq\nfor(i=0;i<N;i++) a[i] = a[i] + b[i];\nomp par\n#pragma omp parallel\n{\n    id = omp_get_thread_num();\n    Nthrds\n    i_start = id * N / Nthrds\n    i_end = (id+1) * N / Nthrds\n    if (id==Nthrds - 1) iend=N\n    for (i=istart;i<iend;i++) ...    \n}\nshorter (but slower)\n#pragma omp parallel {\n    #pragma omp for\n        for () ...\n}\n\n// or equiv\n\n#pragma omp parallel for\n...\n\nloop worksharing constructs\nschedule clause\n\nstatic: least work for scheduling (done at compiling)\ndynamic (complex scheduling at run-time)\n\n\nreduction\nop should be associative\n+,*,-,min,max\n#pragma omp parallel for reduction (+: sum)\n    for (i=0; i < num_steps; i++)\n        x = ...\n        sum = sum + x\nslower than SPMD critical\n\nomp for loop has barrier by default\nhas implicit barrier\nuse nowait to cancel it\n\nmaster construct\n#pragma omp master\nonly master thread will execute\nno barrier for other threads\n\n#pragma omp single\nonly one thread will execute, has barrier\nuse nowait to cancel barrier\n\nsections\n#pragma omp sections\n{\n    #pragma omp section\n        thread A\n    #pragma omp section\n        thread B\n}\nA, B will execute in parallel\nhas barrier at the end of sections\n\nlock routines\nomp_init_lock();\nomp_set_lock(), omp_unset_lock()\nomp_destroy_lock()\ne.g.Â array\nif use critical on a[i], then the whole array a will be lock\n#bins \\(\\gg\\) #threads\nlow prob of false sharing, conficts are rare, use locks\n\nOMP refernce card\n\nomp_set_dynamic()\n\nenv vars\nOMP_NUM_THREADS\n\nheap: shared\nstack: private\nint tmp=0; // in heap\n#pragma omp parallel for private(tmp)\n    for (int i; i < N; i++) {\n        tmp += j; // not initialized, local copy ( in stack )\n    }\nprint(tmp); // 0 here\nint tmp=999; // in heap\n#pragma omp parallel for firstprivate(tmp)\n    for ()\n        tmp += j; // has initialized with 1, local copy ( in stack ), still private\n\nprintf(tmp) // 999\nnot use often: lastprivate\nint tmp=0; // in heap\n#pragma omp parallel for lastprivate(j)\n    for ()\n        tmp = j; // copy the last j to tmp\n    \nprint(tmp); // print the last j"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html",
    "href": "posts/parallel-algorithm/PA-03.html",
    "title": "parallel algorithm course 03",
    "section": "",
    "text": "recursion\nscan\nmap, reduce, scan"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html#compact-alg",
    "href": "posts/parallel-algorithm/PA-03.html#compact-alg",
    "title": "parallel algorithm course 03",
    "section": "compact alg",
    "text": "compact alg\npick up using an indication arr\nA = 0 1 2 3 4 5 6 F = 0 1 1 0 0 1 0\noutput = 1 2 5\nkey to do parallel, acknowledge the target position in advance\nscan F\nidx = 0 1 2 2 2 3 3"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html#fragment-sum",
    "href": "posts/parallel-algorithm/PA-03.html#fragment-sum",
    "title": "parallel algorithm course 03",
    "section": "fragment sum",
    "text": "fragment sum\n\napplication\n\nF zero flag put in the front, one flag in the back\ntwo stage compact, \\(F^{-1}\\) first, then use \\(F\\)\n\nscatter\n\n\n\nwhat is scatter\n\n\nmake it into a large sparse matrix and then reduce sum\n\nbinary base sort\nRadix sort.\nfor from lowbit to highbit\n\nscan\n\\(T_1(n,b)=O(bn)\\)\n\\(T_\\infty(n,b)=(blogn)\\)\nif from highbit to lowbit, need fragment scan\nbit-wise thinking."
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html",
    "href": "posts/parallel-algorithm/pa-09.html",
    "title": "parallel algorithm course 09",
    "section": "",
    "text": "array\nlinked list\ntree (to list)\ngraph (today)\n\nparallel BFS\n\nBFS(G[V,E],S)\n\nD[V] = inf\nD[S] = 0\n\nF = {s}\n\nwhile F not empty do {\n    v = pop(F)\n    for (v, w) in E do {\n        if D[w] = inf {\n            D[w] = D[v] + 1\n            push(F, w)\n        }\n    }\n}\nkey idea: layering \\(F_l\\)\nneed a data structure:\n\nallow repetitive occurance of elements\nunordered\nfast search union and split"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html#bag-of-pennats",
    "href": "posts/parallel-algorithm/pa-09.html#bag-of-pennats",
    "title": "parallel algorithm course 09",
    "section": "Bag of pennats",
    "text": "Bag of pennats\n\nA pennant: \\(2^k\\) nodes\nadd an extra root to a balance binary tree\nunion, split\na bag of pennants can be mapped to a binary number\ninsert -> add one (\\(O(\\log n)\\))\ncombine two bags of pennants -> binary number adding (\\(O(\\log n)\\))\nsplit -> require balance (\\(O(\\log n)\\))\n\n\n\n\nbag of pennant\n\n\n// calculate F(l+1) given F(l)\ninitialize(F(l+1), an empty pennant)\n\nfun process-level (G, F(l), F(l+1), D)\nif |F(l)| > threshold {\n    Fa, Fb = split(F(l))\n    spam {\n        process-level (G, Fa, F(l+1), D)\n        process-level (G, Fb, F(l+1), D)\n    }   \n    sync\n} else {\n    foreach v in F(l)\n    par-for (v, w) in E do {\n        if D[w] = inf {\n            D[w] = D[v] + 1\n            insert(F(l+1), w)\n        }\n    }\n}"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html#extend",
    "href": "posts/parallel-algorithm/pa-09.html#extend",
    "title": "parallel algorithm course 09",
    "section": "extend",
    "text": "extend\nmatrix A, B, C\n\\(A\\times B=C\\)\ntile\nconsider \\(A^k\\)\nsave \\(A\\), \\(A^T\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html",
    "href": "posts/parallel-algorithm/pa-07.html",
    "title": "parallel algorithm course 07",
    "section": "",
    "text": "tensor core\nmath limited, memory limited"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#gpu",
    "href": "posts/parallel-algorithm/pa-07.html#gpu",
    "title": "parallel algorithm course 07",
    "section": "GPU",
    "text": "GPU\nL1 caches are independent, L2 caches are shared\nexpected: \\(T_\\text{math}\\ge T_\\text{mem}\\)\n\\[\n\\frac{\\#\\text{opt}}{\\text{math bandwidth}}\\ge \\frac{\\#\\text{bytes}}{\\text{mem bandwidth}}\n\\]\nArithmetic intensity \\(\\frac{\\#\\text{opt}}{\\#\\text{bytes}}\\)\nmath limited when arithmetic intensity \\(\\ge\\frac{\\text{math band}}{\\text{mem band}}\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#key-ideas",
    "href": "posts/parallel-algorithm/pa-07.html#key-ideas",
    "title": "parallel algorithm course 07",
    "section": "key ideas",
    "text": "key ideas\nsplit, independent ++\nuser cache\nTensor Cores: accelerate dot-product, matrix multiplies, size should better be a mulitple of 128 bits"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#how-to-do-split",
    "href": "posts/parallel-algorithm/pa-07.html#how-to-do-split",
    "title": "parallel algorithm course 07",
    "section": "how to do split",
    "text": "how to do split\n\nkey: tile\nmultiplication, read a tile at a time to L1 cache\nquantization: size should matches the hardware\nrunning time is stagewise increasing as size increases\nsmall batch \\(\\to\\) low usage of GPU"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#post-order-reversal",
    "href": "posts/parallel-algorithm/pa-07.html#post-order-reversal",
    "title": "parallel algorithm course 07",
    "section": "post-order reversal",
    "text": "post-order reversal\nDFS, post-order\nhow to parallelize\nhint: list-ranking\n\ntree \\(\\to\\) list, Eular Tour, add points\ninitialization, upward point -1, downward point +1\nlist scan\nlist \\(\\to\\) tree\n\nadjacency list"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html",
    "href": "posts/parallel-algorithm/pa-04.html",
    "title": "parallel algorithm course 04",
    "section": "",
    "text": "synchronization has costs\nsimple list traversal\ncompute all p in advance"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#task",
    "href": "posts/parallel-algorithm/pa-04.html#task",
    "title": "parallel algorithm course 04",
    "section": "task",
    "text": "task\n#pragma omp parallel {\n    // not often used\n    #pragma omp task\n    foo();\n\n    #pragma omp barrier\n\n    // more common case\n    #pragma omp single {\n        #pragma omp task\n        bar();\n    }\n}\nexample (with bug)\nint fib(int n) {\n    int x, y; // private\n    if (n < 2) return n;\n\n    #pragma omp task \n    // need `shared(x)`. by default, task copy the whole stack, \n    // x is in different address\n    x = fib(n-1);\n\n    #pragma omp task // need `shared(y)`, default is private in task\n    y = fib(n-2);\n\n    #pragma omp taskwait\n\n    return x+y; // wrong here!!!\n}\n\n#pragma omp parallel single\nfib(n);\ntwo important points\n\nbarrier\ndata env\n\nexample 2 (with bug)\nlist ml;\nElement *e;\n#pragma omp parallel\n#pragma omp single\n{\n    for (e=ml->first; e; e=e->next)\n    #pragma omp task // need firstprivate(e)\n        process(e); // has data racing\n}\nreturn to first example\n#pragma omp parallel\n{\n    #pragma omp single\n    {\n        node* p = head;\n        while(p) {\n            #pragma omp task firstprivate(p)\n                process(p);\n            p = p->next;\n        }\n    }\n}\n\n\n\nrunning schema"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#merge-sort",
    "href": "posts/parallel-algorithm/pa-04.html#merge-sort",
    "title": "parallel algorithm course 04",
    "section": "merge sort",
    "text": "merge sort\ndirectly using parallel\n\\(T_1=2T_1(\\frac{n}{2})+O(n)=O(n\\cdot log(n)\\)\n\\(T_\\infty=T_\\infty(\\frac{n}{2})+O(n)=O(n)\\)\n\nuse a differernt merge method\nparallel merge\nsay here we have two sorted seqs (\\(\\le\\))\n\\(A=a_1,\\dots,a_n\\) and \\(B=b_1,\\dots,b_n\\)\nlet \\(t=a_{\\lceil \\frac{n}{2}\\rceil}\\), split A and B into sub arr \\(A_1,A_2,B_1,B_2\\) s.t. \\(A_1\\le t, B_1\\le t\\) and \\(A_2\\gt t, B_2\\gt t\\)\nso seq \\(A_1,B_1,A_2,B_2\\) is semi-merged by t, then go on with \\(merge(A_1,B_1)\\) and \\(merge(A_2,B_2)\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#random-number-generating-problem",
    "href": "posts/parallel-algorithm/pa-04.html#random-number-generating-problem",
    "title": "parallel algorithm course 04",
    "section": "random number generating problem",
    "text": "random number generating problem\n\nproblem\nexample: approximate \\(\\pi\\) using Monto Carlo\nseed(SEED);\n#pragma omp parallel for private(x,y) reduction (+:n_in_circle)\nfor(int i = 0; i < num_trials; i++) {\n    x = random();\n    y = random();\n\n    if (x*x + y*y <= r*r) n_in_circle ++;\n}\n\npi = 4.0 * (1.0 * n_in_circle / num_trials);\n\nLinear Congruential Generator(LCG) (has data racing problem)\nint random_last = 0;\n// need \n// #pragma omp threadprivate(random_last)\n\nint random() {\n    random_next = (MULTIPLIER * random_last + ADDEND) % PMOD;\n    random_last = random_next;\n\n    return random_next;\n}\nparameter: (one possible parameter suite) MULTIPLIER=1366, ADDEND=150889, PMOD=714025\nhas a period (less than PMOD)\n\nsingle thread and multi-thread has different accuracy (multi-thread has poor performance)\ndata racing on var random_last\n\n\nsolution 1\nadd #pragma omp threadprivate(random_last) under int random_last=0\nhas a local copy of random_last when creating a thread\nsimilar to firstprivate\nproblem: still poor than single thread version\ndifferent random range in threads maybe overlap\n\n\nsolution 2\nLeap Frog method\ngenerate different seeds for each thread\nsituation for ADDEND=0\n#pragma omp single\n{\n    nthreads = ...\n    iseed = PMOD / MULTIPLIER;\n    pseed[0] = iseed;\n    mult_n = MULTIPLIER;\n    for (int i = 0; i < nthreads; i++) {\n        iseed = (unsigned long long) ((MULTIPLIER * iseed) % PMOD);\n        pseed[i] = iseed;\n        mult_n = (mult_n * MULTIPLIER) % PMOD;\n    }\n    id = ...\n    random_last = (unsigned long long) pseed[id];\n}"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-01.html",
    "href": "posts/parallel-algorithm/PA-01.html",
    "title": "parallel algorithm course 01",
    "section": "",
    "text": "gcc -fopenmp\nomp_num_thread(int): request\nmulti-data\nomp_get_thread_num, get id\n\nSMP: equal-time access cost, in theory\nNUMA: different .., practically\n\nFalse Sharing\ncache line\ntwo processors may have access to the same region, repeat many useless write back\n\nSynchronization, to avoid data racing, false sharing (avoid global array) d barrier\n#pragma omp barrier\ncritical\nonly one thread can enter (often cost cheap), mutual exclusion, avoid data racing\n(software support)\n#pragma omp critical\natomic\nonly support (hardware support)\n\nx binopr= expr\nx++, ++x, xâ€“, â€“x\n\n#pragma omp atomic"
  },
  {
    "objectID": "posts/paper-reading/moco.html",
    "href": "posts/paper-reading/moco.html",
    "title": "MoCo",
    "section": "",
    "text": "åŠ¨é‡å¯¹æ¯”å­¦ä¹ (Momentum Contrast, MoCo)ç”¨äºå›¾è±¡è¡¨å¾çš„æ— ç›‘ç£å­¦ä¹ ã€‚ä¸»è¦çš„è§‚ç‚¹æ˜¯å°†å¯¹æ¯”å­¦ä¹ çœ‹ä½œæ˜¯ä¸€ç§å­—å…¸æŸ¥è¯¢ï¼Œå³ç»™å®šä¸€ä¸ªå›¾ç‰‡çš„ç¼–ç ä½œä¸ºè¯·æ±‚ï¼Œåœ¨å­—å…¸ä¸­æ‰¾åˆ°ä¸ä¹‹å¯¹åº”çš„å›¾ç‰‡ç¼–ç ã€‚å¹¶ä¾æ­¤æå‡ºäº†åœ¨è§†è§‰é¢†åŸŸä¸­ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•éœ€è¦é¢ä¸´çš„ä¸¤ä¸ªå…³é”®ç‚¹ï¼šä¸€æ˜¯å­—å…¸åº”å°½å¯èƒ½çš„å¤§ï¼›äºŒæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºç¼–ç å­—å…¸çš„ç¼–ç å™¨åº”å°½å¯èƒ½åœ°ä¿æŒä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯è¦ä¿è¯æ‰€æœ‰çš„å›¾ç‰‡éƒ½å¤§è‡´æ˜ å°„åˆ°äº†åŒä¸€ä¸ªè¡¨å¾ç©ºé—´ä¸­ã€‚ä¸ºè§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä½¿ç”¨é˜Ÿåˆ—åŠ¨æ€ç»´æŠ¤å­—å…¸ï¼›é’ˆå¯¹ç¬¬äºŒä¸ªé—®é¢˜ï¼Œä½¿ç”¨åŠ¨é‡æ›´æ–°çš„æ–¹æ³•å»ä¿®æ”¹ç¼–ç å™¨çš„å‚æ•°ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#background",
    "href": "posts/paper-reading/moco.html#background",
    "title": "MoCo",
    "section": "Background",
    "text": "Background\næ— ç›‘ç£å­¦ä¹ çš„æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸå–å¾—äº†å¾ˆå¤§çš„æˆåŠŸï¼Œè¿™å¾—ç›Šäºåœ¨ NLP ä¸­ï¼Œæ¨¡å‹é¢å¯¹çš„è¾“å…¥ä¿¡å·æ˜¯ç¦»æ•£çš„ï¼ŒåŒæ—¶æ˜“äºåˆ†å‰²æˆå°çš„å•å…ƒæ¥å»ºé€ å­—å…¸æˆ–è€…å¯¹åº”çš„è¡¨å¾ã€‚è€Œåœ¨è§†è§‰é¢†åŸŸï¼Œè¾“å…¥çš„ä¿¡å·æ˜¯è¿ç»­çš„ã€é«˜ç»´åº¦çš„ã€éç»“æ„åŒ–çš„ï¼Œæƒ³è¦å¾—åˆ°åŒæ ·æ•ˆæœçš„ç¨€ç–çš„è¡¨å¾æ˜¯å¾ˆå›°éš¾çš„ã€‚\næ— ç›‘ç£å­¦ä¹ åœ¨å¤§å‹æ— æ ‡è®°æ•°æ®é›†ä¸Šçš„ä½¿ç”¨æ˜¯éå¸¸å€¼å¾—å…³æ³¨çš„ï¼Œä½¿ç”¨æ— ç›‘ç£å­¦ä¹ æ¥ä¸è®­ç»ƒå¾—åˆ°çš„è¡¨å¾ä¹Ÿèƒ½ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„å­¦ä¹ ã€‚ä½†ä»–çš„æ•ˆæœå´å¹¶ä¸å¦‚ç›®å‰çš„ç›‘ç£å¼å­¦ä¹ æ•ˆæœå¥½ã€‚è‹¥èƒ½å¾ˆå¥½çš„è§£å†³è¿™ä¸€è¡¨å¾å­¦ä¹ çš„é—®é¢˜ï¼Œåˆ™èƒ½å¤§å¤§ç¼©å°ç›‘ç£ä¸æ— ç›‘ç£å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#retated-work",
    "href": "posts/paper-reading/moco.html#retated-work",
    "title": "MoCo",
    "section": "Retated Work",
    "text": "Retated Work\nä¸ºäº†è§£å†³ä»¥ä¸Šæå‡ºçš„é—®é¢˜ï¼Œå¾ˆå¤šç ”ç©¶æå‡ºäº†ä¸åŒçš„åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚\n\nloss function\næœ€æ—©æœŸçš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯»æ‰¾ä¸€ç§ç¼–ç å™¨è´Ÿè´£é™ç»´ä»¥åŠç‰¹å¾çš„å­¦ä¹ ï¼Œä»¥æˆå¯¹çš„æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œä»¥ä»–ä»¬çš„è·ç¦»æ¥è¡¨è¾¾ç›¸ä¼¼åº¦ã€‚ä¸ºæ­¤ï¼ŒæŸå¤±å‡½æ•°åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå‰è€…ç”¨æ¥ç¼©å°ç›¸ä¼¼æ•°æ®çš„è·ç¦»ï¼Œåè€…ç”¨æ¥æ‰©å¤§ä¸ç›¸ä¼¼æ•°æ®çš„è·ç¦»ã€‚\nä¹‹åè½¬åŒ–æˆä½¿ç”¨äº’ä¿¡æ¯å»ºæ¨¡ï¼Œæå‡º InfoNCE æŸå¤±å‡½æ•°ã€‚é‡‡æ ·æ­£è´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚æœ€ç»ˆæ™®éä½¿ç”¨çš„æ˜¯éå‚æ•°åŒ–çš„softmaxå‡½æ•°ã€‚\n\\[P(i|v)=\\frac{\\exp(v_i^Tv/\\tau)}{\\sum_{j=1}^{n}{\\exp(v_j^Tv/\\tau)}}\\]\n\n\ncontrastive loss mechanisms\n\nend-to-end\nç«¯åˆ°ç«¯çš„æ–¹æ³•ä¸­ï¼Œå½“å‰çš„batchè¢«ä½œä¸ºå­—å…¸è¿›è¡Œè®­ç»ƒï¼Œqueryå’Œkeyåˆ†åˆ«é‡‡ç”¨ä¸åŒçš„ç¼–ç å™¨ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚ç”±äºç®—åŠ›çš„é™åˆ¶ï¼Œæ›´å¤§çš„å­—å…¸éš¾ä»¥è¿›è¡Œæ¢¯åº¦çš„è®¡ç®—ï¼ŒåŒæ—¶ä»–ä¹Ÿå—é™äºbatchçš„å¤§å°ã€‚ä»–çš„ä¸€è‡´æ€§ä¿æŒå¾—å¾ˆå¥½ï¼Œå› ä¸ºä¸€ä¸ªbatchå­—å…¸çš„ç¼–ç å™¨æ€»æ˜¯åŒä¸€ä¸ªï¼Œä½†ç¼ºç‚¹æ˜¯å­—å…¸æ•°é‡éš¾ä»¥ä¿è¯ã€‚\n\n\nmemory bank\nå¦ä¸€ç§æœºåˆ¶æ˜¯æå‰å°†æ‰€æœ‰æ ·æœ¬çš„ç¼–ç è®¡ç®—å‡ºæ¥ï¼Œæ¯æ¬¡queryæ—¶ï¼Œä¼šåœ¨æ‰€æœ‰æ ·æœ¬ä¸­å†è¿›è¡Œé‡‡æ ·ä½œä¸ºå­—å…¸ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸å†å¯¹keyçš„ç¼–ç å™¨è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨queryçš„å‚æ•°ã€‚æ³¨æ„ï¼Œè¿™é‡Œçš„æ›´æ–°æ˜¯ä¸åŠæ—¶çš„ï¼Œæ¯æ¬¡åªæœ‰å½“å‰è¢«é‡‡æ ·çš„æ ·æœ¬ç¼–ç è¢«æ›´æ–°äº†ã€‚è¿™æ ·å­—å…¸å¤§å°çš„é—®é¢˜è§£å†³äº†ï¼Œä½†ä¸€è‡´æ€§çš„é—®é¢˜å´å˜å¾—æ£˜æ‰‹ï¼Œå› ä¸ºæ¯æ¬¡é‡‡æ ·çš„ç¼–ç å¯èƒ½æ˜¯ç”±ä¸åŒçš„ç¼–ç å™¨(å‚æ•°ä¸åŒ)å¾—åˆ°ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#novelty",
    "href": "posts/paper-reading/moco.html#novelty",
    "title": "MoCo",
    "section": "novelty",
    "text": "novelty\nMoCo æå‡ºä½¿ç”¨é˜Ÿåˆ—æ¥åŠ¨æ€ç»´æŠ¤å­—å…¸ï¼ŒæˆåŠŸåšåˆ°äº†å­—å…¸å¤§å°å®Œå…¨ä¸ä¾èµ–batchçš„å¤§å°ã€‚æ–°çš„batchè¢«åŠ å…¥åˆ°é˜Ÿåˆ—ä¸­ï¼Œæœ€æ—§çš„batchä¼šè¢«ç§»é™¤ã€‚è¿™æ ·ï¼Œé˜Ÿåˆ—ä¸­çš„keysä½¿ç”¨çš„ç¼–ç å™¨æ˜¯è¿ç»­ã€å¹³æ»‘çš„æ›´æ–°çš„ã€‚\nç¬¬äºŒç‚¹æ˜¯keyç¼–ç å™¨å‚æ•°çš„æ›´æ–°ä¸Šï¼Œå¹¶ä¸æ˜¯ç›´æ¥å¤åˆ¶queryçš„å‚æ•°ï¼Œè€Œæ˜¯åŠ¨é‡æ›´æ–°ï¼Œæ›´åŠ çš„å¹³æ»‘ï¼Œä¿è¯äº†ä¸€è‡´æ€§ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#dict-look-up",
    "href": "posts/paper-reading/moco.html#dict-look-up",
    "title": "MoCo",
    "section": "dict look-up",
    "text": "dict look-up\nä¸ä¸Šæ–‡æåˆ°çš„ memory-bank æ–¹æ³•ç±»ä¼¼ï¼Œæ ¹æ®ä¸€ä¸ªæ ·æœ¬ï¼Œé€šè¿‡éšæœºå˜æ¢å¾—åˆ°æ–°æ ·æœ¬ä½œä¸ºæ­£ä¾‹ï¼Œä¹‹åå– K ä¸ªè´Ÿä¾‹æ±‚å‡º InfoNCE loss è¿›è¡Œä¼˜åŒ–ã€‚ä¸åŒçš„æ˜¯è´Ÿä¾‹çš„é€‰æ‹©æ–¹å¼å’Œç¼–ç å™¨çš„æ›´æ–°æ–¹å¼ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#momentum-contrast",
    "href": "posts/paper-reading/moco.html#momentum-contrast",
    "title": "MoCo",
    "section": "Momentum Contrast",
    "text": "Momentum Contrast\n\nqueue\næ¯æ¬¡å–å¾—ä¸€ä¸ªæ–°çš„batchæ•°æ®è¿›è¡Œéšæœºå¢å¼ºä½œä¸ºè¯·æ±‚ qï¼Œå†åšå¦ä¸€ç§éšæœºå¢å¼ºä½œä¸ºæ¯ä¸ªå•ç‹¬è¯·æ±‚çš„æ­£ä¾‹ kã€‚å°†é˜Ÿåˆ—ä¸­çš„å…¨éƒ¨å…ƒç´ ä½œä¸ºæ¯ä¸ªè¯·æ±‚çš„è´Ÿä¾‹ï¼Œæ±‚å¾—æŸå¤±ï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ã€‚éšåå°† k åŠ å…¥åˆ°é˜Ÿåˆ—ä¸­ï¼ŒåŒæ—¶å»æ‰æ—§çš„batchã€‚ ç”±äºé˜Ÿåˆ—çš„è§„æ¨¡ä¼šå¾ˆå¤§ï¼Œé˜Ÿåˆ—ä¸­çš„ç¼–ç åŠ å…¥é˜Ÿåˆ—åå¹¶ä¸ä¼šå†æ›´æ–°ï¼Œä½†è¿™å¹¶ä¸ä¼šå¦¨ç¢ä¸€è‡´æ€§çš„ä¿è¯ï¼Œå› ä¸ºå®ƒæ€»ä¼šæŠŠæœ€ä¸ä¸€è‡´çš„ç¼–ç å»é™¤æ‰ã€‚\n\n\nMomentum update\nåœ¨æ›´æ–°å‚æ•°æ—¶ï¼Œåªå¯¹queryçš„ç¼–ç å™¨å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œè€Œå¯¹keyç¼–ç å™¨è¿›è¡Œå¦‚ä¸‹æ›´æ–°: \\[\\theta_k \\leftarrow m\\theta_k+(1-m)\\theta_q\\]\nè€Œåœ¨å®éªŒä¸­ï¼Œç¨å¤§çš„ m å¾—åˆ°çš„ç»“æœä¼šæ›´å¥½ï¼Œè¿™ä¹Ÿå°è¯äº†ä¸€è‡´æ€§çš„é‡è¦ã€‚\n\n\nPretext Task\nå‰ç½®ä»»åŠ¡ä¸ memory-bank ç±»ä¼¼ã€‚è¿™é‡Œä½œè€…é‡‡ç”¨äº† ResNet ä½œä¸ºç¼–ç å™¨ï¼Œè€Œä½œè€…é€šè¿‡å®éªŒå‘ç° BN å¯¹ç¼–ç çš„å­¦ä¹ èµ·åˆ°äº†æ¶ˆæçš„ä½œç”¨ï¼Œè€Œè¿™å¯èƒ½æ˜¯ç”±äº BN èåˆäº†å½“å‰batchçš„ä¿¡æ¯ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šä¼šä½¿æ¨¡å‹â€èµ°æ·å¾„â€ã€‚\nå¯¹ BN çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº† shuffling BN çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åœ¨ BN å‰å…ˆå°† sample è¿›è¡Œæ‰“ä¹±ï¼Œæ±‚å¾—ç¼–ç åå†å¤åŸã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#training",
    "href": "posts/paper-reading/moco.html#training",
    "title": "MoCo",
    "section": "training",
    "text": "training\næ•°æ®é›†é‡‡ç”¨çš„æ˜¯\n\nImageNet: åŒ…å«çº¦ 1 ç™¾ä¸‡å¼ å›¾ç‰‡ï¼Œ1000 ä¸ªç±»åˆ«ã€‚å›¾ç‰‡çš„ç±»åˆ«åˆ†å¸ƒå‡åŒ€\nInstagram: ç”± Instagram ä¸­å¾—åˆ°çš„çº¦ 10 äº¿å¼ å›¾ç‰‡ã€‚å…·æœ‰é•¿å°¾åˆ†å¸ƒã€‚\n\nè¿™é‡Œä½œè€…æ˜¯å°†ç‰¹å¾å±‚å†»ç»“ï¼Œå¾®è°ƒçº¿æ€§åˆ†ç±»å±‚æ¥éªŒè¯æ¨¡å‹çš„æ•ˆæœã€‚ä¸ä¼—å¤šæ— ç›‘ç£æ¨¡å‹è¿›è¡Œæ¯”å¯¹ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#findings",
    "href": "posts/paper-reading/moco.html#findings",
    "title": "MoCo",
    "section": "findings",
    "text": "findings\n\nåœ¨å¾®è°ƒMoCoæ—¶ï¼Œä¸€äº›è¶…å‚æ•°ä¸ç›‘ç£å¼å­¦ä¹ æ¨¡å‹ç›¸å·®å¾ˆå¤§ï¼Œè¿™è¡¨æ˜ä»–ä»¬çš„ç‰¹å¾åˆ†å¸ƒæœ‰è¾ƒå¤§çš„å·®å¼‚\né€šè¿‡ä¸ end-to-end å’Œ memory-bank ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”å¯ä»¥å‘ç°ï¼Œå½“å­—å…¸è§„æ¨¡è¾ƒå°æ—¶ï¼Œend-to-end æ¨¡å‹å’Œ MoCo ä¸ç›¸ä¸Šä¸‹ï¼Œmemory-bank åˆ™ç¨é€Šé£éªšï¼›ä½†å¢å¤§å­—å…¸è§„æ¨¡åï¼Œend-to-end æ¨¡å‹åˆ™å—é™äºç®—åŠ›ï¼Œæ— æ³•è®¡ç®—ï¼Œå…¶ä»–ä¸¤ä¸ªæ¨¡å‹éƒ½å¯ä»¥æŒç»­å¢åŠ ï¼Œä½† MoCo åˆ™æ˜æ˜¾èƒœå‡ºè®¸å¤šã€‚è¯´æ˜ MoCo å¾ˆå¤§çš„æå‡äº†æ— ç›‘ç£å­¦ä¹ çš„æ•ˆæœã€‚\nè‹¥å®Œå…¨å¤åˆ¶queryç¼–ç å™¨å‚æ•°ï¼Œåˆ™æ— æ³•æ”¶æ•›\nMoCo å¯ä»¥å¾ˆå¥½çš„ä½œä¸ºpre-trainedæ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å­¦ä¹ ã€‚"
  },
  {
    "objectID": "posts/paper-reading/moco.html#conclusion",
    "href": "posts/paper-reading/moco.html#conclusion",
    "title": "MoCo",
    "section": "conclusion",
    "text": "conclusion\nMoCo æä¾›äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œåœ¨èƒ½å¤Ÿä½¿å­—å…¸å°½å¯èƒ½å¤§çš„åŒæ—¶ï¼Œè¿˜å¯ä»¥ä¿è¯è¡¨å¾ç©ºé—´çš„ä¸€è‡´æ€§ã€‚è¿™æ˜¯ä¸€ç§éå¸¸ç®€æ´è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html",
    "href": "posts/paper-reading/multi-view-clustering.html",
    "title": "some papers about multi-view-clustering",
    "section": "",
    "text": "very brief summaries of some papers I read"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#binary-multi-view-clustering",
    "href": "posts/paper-reading/multi-view-clustering.html#binary-multi-view-clustering",
    "title": "some papers about multi-view-clustering",
    "section": "Binary Multi-View Clustering",
    "text": "Binary Multi-View Clustering\nlarge scale multi-view image clustering\njointly learn collaborative discrete representation and binary cluster structures\nhas an algorithm with proved convergence analysis"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#contrastive-clustering",
    "href": "posts/paper-reading/multi-view-clustering.html#contrastive-clustering",
    "title": "some papers about multi-view-clustering",
    "section": "Contrastive Clustering",
    "text": "Contrastive Clustering\nunified instance- and cluster-level contrastive learning\nrow vectors and column vectors as instance representation and cluster representation\nQ: how to construct negative instance, how to design the training target"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#deep-clustering-on-the-link-between-discriminative-models-and-k-means",
    "href": "posts/paper-reading/multi-view-clustering.html#deep-clustering-on-the-link-between-discriminative-models-and-k-means",
    "title": "some papers about multi-view-clustering",
    "section": "Deep Clustering: On the Link between Discriminative Models and K-Means",
    "text": "Deep Clustering: On the Link between Discriminative Models and K-Means\ndiscover the equivilance between discriminative models using L2 regularized MI loss and soft regularized K-means loss, under some conditions"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#multiview-clustering-a-scalable-and-parameter-free-bipartite-graph-fusion-method",
    "href": "posts/paper-reading/multi-view-clustering.html#multiview-clustering-a-scalable-and-parameter-free-bipartite-graph-fusion-method",
    "title": "some papers about multi-view-clustering",
    "section": "Multiview Clustering: A Scalable and Parameter-Free Bipartite Graph Fusion Method ",
    "text": "Multiview Clustering: A Scalable and Parameter-Free Bipartite Graph Fusion Method \nparameter-free, graph based multi-view clustering (graph fusion frameswork ?)"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#dual-contrastive-prediction-for-incomplete-multi-view-representation-learning",
    "href": "posts/paper-reading/multi-view-clustering.html#dual-contrastive-prediction-for-incomplete-multi-view-representation-learning",
    "title": "some papers about multi-view-clustering",
    "section": "Dual Contrastive Prediction for Incomplete Multi-view Representation Learning ",
    "text": "Dual Contrastive Prediction for Incomplete Multi-view Representation Learning \nin-complete MvRL\nunify consistency learning and missing data recovery: proved using information theory\nframework based on contrastive learning loss"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#robust-multi-view-clustering-with-incomplete-information",
    "href": "posts/paper-reading/multi-view-clustering.html#robust-multi-view-clustering-with-incomplete-information",
    "title": "some papers about multi-view-clustering",
    "section": "Robust Multi-view Clustering with Incomplete Information",
    "text": "Robust Multi-view Clustering with Incomplete Information\nunified framework to solve PVP and PSP\ncontrastive loss to eliminate the false negative samples"
  },
  {
    "objectID": "posts/inequality.html",
    "href": "posts/inequality.html",
    "title": "Inequality",
    "section": "",
    "text": "I found a great book on concentration inequalities\nConcentration inequalities: A nonasysmtotic theory of independence by Boucheron\nhope I can read this book thoroughly one day :/"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html",
    "href": "posts/understanding-machine-learning/UML-01.html",
    "title": "Understanding Machine Learning 01",
    "section": "",
    "text": "domain set. a set of instances \\(\\mathcal{X}\\)\nlabel set. \\(\\mathcal{Y}\\)\ntraining data. finite sequence \\(\\mathcal{S}\\) in \\(\\mathcal{X}\\times \\mathcal{Y}\\)\nlearner. output a rule, \\(h:\\mathcal{X}\\rightarrow\\mathcal{Y}\\). learning algorithm \\(\\mathcal{A(S)}\\)\ndata-generation model. ideally, we assume (for sim) there exists a \"correct\" labeling funciton, \\(f\\). and \\(\\mathcal{S}\\) is generated under an unknown distrib \\(\\mathcal{D}\\), then labeling it using \\(f\\).\nmeasurement. actually generalization error (risk or true error of \\(f\\))\n\n\\[\nL_{\\mathcal{D},f}(h):=\\mathbb{P}_{x\\sim D}[h(x)\\neq f(x)]:=\\mathcal{D}(\\{x:h(x)\\neq f(x)\\})\n\\]\nwhere \\(\\mathcal{D}\\) desc the prob of the event of observation.\n\\(\\mathcal{D}(A):=\\mathbb{P}_{x\\sim D}[\\pi(x)]\\), where \\(\\pi\\) is the char func of whether it was observed. \\(A={x\\in \\mathcal{X}:\\pi(x)=1}\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#erm-empirical-risk-minimization",
    "href": "posts/understanding-machine-learning/UML-01.html#erm-empirical-risk-minimization",
    "title": "Understanding Machine Learning 01",
    "section": "ERM (empirical risk minimization)",
    "text": "ERM (empirical risk minimization)\nuse training loss to approx generalization loss.\n\noverfitting\nwe may obtain a bunch of funcs just by ERM. so we need an inductive bias to set a preference on a certain funcs.\nwe choose the hypothesis space H before seeing the data. restric our search space of the ERM, otherwise we got a trivial useless solution.\nbefore seeing the data \\(\\rightarrow\\) should be based on some prior knowledge."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#h",
    "href": "posts/understanding-machine-learning/UML-01.html#h",
    "title": "Understanding Machine Learning 01",
    "section": "H",
    "text": "H\nexamples of H\n\nfinite H space\n\\(\\mathcal{H}\\) will not overfit provided a sufficiently large training set.\nnote: the class of axix-aligned rectangles could be finite if we consider it on a computer. (discrete repr of real numbers)\n\\(h_S\\in argmin_{h\\in H}L_s(h)\\)\nsince S are randomly chosen, so \\(h_S,L_{D,f}\\) are actually random vars.\n\n\na few assumptions\na few assumptions on the PAC learnability\n\ndef the realizability assumption\nthere exists \\(h^\\star\\in\\mathcal{H}\\) s.t. \\(L_{D,f}(h^\\star)=0\\)\nfurther, we have\n\\(\\rightarrow L_S(h^\\star)=0 \\text{ with prob 1 over the S }\\rightarrow L_S(h_S)=0\\)\nwe are interested in \\(L_{D,f}(h_S)\\)\n\n\nconfidence param\nwe address a prob \\(\\delta\\) of getting a very nonrepresentative training set (e.g all lie in class A). and \\((1-\\delta)\\) is the confidence of our prediction.\n\n\naccuracy param\n\\(\\epsilon\\)\nwe call \\(L_{D,f}(h_S)\\ge \\epsilon\\) as a failure of the learner, otherwise approx correct predictor.\nSo we're interested in the upper bound of prob to sample S that leads to the learnerâ€™s failure.\nupper bound of\n\\[\nD^m\\{S|_x:L_{D,f}(h_S)\\gt \\epsilon\\}\n\\]\nlet \\(H_B\\) be the set of bad hypotheses\n\\[\n\\{h\\in H: L_{D,f}\\gt \\epsilon\\}\n\\]\nlet M be the set of the misleading training set\n\\[\n\\{S|_x:\\exists h\\in H_B, L_S(h)=0\\}\n\\]\nwhere \\(S|_x\\) is the instances of tr set\ndue to real.. assumption, only M will cause failure.\nso only a subset of S from M will cause \\(h_S\\) to fail.\n\\[\n\\begin{align}\n&D^m\\{S|_x:L_{D,f}(h_S)\\gt \\epsilon\\} \\\\\n\\le& \\sum_{h\\in H_B}D^m\\{S|_x:L_S(h)=0\\}\\\\\n=&\\sum_{h\\in H_B}\\prod D\\{x_i:h(x_i)=f(x_i)\\}\n\\end{align}\n\\]\nhere, the countability of \\(\\mathcal{H}\\) is used, and I think if we can control the order of \\(|\\mathcal{H}|\\) and with more careful scaling (with more assumption or knowledge about the h's, like \\(D^m\\{S|_x:L_S(h)=0\\}\\) can be approx related to h) then we could have the inf conclusion, though maybe not that interesting, and there are other ways on it.\nand\n\\[\nD\\{x_i:h(x_i)=f(x_i)\\}=1-L_{D,f}(h)\\le 1-\\epsilon\n\\]\nso using a series of loose relaxation, we have\n\\[\nD^m\\{S|_x:L_{D,f}(h_S)\\}\\le |H_B|e^{-\\epsilon m}\\le |H|e^{-\\epsilon m}\n\\]\nfinite is used here\nLHS is \\(\\delta\\)\nNote sometimes m should be really large to ensure with at least \\(1-\\delta\\) confidence over the choice of S, every ERM hypothesis, \\(h_S\\) is approx correct."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#small-corollary",
    "href": "posts/understanding-machine-learning/UML-01.html#small-corollary",
    "title": "Understanding Machine Learning 01",
    "section": "small corollary",
    "text": "small corollary\nwhen hypothesis space is finite, then we can immediately have an upper bound for \\(m_{\\mathcal{H}}\\)\n\\[\nm_{\\mathcal{H}}(\\epsilon,\\delta)\\le \\left \\lceil \\frac{log\\left(|\\mathcal{H}|/\\delta\\right)}{\\epsilon} \\right \\rceil\n\\]"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-03.html",
    "href": "posts/understanding-machine-learning/UML-03.html",
    "title": "Understanding Machine Learning 03",
    "section": "",
    "text": "Before, the way of choosing \\(m_H\\) and the effect of \\(\\delta\\) was related to the learner. However, we can borrow the concept of uniform convergence from analysis to make it independent of what learner you use.\nhere, we can treat \\(L_S(h)\\) as \\(\\sum_i^nf_i(x)\\) since they are both intermediate vals during the convergence, and \\(L_D(h)\\) to be the final end\n\n\n\\(S\\) is \\(\\epsilon\\)-representation sample (w.r.t domain, \\(H\\), \\(l\\) and \\(D\\)) if\n\\[\n\\forall h\\in\\mathcal{H},\\, |L_S(h)-L_D(h)|\\le \\epsilon\n\\]\nlemma\nif S is \\(\\frac{\\epsilon}{2}\\)-representative, then \\(\\forall h_S\\in\\argmin_{h\\in\\mathcal{H}}L_S(h)\\)\n\\[\nL_D(h_S)\\le \\min_{h\\in\\mathcal{H}}L_D(h)+\\epsilon\n\\]\nthrough this lemma, we can immediately have\n\\(S\\) is \\(\\frac{\\epsilon}{2}\\)-representative with prob \\(1-\\delta\\) \\(\\implies\\) Agnostic PAC learnability\n\n\n\nH has the uniform convergence \\(\\coloneqq\\) exists a func \\(m_H^{UC}(\\epsilon, \\delta)\\), for every \\(D\\), if \\(|S|\\gt m_H^{UC}\\) then with \\(1-\\delta\\) prob, it is \\(\\epsilon\\)-representative\nIt seems stronger than the original agnostic PAC, just like the rel between uniform conv and normal conv in analysis. normal conv only cares the situation in a certain area (here the decider generated by the learner), while uni conv holds on the whole area (all \\(h\\in\\mathcal{H}\\))\n\n\n\\(m_H\\le m_H^{UC}\\) if \\(H\\) has the uni conv property"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-03.html#situation-of-the-finite-h-class",
    "href": "posts/understanding-machine-learning/UML-03.html#situation-of-the-finite-h-class",
    "title": "Understanding Machine Learning 03",
    "section": "situation of the finite H class",
    "text": "situation of the finite H class\nneed to find \\(m\\), so that\n\\[\nD^m(\\{S:\\forall h\\in \\mathcal{H},|L_S(h)-L_D(h)|\\le\\epsilon\\})\\ge 1-\\delta\n\\]\nand we may convert it into a more familiar form (convenient for using inequalities)\n\\[\nD^m(\\{S:\\exists h\\in \\mathcal{H},|L_S(h)-L_D(h)|\\gt\\epsilon\\})\\lt \\delta\n\\]\nusing union bound and Hodeffing inequalities (note that \\(L_D(h)=\\mathbb{E}_{S\\sim D^m}(L_S(h))\\)), we have\n\\[\nLHS\\le\\sum_{h\\in\\mathcal{H}}2exp(-2m\\epsilon^2)\n\\]\nas a corollary, we have the upper bound for finite hypothesis class which is agnostic PAC learnable\n\\[\nm_H^{UC}(\\epsilon,\\delta)\\le\\left\\lceil\\frac{log(2|\\mathcal{H}|/\\delta)}{2\\epsilon^2}\\right\\rceil\n\\]\n\n\n\n\n\n\nsummary\n\n\n\nif uni conv holds, then in most cases, the empirical risks of h in H will faithfully represent their true risks\n\n\n\nexercises\n\n4.1\n\n\n\\(\\forall \\epsilon,\\delta\\gt 0,\\exists m(\\epsilon,\\delta)\\,s.t.\\) \\[\n\\forall m\\ge m(\\epsilon,\\delta),\\,\\mathcal{P}_{S\\sim D^m}[L_D(A(S))\\gt\\epsilon]\\lt\\delta\\]\n\\(\\lim_{m\\to \\infty}\\mathbb{E}_{S\\sim D^m}[L_D(A(S))]=0\\)\n\n1 \\(\\iff\\) 2"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html",
    "href": "posts/understanding-machine-learning/uml-06.html",
    "title": "Understanding Machine Learning 06",
    "section": "",
    "text": "we will summarize different kinds of defs of learnability first"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#learnability",
    "href": "posts/understanding-machine-learning/uml-06.html#learnability",
    "title": "Understanding Machine Learning 06",
    "section": "learnability",
    "text": "learnability\n\nPAC learnability\nH is PAC learnable if the realizability assumption holds and\n\\(\\forall \\epsilon,\\delta\\gt 0\\), there exists a learner A and \\(m_H(\\epsilon,\\delta)\\) s.t. if \\(m\\ge m_H\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le \\epsilon\n\\]\n\n\nagnostic PAC learnability\nH is agnostic PAC learnable if\n\\(\\forall \\epsilon,\\delta\\gt 0\\), there exists a learner A and \\(m_H(\\epsilon,\\delta)\\) s.t. if \\(m\\ge m_H\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le \\min_{h\\in H}\\{L_D(h)\\}+\\epsilon\n\\]\n\n\nuniform convergence property\nH enjoys the uniform convergence property if\n\\(\\forall \\epsilon,\\delta \\gt 0\\), there exists \\(m_H^{UC}(\\epsilon, \\delta)\\) s.t. if \\(m\\ge m_H^{UC}\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\), \\(\\forall h\\in H\\)\n\\[\n|L_S(h) - L_D(h)|\\le\\epsilon\n\\]\nthese defination of learnibility are equal according to the fundamental theory\n\nhere we have another def which has different power with the above"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#nonuniform-learnability",
    "href": "posts/understanding-machine-learning/uml-06.html#nonuniform-learnability",
    "title": "Understanding Machine Learning 06",
    "section": "nonuniform learnability",
    "text": "nonuniform learnability\nwe allow \\(m_H\\) to be non-uniform over h\nH is nonuniform learnable if\n\\(\\forall \\epsilon,\\delta\\gt 0\\) there exists a learner A and \\(m_H^{NU}(\\epsilon,\\delta,h)\\), s.t. for all \\(h\\in H\\), if \\(m\\ge m_H^{NU}(\\epsilon,\\delta,h)\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le L_D(h)+\\epsilon\n\\]\nnote when m is decided, and it is that makes non-uniform learnability weaker than PAC\n\nproperty\nH is nonuniform learnable iff H can be expressed as a union of countable \\(H_i\\) with uniform convergence property\nwe can easily construct H that is nonuniform learnable but not PAC learnable which means PAC learnability is stronger\n\n\ngeneric learner\nERM is a fittable learer for PAC learnability\nSRM (structural risk minimization) is a fittable learner for NU learnability\nSRM requires us to provide more prior knowledge on the priority (weights) of \\(H_i\\)â€™s.\ndef \\(\\epsilon_n(m,\\delta)=\\min\\{\\epsilon\\in (0,1):m_{H_n}^{UC}(\\epsilon,\\delta)\\le m\\}\\) which means the minimum est error we can get with m samples\nso given m samples\n\\[\n\\forall h\\in H_n,|L_D(h)-L_S(h)|\\le \\epsilon_n(m,\\delta)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nnote m here can be varied (Iâ€™m not so sure about this) or large enough (s.t. \\(\\forall n,\\epsilon_n(m,\\delta)\\le \\epsilon\\)) during training\n\n\nwhen we put this on a larger range (\\(H_n\\rightarrow H\\)) directly, we canâ€™t garantee we satisfy the \\(\\delta\\) constraint, since each one has relatively low confidence \\(1-\\delta\\). So we have to split the confidence to each \\(H_i\\), that is providing weights \\(\\sum_{n\\in\\mathbb{N}}w(n)\\le 1\\) on each \\(\\delta\\) (use union bound inequality to merge it back)\nto put it formally\ngiven \\(H=\\cup_{n\\in\\mathbb{N}}H_n\\) and \\(\\sum_{n\\in\\mathbb{N}}w(n)\\le 1\\), where \\(H_i\\) satisfy UC property, then\nwith probatility of at least \\(1-\\delta\\) over the choice \\(S\\sim D^m\\)\nfor any \\(n\\in\\mathbb{N}\\) and \\(h\\in H_n\\)\n\\[\n|L_D(h)-L_S(h)|\\le \\epsilon_n(m,w(n)\\cdot \\delta)\n\\]\nwhich implies for \\(\\forall h\\in H\\)\n\\[\nL_D(h)\\le L_S(h)+\\min_{n:h\\in H_n}\\epsilon_n(m,w(n)\\cdot\\delta)\n\\]\nif we make it simpler (but looser), let \\(n(h)=min\\{n:h\\in H_n\\}\\)\n\\[\nL_D(h)\\le L_S(h)+\\epsilon_n(m,w(n(h))\\cdot\\delta)\n\\]\nSO, SRM is to minimizing the RHS\nwe can proof that \\(L_D(A(S))\\le L_D(h)+\\epsilon\\) with p at least \\(1-\\delta\\) over the choice of S (if \\(m\\ge m_{H_{n(h)}}^{UC}(\\epsilon/2,w(n(h))\\cdot \\delta)\\))\n\nin fact, any converged sumations should be ok for w, like \\(w(n)=\\frac{6}{n^2\\pi^2}\\)\nintuitively, \\(H_n\\) with larger \\(w(n)\\) will need less samples since it is required for less confidence, we actually focus on some hypothesis classes instead of treat all \\(H_n\\) evenly.\nsecond, if \\(h_1\\) and \\(h_2\\) has the same empirical risk, we will prefer the one with higher weight if using SRM\nseems familiar? sounds like the principle Occamâ€™s razor\n\n\ndescription length\nwe now consider a countable \\(H\\). it can be expressed as a union of singleton class \\(H_i=\\{h_i\\}\\) and for each \\(H_i\\), \\(m_{H_i}^{UC}(\\epsilon,\\delta)= \\left\\lceil\\frac{log(2/\\delta)}{2\\epsilon^2}\\right\\rceil\\)\nthen \\(e_n(m,w(n(h))\\cdot \\delta)= \\sqrt{\\frac{-logw(n(h)) +log(2/\\delta)}{2m}}\\)\ndef description language \\(\\{0,1\\}^\\star\\)\nwe assign each \\(H_i\\) with a description \\(d(h)\\) and denote \\(|h|=|d(h)|\\)\n\n\n\n\n\n\nNote\n\n\n\nif S is a prefix-free set of strings, then\n\\[\n\\sum_{\\sigma\\in S}\\frac{1}{2^{|\\sigma|}}\\le 1\n\\]\n\n\nso \\(w(h)=\\frac{1}{2^{|h|}}\\) is a legal weight function\nand the hypothesis with smaller description length is preferable if they have the same risk\nthatâ€™s the principle of Occamâ€™s razor"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#consistency",
    "href": "posts/understanding-machine-learning/uml-06.html#consistency",
    "title": "Understanding Machine Learning 06",
    "section": "consistency",
    "text": "consistency\n\n\n\n\n\n\nNote\n\n\n\nif we let \\(m_H\\) further be dependent on the distribution, we have the def of consistency\n\n\na learner A is consistency with respect to H and P where P is the set of possible distribution Dâ€™s, if\n\\(\\forall \\epsilon,\\delta\\gt 0\\) there exists a learner A and \\(m_H^{NU}(\\epsilon,\\delta,h,D)\\), s.t. for all \\(h\\in H\\) and \\(D\\in P\\), if \\(m\\ge m_H^{NU}(\\epsilon,\\delta,h,D)\\)\nwith probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le L_D(h)+\\epsilon\n\\]\n\n\n\n\n\n\nNote\n\n\n\nif P is the set of all distributions, then A is universally consistent with respect to H\n\n\nthis def of learnability is even weaker than NU\nthe algorithm Memorize is universally consistent which will be overfit in the context of PAC learnability!!! (for every countable domain and finite label set w.r.t. zero-one loss)\ndef Memorize(x)\n    return y if (x,y) in S else 0 # any default value \n\nwhy different ability?\nnote when we determine the \\(m\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#exercise",
    "href": "posts/understanding-machine-learning/uml-06.html#exercise",
    "title": "Understanding Machine Learning 06",
    "section": "exercise",
    "text": "exercise\n\n7.5. H that contains all functions is not nonuniform learnable\n\nconclusion: \\(H=\\cup_{n\\in\\mathbb{N}}H_n\\), if H shatters an infinite set, the some \\(H_n\\) has infinite VC dim"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html",
    "href": "posts/understanding-machine-learning/UML-05.html",
    "title": "Understanding Machine Learning 05",
    "section": "",
    "text": "here we go to one of the famous theories, the VC dimension\nbefore going a little deeper, we will have a look at the motivation\n\n\nfirst, finite is not a sufficient and necessary condition of PAC learnability. exercise 3.3 is a simple example. Moreover, in the last post (No-Free-Lunch theorem), we have seen H that contains all possible functions is not PAC learnable. When we rethink the proof, we may notice that the construction of set \\(C\\) is the key point. Since we are considering all possible functions, the error of different fâ€™s can cancel, resulting in a large error.\nborrow the idea, if we can find a subset \\(C\\) of domain \\(\\mathcal{X}\\), and if H contains all the functions when taking \\(C\\) as the domain, then it will cause a considerable risk using the same proof\nfurther, if such kind of \\(C\\) is infinitely large, then \\(H\\) is not learnable\nthe thoughts above give us the basic idea of how the VC dimension comes\n\n\n\n\nDef. restriction of H to C\nhere, \\(\\mathcal{C}=\\{c_1,c_2,\\dots,c_m\\}\\subseteq\\mathcal{X}\\)\nand \\(\\mathcal{H}_C=\\{h(c_1),\\dots,h(c_m)\\}\\)\n\nDef. Shattering\nH shatters C \\(\\iff\\) \\(|H_C|=2^{|C|}\\)\n\nDef. VC-dimension\nthe VC-dimension of H \\(\\coloneq\\max_C\\{|H_C|:\\text{H shatters C}\\}\\)\n\na simple method to show VCdim=d we need to show that\n\n\\(\\exists C\\) of size d that is shattered by H\nevery C of size d+1 is not shattered by H\n\n\nsince \\(2^{|VCdim(H)|}\\le |H|\\), we have a loose upper bound of VC dim which is \\(log_2(|H|)\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html#fundamental-theorem-of-pac-learning",
    "href": "posts/understanding-machine-learning/UML-05.html#fundamental-theorem-of-pac-learning",
    "title": "Understanding Machine Learning 05",
    "section": "fundamental theorem of PAC learning",
    "text": "fundamental theorem of PAC learning\nthe followings are eq\n\nuniform convergence\nPAC learnable on every ERM learner\nagnostic PAC learnable on every ERM learner\nfinite VC dimension\n\n\nfrom 4. to 1. is non-trivial. here we will go deeper into that\nwe need to find \\(m_H^U\\) s.t. for any S that \\(|S|\\ge m_H^U\\) is \\(\\frac{\\epsilon}{2}\\)-representative (i.e.Â \\(L_D(h)-L_S(h)|\\le \\frac{\\epsilon}{2}\\))\nintuitively, if \\(m\\) is larger than \\(d\\), where \\(d\\) is the VC dimension, \\(H_C\\) will only take small part of \\(2^C\\) (in fact that is polynomial large w.r.t. \\(|C|\\)), the estimation error could be bounded by \\(o(m)\\)\n\nSauerâ€™s Lemma\n\nDef. Growth Function\n\\[\\tau_H(m)=\\max_{C\\subset \\mathcal{X}:|C|=m}|H_C|.\\]\n\naccording to Sauerâ€™s lemma, \\(\\tau_H(m)\\le \\sum_{i=0}^d\\tbinom{n}{i}\\), if \\(m\\gt d+1\\), we have a looser but neat form \\(\\tau_H(m)\\ge (em/d)^d\\)\n\nproof\nbasic idea\nwe prove a stronger claim, \\(\\forall C=\\{c_1,\\dots,c_m\\}\\)\nwe have \\(\\forall H,\\ |H_C|\\le |\\{B\\subseteq C: \\text{H shatters B}\\}|\\)\ninduction on m\nwhen m=1, both sides are equal\nwhen m=k+1, suppose the claim holds for \\(m\\le k\\).\nto use the claim, we need to split \\(C\\) as \\(\\{c_1\\}\\) and ${c_1,, c_m} which is denoted as \\(C'\\)\nwe need to find such \\(H'\\) that can naturally convert from \\(C'\\) to \\(C\\) but still holds the shattering property\nnote:\n\\[\nH_C=H_{C'}\\oplus H'_{C'}\n\\]\nwhere \\(H'_{C'}=\\{\\exists f(c_1)=1\\land g(c_1)=0\\land f_{C'}=g_{C'}, \\text{ where }f,g\\in H\\}\\), \\(\\oplus\\) means direct sum (\\(H_{C'}\\cap H'_{C'}=\\emptyset\\))\n\nif \\(f_{C'}\\) and \\(g_{C'}\\) are exactly the same, they represent the same function in \\(H_{C'}\\), and hence only count once in \\(H_{C'}\\) which should be treated separatedly in \\(H_C\\).\n\nso we have that\n\\[\n\\begin{aligned}\n    |H_C|&=|H_{C'}|+|H'_{C'}|\\\\\n    &\\le |\\{B\\subseteq C':\\text{H shatters B}\\}|+|\\{B\\subseteq C':\\text{H' shatters B}\\}|\n\\end{aligned}\n\\]\nnote that \\(H'\\) shatters B \\(\\iff\\) H shatters \\(\\{c_1\\}+B\\)\nso RHS \\(\\le |\\{B\\subseteq C:\\text{H shatters B}\\}|\\) \\(\\blacksquare\\)\n\n\n\nuniform convergence\nwe will give the upper bound without proof :(\nfor every \\(\\delta\\), with prob \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\n|L_D(h)-L_S(h)|\\le \\frac{4+\\sqrt{log(\\tau_H(2m))}}{\\delta \\sqrt{2m}}\n\\]\nRHS \\(\\in o(m)\\)\n\nIâ€™ll add the proof when I master this math skill :("
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html#exercise",
    "href": "posts/understanding-machine-learning/UML-05.html#exercise",
    "title": "Understanding Machine Learning 05",
    "section": "exercise",
    "text": "exercise\n\n6.1. monotonicity of VC dim\n6.2. finite X, \\(k\\lt |\\mathcal{X}|\\), compute the VC dim\n\n\\(H_{=k}^\\mathcal{X}=\\{h\\in \\{0,1\\}^\\mathcal{X}:|x:h(x)=1|=k\\}\\). VC dim=\\(\\min\\{k,|\\mathcal{X}|-k\\}\\), otherwise you canâ€™t assign all one (zero) to these instances\n\\(H_{\\text{at most k}}^\\mathcal{X}=\\{|x:h(x)=1|\\le k \\text{ or }|x:h(x)=0|\\le k\\}\\). VC dim=k.\n\n6.3. parity function \\(h_I\\) (h computes the parity of bits at \\(I\\) ). finte H, \\(d\\le n\\), and easy to construct such \\(C\\) that \\(|C|=n\\)\n6.4. skip\n6.5. The VC dim of axis-aligned rectangles in d-dim is 2d. Since we need 2d points to construct a rectangle that assigns all points as one if there are 2d+1 points, one of them must stay inside the box (or lie on the border with no points in the box). With that point assigned as zero, others assigned as one, no function will satisfy\n6.6. VC-dimension of Boolean conjuntions \\(H_{con}^d\\): \\(f(x)=x_{i_1}\\land \\dots \\land x_{i_k}\\)\n\nprove \\(|H_{con}^d|\\le 3^d + 1\\). each \\(x_i\\) has three states, not chosen, origin \\(x_i\\), inverse \\(\\bar{x}_i\\), so should be \\(= 3^d\\) ??\nprove \\(VCdim(H)\\le d log 3\\). \\(VDdim\\le log(|H|)=dlog3\\)\nshow that \\(H_{con}\\) shatters \\(\\{e_i:i\\le d\\}\\), seems easy\nshow that \\(VCdim(H)\\le d\\). if \\(d+1\\), consider hypothesis \\(h_i(c_j) = \\begin{cases}  0 &\\text{if } i=j \\\\  1 &\\text{otherwise}  \\end{cases}\\) .that means each \\(c_i\\) has at least one bit that is different from others, which is a contradiction\n\\(H_{mcond}^d\\) which do not contain negations, empty conjunction is considered as all positive, the vc dim of \\(H_{mcond}^d\\) with all negative h =d.Â first \\(VCdim(H_{mcond})\\le d\\). consider \\(x_i\\) is all one but zero at index i, \\(i\\le d\\)\n\n6.7. skip\n6.8. show vcdim of \\(H=\\{x\\mapsto \\lceil sin(\\theta x) \\rceil: \\theta \\in \\mathbb{R}\\}\\) is infinity.\n\nhint: if \\(0.x_1x_2x_3\\dots\\) is the binary expansion of \\(x\\in (0,1)\\), then for any natual number m, \\(\\lceil sin(2^m\\pi x) \\rceil =(1-x_m)\\), provided that \\(\\exist k\\ge m\\) s.t. \\(x_k=1\\).\nwith the hint, that is easy. Just construct a huge matrix with each row representing a binary expansion of \\(x_i\\) and the columns running through all possible combinations.\n\n6.9. skip\n6.10. skip\n6.11. VC of union. \\(H_1,\\dots,H_r\\).\n6.12. Dudley classes.\n\n\\(VCdim(POS(\\mathcal{F}+g))=VCdim(POS(\\mathcal{F}))\\). â€œ\\(\\mathbb{R}ightarrow\\)â€, \\((f_1+g)-(f_2+g)\\).\n\\(VCdim(POS(\\mathcal{F}))=\\dim(\\mathcal{F})\\). half space, full rank, bla bla bla\nexamples of Dudley classes\n\n\nbtw, Dudley class seems to be a fascinating and important topic"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html",
    "href": "posts/understanding-machine-learning/UML-02.html",
    "title": "Understanding Machine Learning 02",
    "section": "",
    "text": "def PAC learnability\nhypothesis H is PAC learnable if realizability assumption holds, and exists \\(m_H(\\epsilon,\\delta)\\rightarrow\\mathbb{N}\\)\nwhere with #i.i.d. sample \\(\\ge m_H\\), we always have a probability approx correct h just using ERM."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#removing-realizability-assumption",
    "href": "posts/understanding-machine-learning/UML-02.html#removing-realizability-assumption",
    "title": "Understanding Machine Learning 02",
    "section": "removing realizability assumption",
    "text": "removing realizability assumption\nthe realizability assumption is too strong and unrealistic where we assume the existence of a perfect hypothesis from \\(\\mathcal{H}\\) that can reveal ground truth \\(f\\) with \\(Pr=1\\)\nso, change \\(f(x)\\) into a joint distrib \\(D(x,y)\\) as most researchers would do\ndef generalized risk as \\(L_D(h)\\coloneqq \\mathcal{P}_{(x,y)\\sim D}[h(x)\\neq y]\\coloneqq D(\\{(x,y):h(x)\\neq y\\})\\), just changed \\(D\\) into a joint distrib\nempirical risk is the same\nstill, when take \\(D\\) to be a uniform distrib on \\(S\\) they are eq\nideally, func â€œ\\(f_D(x)=1\\text{ if }Pr[y=1|x]\\ge 0.5\\text{ and 0 otherwise}\\)â€ is the optimal sol to the gen risk min problem when \\(\\mathcal{Y}=\\{0,1\\}\\), w.r.t 0-1 loss. Bayes optimal sol.\n\nnote: my stupid short proof about the above\nwe need to proof\n\\[\n[f(x)\\neq 0]Pr(y=0|x)+[f(x)\\neq 1]Pr(y=1|x)\\\\\n\\le [g(x)\\neq 0]Pr(y=0|x)\\dots\n\\]\njust consider cond on \\(Pr(y=0|x)\\gt 0.5\\), and it becomes\n\\[\nLHS=Pr(y=1|x)\\\\\n=\\{[g(x)\\neq 0]+[g(x)\\neq 1]\\}Pr(y=1|x)\\\\\n\\le RHS\n\\]\nvery ugly and not clever proof :/ \\(\\blacksquare\\)\n\nhere, we still have the opt function \\(f\\), which minimizes the gen risk but does not minimize it to zero"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#def-agnosticä¸å¯çŸ¥è®ºçš„-pac-learnability",
    "href": "posts/understanding-machine-learning/UML-02.html#def-agnosticä¸å¯çŸ¥è®ºçš„-pac-learnability",
    "title": "Understanding Machine Learning 02",
    "section": "def Agnostic(ä¸å¯çŸ¥è®ºçš„) PAC learnability",
    "text": "def Agnostic(ä¸å¯çŸ¥è®ºçš„) PAC learnability\nbefore: \\(L_{D,f}(h)\\le \\epsilon\\), now: \\(L_{D}(h)\\le min_{g\\in\\mathcal{H}}L_{D}(g)+\\epsilon\\)\nso \\(f\\) above will not be used, but the joint distrib \\(D\\) instead\nand we see that if the realizability assumption holds, itâ€™s the same as the original PAC learnability\nagnostic just means we canâ€™t obtain an h with arbitrary small gen risk\nrelative best instead of abs best"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#other-loss-functions",
    "href": "posts/understanding-machine-learning/UML-02.html#other-loss-functions",
    "title": "Understanding Machine Learning 02",
    "section": "other loss functions",
    "text": "other loss functions\nwe can actually use other measures in place of the 0-1 loss, especially in regression problems\nnaturally, we can extend the loss function into a more formal definition, \\(l:\\mathcal{H}\\times\\mathcal{Z}\\rightarrow \\mathbb{R}_+\\), where \\(\\mathcal{Z}\\) is the set of instances, in prediction tasks, it could be \\(\\mathcal{X}\\times\\mathcal{Y}\\)\n\nAgnostic PAC learnability for General Loss Functions\n\\(L_D(h)\\le\\min_{h'\\in\\mathcal{H}}L_D(h')+\\epsilon\\)\nwhere \\(L_D(h)=\\mathcal{E}_{z\\sim D}[l(h,z)]\\)\nnote: \\(l(h,\\dot)\\) is a rand var, it should be measurable.."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#some-exercises",
    "href": "posts/understanding-machine-learning/UML-02.html#some-exercises",
    "title": "Understanding Machine Learning 02",
    "section": "some exercises",
    "text": "some exercises\n\n3.1. about monotonicity of \\(m_\\mathcal{H}\\) on \\(\\epsilon\\) and \\(\\delta\\) respectively: seems trivial\n3.2. about \\(\\mathcal{H}_{singleton}\\): first, you need to come up with a simple learning alg. if no pos samples appear, just choose \\(h^-\\). this is enough to prove the PAC learnability, as m is large enough, the cost will be small enough\n3.3. about \\(\\mathcal{H}\\) consist of disks. similar to the rect situation from this chap, but simpler, â€™cause itâ€™s convenient to construct how the bad samples look like\n3.4. about \\(\\mathcal{H}\\) consist of boolean conjunctions. similar to 3.2. itâ€™s easy to determine f if \\(S\\) contains a positive sample. Otherwise, we just return an all-negative hypothesis\n3.5. about samples from i. but not i.d. from the derivation of the finite hypo corollary, the key is to deal with \\(\\prod_i \\mathcal{P}_{x\\in D_i}(x_i|h(x_i)=f(x_i))\\), where the GA mean ineq could be used to make it as \\((1-\\epsilon)^m\\)\n3.6. ??, since you added the realizability assumption, itâ€™s naive to have the PAC learnability, maybe?\n3.7. about ideally opt sol of binary classification (w.r.t 0-1 loss). see this\n3.8.1. same question as above, but use abs loss, and consider probabilistic hypothesis (outputs a distribution instead of the deterministic answer), method from here seems enough?\n3.8.2. prove the existence of a learning algo that is better than any others provided \\(D\\). ??? an algo from God that can directly output \\(f\\) from the previous question -_-\n3.8.3. for every learning algo A from \\(D\\), there always exists a B and \\(D'\\) that A is not better than B on \\(D\\). shit, this is also naive from 3.8.2\n3.9. about a variant of PAC learnability, which uses a two-oracle model that allows learner access to \\(D^+\\) and \\(D^-\\) on its preference. the learner can actually change the popularity of the observances, e.g.Â a learner can take samples from both pos and neg with equal probability (I donâ€™t understand here so much, does )\n\n\nproof PAC in the standard model \\(\\implies\\) PAC in the two-oracle model. we need to construct a learner from the one in the standard model. suppose the learner puts equal weights on pos and neg samples. It learns a population with equal pos and neg using a standard model. denote the new distribution as \\(D'\\), so\n\n\n\\[\\begin{aligned}\nL_{D'}(h)&=P_{D'}[h\\neq f,f=0]+P_{D'}[h\\neq f, f=1]\\\\\n&=P_{D'}[f=0]P_D[h\\neq f|f=0]+P_{D'}[f=1]P_D[h\\neq f|f=1]\\\\\n&=\\frac{1}{2} L_{D^+}(h)+\\frac{1}{2}L_{D^-}(h)\n\\end{aligned}\n\\]\n\n\nproof PAC in the two-oracle model \\(\\implies\\) PAC in the standard model if \\(h^+,h^-\\in\\mathcal{H}\\). if we have enough samples (\\(m_H\\)), it will contain \\(m^+\\) pos, and \\(m^-\\) neg with high prob, and even it fails to have enough pos (or neg), we can just return \\(h^-\\) ((\\(h^+\\)) and wonâ€™t cost much risk."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-04.html",
    "href": "posts/understanding-machine-learning/UML-04.html",
    "title": "Understanding Machine Learning 04",
    "section": "",
    "text": "What is a universal learner? It means it doesnâ€™t need any prior knowledge. To be more specific, \\(\\mathcal{H}\\) that contains all possible functions is PAC-learnable. ? that is certainly not true since this is about the NFL theorem.\nTypically, prior knowledge could be assumptions of knowing \\(\\mathcal{D}\\) comes from some family of distribution or assuming \\(\\exists h\\in\\mathcal{H}\\) that \\(L_D(h)\\) is small enough.\n\n\nwe consider binary classification problem and 0-1 loss over \\(\\mathcal{X}\\)\n\\(m\\) be the tr set size smaller than \\(\\frac{|\\mathcal{X}|}{2}\\), A be any learner\nthere is a D that\n\nthere exists \\(f\\), with zero error on \\(\\mathcal{D}\\)\nwith a probability of at least 1/7 over the choice of \\(S\\sim \\mathcal{D}^m\\) we have the error of \\(A(S)\\ge 1/8\\)\n\nLetâ€™s prove this famous theorem!!!\n\nlet C be a subset of \\(\\mathcal{X}\\) with size 2m as the finite â€˜domainâ€™ we will consider\nthe intuition is that since the learner can only see at most half of the domain set, we can then make the other part of distribution anything we want to defeat A\nwe need to find the D and f\nthere could be like \\(2^{2m}\\) possible fâ€™s; we can prove that\n\\[\n\\max_{D,f}\\mathbb{E}_{S\\sim D^m}[L_{D,f}(A(S))]\\ge 1/4\n\\]\nwhich means there exists some bad f that will make A fail\nnote D and f should be matched, so we construct \\(D_i\\) as follows for each \\(f_i\\), \\(i=1\\dots T\\) where \\(T=2^{2m}\\)\n\\[\nD_i(x,y) = \\begin{cases}\n            1/|C| &\\text{if } f_i(x)=y \\\\\n            0 &\\text{otherwise}\n           \\end{cases}\n\\]\nsince \\(C\\) is finite, we can enumerate all S\ndenote as \\(S_i\\), \\(i=1\\dots M\\) where \\(M=(2m)^m\\), note instances in S can be duplicated\n\\[\n\\begin{aligned}\n   &\\max_i\\mathbb{E}_{S\\sim D_i}[L_{D_i}(A(S))]\\\\\n   &=\\frac{1}{M}\\max_i\\sum_j^ML_{D_i}(A(S_j^i))\\\\\n   &\\ge\\frac{1}{MT}\\sum_i^T\\sum_j^ML_{D_i}(A(S_j^i))\\\\\n   &=\\frac{1}{TM}\\sum_j^M\\sum_i^TL_{D_i}(A(S_j^i))\\\\\n   &\\ge\\frac{1}{T}\\min_j\\sum_{i}^{T}L_{D_i}(A(S_j^i))\n\\end{aligned}\n\\]\nhere the \\(i\\) in \\(S_j^i\\) means the label is assigned by \\(D_i\\), hence \\(f_i\\)\nWe now convert the problem of finding the \\(f\\) into \\(S_j^i\\) that has a significant error averaged on all functions. Intuitively, since we consider all functions, they must have some disagreement on the samples\nfor \\(S_j^i\\), we let \\(P=\\{x_i\\in C|x_i\\notin S_j^i\\}\\) and \\(p=|P|\\), where \\(p\\ge m\\)\n\\[\n\\begin{aligned}\n    &L_{D_i}(A(S_j^i))\\\\\n    =&\\frac{1}{2m}\\sum_k^{2m}\\mathbf{1}[f_i(x_k)\\neq A(S_j^i)(x_k)]\\\\\n    \\ge&\\frac{1}{2p}\\sum_{x\\in P}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n\\end{aligned}\n\\]\nthen\n\\[\n\\begin{aligned}\n    &\\frac{1}{T}\\sum_{i}^{T}L_{D_i}(A(S_j^i))\\\\\n    \\ge&\\frac{1}{T}\\sum_i^T\\frac{1}{2p}\\sum_{x\\in P}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n    =&\\frac{1}{2}*\\frac{1}{p}\\sum_{x\\in P}\\frac{1}{T}\\sum_{i}^{T}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n\\end{aligned}\n\\]\nthere are details here, note we have removed out the instances that are in \\(S_j\\) (no \\(i\\) here), that is because \\(\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\) will always be zero if \\(x\\in S_j\\) and we can certainly not consider them\nsecond, since we take \\(f\\) from func space that contains all possible funcs, so they can be separated into pairs that can cancellate each other\nthere must always exist a pair of fâ€™s (\\(f_a\\) and \\(f_b\\)) that only differ on one instance \\(x\\) which is not in \\(S_j\\) (\\(x\\in P\\))\ns.t.\n\\(\\mathbf{1}[f_a(x)\\neq A(S_j^a)(x)]+\\mathbf{1}[f_b(x)\\neq A(S_j^b)(x)]=1\\)\nthen the ave above actually is \\(\\frac{1}{4}\\)\nnow we have proved that\n\\[\n\\max_{D,f}\\mathbb{E}_{S\\sim D^m}[L_{D,f}(A(S))]\\ge 1/4\n\\]\nuse some prob inequality can give us the conclusion that \\(\\exists D\\) \\[\n\\mathbb{P}_{S\\sim D^m}[L_D(A(S))\\ge1/8]\\ge1/7\n\\]\n\nusing the def, \\(H\\) that contains every possible h is not PAC learnable"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-04.html#bias-complexity-trade-off",
    "href": "posts/understanding-machine-learning/UML-04.html#bias-complexity-trade-off",
    "title": "Understanding Machine Learning 04",
    "section": "bias-complexity trade-off",
    "text": "bias-complexity trade-off\nUsually, we see a bias-variance trade-off. I think there may be some connection, but I havenâ€™t seen it\nthe idea is we divide the true risk as below\n\\[\n\\mathcal{L}_D(h)=\\epsilon_{app}+\\epsilon_{est}\n\\]\nwhere \\(\\epsilon_{app}=\\min_{h'\\in H}L_D(h')\\) and \\(\\epsilon_{est}=\\mathcal{L}_D(h)-\\epsilon_{app}\\)\n\n\\(\\epsilon_{app}\\), approximation error, it measures how well your hypothesis space is\n\\(\\epsilon_{est}\\), estimation error, it measures how well your learner can estimate the best h (\\(\\in H\\)) using ERM\n\napprox error has nothing to do with how you train with the dataset. if your hypothesis space is well enough (or large enough), it will be small\nest error is on the opposite. It really depends on the learner, sample size so on. Itâ€™s similar to the \\(\\epsilon\\) in the definition of agnostic PAC learnability. If your h space is too large (too complicated), then it will need more samples to decrease the est error.\nso, large \\(|H|\\) will reduce approx error, but it may be hard to have a small est error (overfitting), with a high probability a tr set will cause a bad generalization ability\non the contrary, small \\(|H|\\) indeed will give us a small est error but will cause a large approximate error (underfitting)"
  },
  {
    "objectID": "posts/test/example.html",
    "href": "posts/test/example.html",
    "title": "my first post",
    "section": "",
    "text": "This is a test for the \\(\\LaTeX\\) rendering.11Â this is a note\nIt successes with a little effort !!!"
  },
  {
    "objectID": "posts/test/index.html",
    "href": "posts/test/index.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see FigureÂ 1. and SectionÂ 2. and EquationÂ 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigureÂ 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/test/index.html#sec-two",
    "href": "posts/test/index.html#sec-two",
    "title": "Quarto Basics",
    "section": "equations and callouts",
    "text": "equations and callouts\nnothing\nEinsteinâ€™s theory of special relatively that expresses the equivalence of mass and energy:\n\\[\nE = mc^{2}\n\\tag{1}\\]\n\n\n\n\n\n\nmy note\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important."
  },
  {
    "objectID": "posts/test/index.html#cite-at-the-margin",
    "href": "posts/test/index.html#cite-at-the-margin",
    "title": "Quarto Basics",
    "section": "cite at the margin",
    "text": "cite at the margin\nColorbars indicate the quantitative extent of image data. Placing in a figure is non-trivial because room needs to be made for them. The simplest case is just attaching a colorbar to each axes:1.1Â See the Matplotlib Gallery to explore colorbars further\nColorbars indicate the quantitative extent of image data. Placing in a figure is non-trivial because room needs to\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axs = plt.subplots(2, 2)\nfig.set_size_inches(20, 8)\ncmaps = ['RdBu_r', 'viridis']\nfor col in range(2):\n    for row in range(2):\n        ax = axs[row, col]\n        pcm = ax.pcolormesh(\n          np.random.random((20, 20)) * (col + 1),\n          cmap=cmaps[col]\n        )\n        fig.colorbar(pcm, ax=ax)\nplt.show()"
  },
  {
    "objectID": "posts/programming/pytorch-notes.html",
    "href": "posts/programming/pytorch-notes.html",
    "title": "PyTorch Notes",
    "section": "",
    "text": "x.numel() number of elements in x\nbroadcast mechanism\nY = Y + X would allocate a new space for the result of Y + X. Y then is the reference to this new memory. Y += X or Y[:] = Y + X is better. (id(Y) will be unchanged.)\nfloat() or int() converts a scalar tensor to a standard number, similar to x.item()\nA * B element-wise product.\nA_sum = A.sum(axis=1, keepdims=True) will keep the number of axis. A / A_sum will enable broadcast mechanism.\nA.cumsum(axis=0) accumulated sum.\nlinear algebra (matrix, vector) multiplication\n\ndot product: torch.dot(x, y) only accept 1-D tensor.\nmatrix-vector multiplication: torch.mv(A, x) matrix and vector. x is 1-D vector. All in column vectors.\nmatrix-matrix multiplication: torch.mm(A, B).\n\ntorch.distributions\n\nmultinomial.Multinomial(n, probs).sample(N, )"
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#gadget-functions-pandas",
    "href": "posts/programming/pytorch-notes.html#gadget-functions-pandas",
    "title": "PyTorch Notes",
    "section": "gadget functions (pandas)",
    "text": "gadget functions (pandas)\n\ndata.fillna(data.mean())\npd.get_dummies(data, dummy_na=True) Convert categorical variable into dummy/indicator variables. Works for str."
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#gadget-functions-d2l",
    "href": "posts/programming/pytorch-notes.html#gadget-functions-d2l",
    "title": "PyTorch Notes",
    "section": "gadget functions (d2l)",
    "text": "gadget functions (d2l)\n\nd2l.plot example\n\nx = np.arange(0, 3, 0.1) d2l.plot(x, [x ** 2, 2 * x - 1], 'x', 'f(x)', legend=['f(x)', 'Tangent'])"
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#plot",
    "href": "posts/programming/pytorch-notes.html#plot",
    "title": "PyTorch Notes",
    "section": "plot",
    "text": "plot\n\nexample\n\nplt.plot(y, label=(\"label\"))\nplt.axhline(y=0, color='black', linestyle='dashed')\nplt.gca().set_xlabel('x label')\nplt.gca().set_ylabel('y label')\nplt.legend()"
  },
  {
    "objectID": "posts/programming/vim_tech.html",
    "href": "posts/programming/vim_tech.html",
    "title": "vim techs",
    "section": "",
    "text": "add key mapping yourself\nsave below as xxx.vim in /path/to/nerdtree/nerdtree_plugin/xxx.vim\nyy for abs path of current node, yr for rel path\n```{vimscript}\ncall NERDTreeAddKeyMap({\n        \\ 'key': 'yy',\n        \\ 'callback': 'NERDTreeYankFullPath',\n        \\ 'quickhelpText': 'put full path of current node into the default register' })\n\nfunction! NERDTreeYankFullPath()\n    let n = g:NERDTreeFileNode.GetSelected()\n    if n != {}\n        call setreg('\"', n.path.str())\n    endif\n    call nerdtree#echo(\"Node full path yanked!\")\nendfunction\n\ncall NERDTreeAddKeyMap({\n        \\ 'key': 'yr',\n        \\ 'callback': 'NERDTreeYankRelativePath',\n        \\ 'quickhelpText': 'put relative path of current node into the default register' })\n\n\nfunction! NERDTreeYankRelativePath()\n    let n = g:NERDTreeFileNode.GetSelected()\n    if n != {}\n        call setreg('\"', fnamemodify(n.path.str(), ':.'))\n    endif\n    call nerdtree#echo(\"Node relative path yanked!\")\nendfunction\n```"
  },
  {
    "objectID": "posts/programming/vim_tech.html#add-sth.-surround",
    "href": "posts/programming/vim_tech.html#add-sth.-surround",
    "title": "vim techs",
    "section": "add sth. surround",
    "text": "add sth. surround\nys takes a motion or text object, and then the char you want to put surround with\nlike ysiw\" surr current word with \""
  },
  {
    "objectID": "posts/notes-about-technical-writing.html",
    "href": "posts/notes-about-technical-writing.html",
    "title": "Notes about Technical Writing",
    "section": "",
    "text": "extra spaces between words and extra blank lines between passages will be ignored.\n\nspaces in the front will be ignored.\nnewline is regarded as an extra space, adding % at the end will remove it.\nnote: space before macros will not be ignored, e.g., \\TeX ing. But spaces at the rear will be ignored. Adding brankets {} will resolve it, e.g., {\\TeX} ing or \\Tex{} ing.\n\n\\qquad: as wide as about two M s\ndouble hyphen: -- denotes number range, en dash; triple hyphen: --- denotes punctuation dash, em dash. en and em denotes the width\nfloat environment: e.g., figure, table. accept an optional arg (h: here; t: top; H: here and not float, extended from package float)\n\\eqref is specified for math equations from package amsmath\navoid setting font or controlling indents, etc., within the document environment. try to replace them with meaningful commands or environments.\n\n\\newenvironment{myenv}\n\\newcommand\n\nellipsis: \\ldots or \\dots. instead of \\cdots or ...(three dots).\n\nH\\dots. (OK)\nH \\ldots H (not good), H $\\ldots$ H (recommended, math env)\n\nescape chars in the main body.\n\n\\_ for _; \\textbackslash for \\\n\nnon-breakable space.\n\nQuestion~1\nDonald~E. Knuth within names\nMr.~Knuth\nfunction~$f(x)$\n1,~2, and~3\n\nperiod after capital is regarded as a abbreviation. use \\ or \\@ to resolve it. E.g., Roman number XII\\@. Yes.\nBibTeX\n\n\\citep (index) and \\citet (author) are recommended. remember to use package natbib and use plainnat bibliography style\n\nforce line break\n\n\\\\ accepts an optional argument for vertical space. e.g., \\\\[2cm]. Often used for in equation environment\n\nspecial chars used in main body\n\nÂ§: \\S\n: \\dag\n: \\ddag\nÂ¶: \\P\nÂ©: \\copyright\nÂ£: \\pounds\n\\(\\bullet\\): \\textbullet (\\bullet in math env)\ngo check the book The Comprehensive LATEX Symbol List, 2009 by Scott Pakin for more"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-family",
    "href": "posts/notes-about-technical-writing.html#font-family",
    "title": "Notes about Technical Writing",
    "section": "font family",
    "text": "font family\n\nroman family: \\textrm{font family} font family\nsans-serif family: \\textsf{font family} font family\ntypewritter family: \\texttt{font family} font family\n\ndeclaration, {\\rmfamily font family} font family"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-shape",
    "href": "posts/notes-about-technical-writing.html#font-shape",
    "title": "Notes about Technical Writing",
    "section": "font shape",
    "text": "font shape\n\nupright shape: \\textup{Font Shape} Font Shape\nitalic shape: \\textit{Font Shape} Font Shape\nslanted shape: \\textsl{Font Shape} Font Shape\nsmall captical shape: \\textsc{Font Shape} Font Shape\n\ndeclaration: {\\itshape Font Shape} Font Shape"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-series",
    "href": "posts/notes-about-technical-writing.html#font-series",
    "title": "Notes about Technical Writing",
    "section": "font series",
    "text": "font series\n\nmedium series: \\textmd{font series} font series (default in main body)\nbold series: \\textbf{font series} font series\n\ndeclaration: {\\mdseries font series} font series"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-typeface",
    "href": "posts/notes-about-technical-writing.html#font-typeface",
    "title": "Notes about Technical Writing",
    "section": "font typeface",
    "text": "font typeface\n\nThe default font family of \\(\\LaTeX\\) is Computer Modern\nSerif Times Roman (i.e., Times New Roman) is recommended for papers, magazines and books. Use package txfonts\nConcrete is recommended for presentation. package combination ccfonts, eulervm is great. (also arec ,cmbright)\n\nwe can specify three font family individually\n\\usepackage{fontspec}\n\\setmainfont{Times New Roman}\n\\setsansfont{Verdana}\n\\setmonofont{Courier New}"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#emphasis",
    "href": "posts/notes-about-technical-writing.html#emphasis",
    "title": "Notes about Technical Writing",
    "section": "emphasis",
    "text": "emphasis\nmake upright or make italic upright\n\nthis is \\emph{emphasis}: this is emphasis\n\\textit{this is \\emph{emphasis}} this is emphasis**\n\nunderline\n\\underline{Emphasized} text and \\underline{another}: Emphasized text and another\nuse package \\usepackage[normalem]{ulem}\n\\uline{Emphasized} text and \\uline{another}: Emphasized text and another"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-size",
    "href": "posts/notes-about-technical-writing.html#font-size",
    "title": "Notes about Technical Writing",
    "section": "font size",
    "text": "font size"
  },
  {
    "objectID": "posts/english/vocab-example.html",
    "href": "posts/english/vocab-example.html",
    "title": "vocabs with simple examples",
    "section": "",
    "text": "some words with explanations (in Eng.) and simple examples\nall the contents are from Combridge Dictionary, I just did some picking up"
  },
  {
    "objectID": "posts/english/vocab-example.html#relate",
    "href": "posts/english/vocab-example.html#relate",
    "title": "vocabs with simple examples",
    "section": "relate",
    "text": "relate\nto be able to understand a situation or someoneâ€™s feelings because you have experienced something similar yourself:\n\nI often wake very early - Iâ€™m sure most readers over 50 can relate\n\nto tell a story or describe a series of events:\n\nShe related the events of the previous week to the police."
  },
  {
    "objectID": "posts/english/vocab-example.html#persist",
    "href": "posts/english/vocab-example.html#persist",
    "title": "vocabs with simple examples",
    "section": "persist",
    "text": "persist\nIf an unpleasant feeling or situation persists, it continues to exist:\n\nIf the pain persists, consult a doctor.\nThe cold weather is set to persist throughout the week.\n\nto try to do or continue doing something in a determined but often unreasonable way:\n\nIf he persists in asking awkward questions, then send him to the boss"
  },
  {
    "objectID": "posts/english/vocab-example.html#holistically",
    "href": "posts/english/vocab-example.html#holistically",
    "title": "vocabs with simple examples",
    "section": "holistically",
    "text": "holistically\nin a way that deals with or treats the whole of something or someone and not just a part:\n\nThe problem needs to be addressed holistically.\nNutrition is being viewed more holistically as a health issue."
  },
  {
    "objectID": "posts/english/vocab-example.html#entice",
    "href": "posts/english/vocab-example.html#entice",
    "title": "vocabs with simple examples",
    "section": "entice",
    "text": "entice\nto persuade someone to do something by offering them something pleasant:\n\nThe adverts entice the customer into buying things they donâ€™t really want.\nPeople are being enticed away from the profession by higher salaries elsewhere.\nA smell of coffee in the doorway enticed people to enter the shop."
  },
  {
    "objectID": "posts/english/vocab-example.html#embrace",
    "href": "posts/english/vocab-example.html#embrace",
    "title": "vocabs with simple examples",
    "section": "embrace",
    "text": "embrace\nto accept something enthusiastically:\n\nThis was an opportunity that he would embrace.\n\nto hold someone tightly with both arms to express love, liking, or sympathy, or when greeting or leaving someone:\n\nShe saw them embrace on the station platform.\n\nto include something, often as one of a number of things:\n\nLinguistics embraces a diverse range of subjects such as phonetics and stylistics."
  },
  {
    "objectID": "posts/english/vocab-example.html#bolster",
    "href": "posts/english/vocab-example.html#bolster",
    "title": "vocabs with simple examples",
    "section": "bolster",
    "text": "bolster\nto support or improve something or make it stronger:\n\nMore money is needed to bolster the industry."
  },
  {
    "objectID": "posts/english/vocab-example.html#surveillance",
    "href": "posts/english/vocab-example.html#surveillance",
    "title": "vocabs with simple examples",
    "section": "surveillance",
    "text": "surveillance\nthe careful watching of a person or place, especially by the police or army, because of a crime that has happened or is expected:\n\nThe police have kept the nightclub under surveillance because of suspected illegal drug activity."
  },
  {
    "objectID": "posts/english/vocab-example.html#antibiotic",
    "href": "posts/english/vocab-example.html#antibiotic",
    "title": "vocabs with simple examples",
    "section": "antibiotic",
    "text": "antibiotic\na medicine that can destroy harmful bacteria or limit their growth\n\nIâ€™m taking antibiotics for a throat infection."
  },
  {
    "objectID": "posts/english/vocab-example.html#radiate",
    "href": "posts/english/vocab-example.html#radiate",
    "title": "vocabs with simple examples",
    "section": "radiate",
    "text": "radiate\nto produce heat and light\n\nThe planet Jupiter radiates twice as much heat from inside as it receives from the Sun.\n\nto show an emotion or quality, or (of an emotion or quality) to be shown or felt:\n\nHe was radiating joy and happiness."
  },
  {
    "objectID": "posts/english/vocab-example.html#voyage",
    "href": "posts/english/vocab-example.html#voyage",
    "title": "vocabs with simple examples",
    "section": "voyage",
    "text": "voyage\na long journey, especially by ship\n\nHe was a young sailor on his first sea voyage.\n\nto travel\n\nIn their little boat they planned to voyage to distant lands."
  },
  {
    "objectID": "posts/english/vocab-example.html#invert",
    "href": "posts/english/vocab-example.html#invert",
    "title": "vocabs with simple examples",
    "section": "invert",
    "text": "invert\nto turn sth. upside down or change the order of two things\n\nIn some languages, the word order in questions is inverted"
  },
  {
    "objectID": "posts/english/vocab-example.html#constitution",
    "href": "posts/english/vocab-example.html#constitution",
    "title": "vocabs with simple examples",
    "section": "constitution",
    "text": "constitution\nthe set of political principles\nthe general state of someoneâ€™s health:\n\nHe has a very strong constitution.\n\nhow something is made up of different parts:\n\nthe constitution of a chemical compound"
  },
  {
    "objectID": "posts/english/vocab-example.html#stuffy",
    "href": "posts/english/vocab-example.html#stuffy",
    "title": "vocabs with simple examples",
    "section": "stuffy",
    "text": "stuffy\nA stuffy room or building is unpleasant because it has no fresh air:\n\nItâ€™s really hot and stuffy in here - letâ€™s open the window.\n\nold-fashioned, formal, and boring:\n\nShe is trying to promote a less stuffy image of librarians.\n\nstuffy nose"
  },
  {
    "objectID": "posts/english/vocab-example.html#propagate",
    "href": "posts/english/vocab-example.html#propagate",
    "title": "vocabs with simple examples",
    "section": "propagate",
    "text": "propagate\nproduce a new plant using a parent plant (or animal)\nto spread opinions, lies, or beliefs among a lot of people:\n\nThe government have tried to propagate the belief that this is a just war.\n\nto send out or spread light or sound waves, movement, etc., or to be sent out or spread:\n\nHow are sound waves propagated?"
  },
  {
    "objectID": "posts/english/vocab-example.html#penalize",
    "href": "posts/english/vocab-example.html#penalize",
    "title": "vocabs with simple examples",
    "section": "penalize",
    "text": "penalize\nto cause someone a disadvantage:\n\nThe present tax system penalizes poor people.\n\nto punish someone, esp.Â for breaking the law or a rule:\n\nThe new law penalizes the taxpayers who can least afford to pay."
  },
  {
    "objectID": "posts/english/vocab-example.html#graze",
    "href": "posts/english/vocab-example.html#graze",
    "title": "vocabs with simple examples",
    "section": "graze",
    "text": "graze\nto break the surface of the skin by rubbing against something rough:\n\nHe fell down and grazed his knee.\n\nIf an object grazes something, it touches its surface lightly when it passes it:\n\nThe aircraftâ€™s landing gear grazed the treetops as it landed.\n\neat grass\ninjury"
  },
  {
    "objectID": "posts/english/vocab-example.html#sustain",
    "href": "posts/english/vocab-example.html#sustain",
    "title": "vocabs with simple examples",
    "section": "sustain",
    "text": "sustain\nto cause or allow something to continue for a period of time:\n\nThe economy looks set to sustain its growth into next year.\n\nto keep alive\n\nThe soil in this part of the world is not rich enough to sustain a large population.\n\nto suffer or experience, especially damage or loss:\n\nShe sustained multiple injuries in the accident.\n\nto support emotionally:\n\nShe was sustained by the strength of her religious faith."
  },
  {
    "objectID": "posts/english/vocab-example.html#steer",
    "href": "posts/english/vocab-example.html#steer",
    "title": "vocabs with simple examples",
    "section": "steer",
    "text": "steer\nto control the direction of a vehicle\n\nThis car is very easy to steer.\n\nIf a vehicle steers, it follows a particular route or direction:\n\nThe ship passed Landâ€™s End, in Cornwall, then steered towards southern Ireland.\n\nto take or make sb./sth. go in the direction you want them\n\nShe steered her guests into the dining room."
  },
  {
    "objectID": "posts/english/vocab-example.html#comprise",
    "href": "posts/english/vocab-example.html#comprise",
    "title": "vocabs with simple examples",
    "section": "comprise",
    "text": "comprise\nto have things or people as parts or members; to consist of:\n\nThe course comprises a class book, a practice book, and a CD.\n\nto be the parts or members of something; to make up something:\n\nItalian students comprise 60 percent of the class.\nThe class is comprised mainly of Italian and French students."
  },
  {
    "objectID": "posts/english/vocab-example.html#prosper",
    "href": "posts/english/vocab-example.html#prosper",
    "title": "vocabs with simple examples",
    "section": "prosper",
    "text": "prosper\nto be or become successful, especially financially:\n\nLots of microchip manufacturing companies prospered at that time."
  },
  {
    "objectID": "posts/english/vocab-example.html#compromise",
    "href": "posts/english/vocab-example.html#compromise",
    "title": "vocabs with simple examples",
    "section": "compromise",
    "text": "compromise\nan agreement in an argument in which the people involved reduce their demands or change their opinion in order to agree:\n\nIt is hoped that a compromise will be reached in todayâ€™s talks.\nIn a compromise between management and unions, a four percent pay rise was agreed in return for an increase in productivity.\n\nto allow your principles to be less strong or your standards or morals to be lower:\n\nDonâ€™t compromise your beliefs/principles for the sake of being accepted.\nIf we back down on this issue, our reputation will be compromised.\n\nto risk having a harmful effect on something:\n\nWe would never compromise the safety of our passengers."
  },
  {
    "objectID": "posts/english/vocab-example.html#manufacture",
    "href": "posts/english/vocab-example.html#manufacture",
    "title": "vocabs with simple examples",
    "section": "manufacture",
    "text": "manufacture\nto produce goods in large numbers, usually in a factory using machines:\n\nHe works for a company that manufactures car parts.\nThe report notes a rapid decline in manufactured goods.\n\nto invent something, such as an excuse or story, in order to deceive someone:\n\nShe insisted that every scandalous detail of the story had been manufactured.\n\nthe process of producing goods:\n\nOil is used in the manufacture of a number of fabrics."
  },
  {
    "objectID": "posts/english/vocab-example.html#revelation",
    "href": "posts/english/vocab-example.html#revelation",
    "title": "vocabs with simple examples",
    "section": "revelation",
    "text": "revelation\nthe act of making something known that was secret, or a fact that is made known:\n\na moment of revelation\nHis wife divorced him after the revelation that he was having an affair."
  },
  {
    "objectID": "posts/english/vocab-example.html#impart",
    "href": "posts/english/vocab-example.html#impart",
    "title": "vocabs with simple examples",
    "section": "impart",
    "text": "impart\nto communicate information to someone:\n\nto impart the bad news\nI was rather quiet as I didnâ€™t feel I had much wisdom to impart on the subject.\n\nto give something a particular feeling, quality, or taste:\n\nPreservatives can impart colour and flavour to a product."
  },
  {
    "objectID": "posts/english/vocab-example.html#concession",
    "href": "posts/english/vocab-example.html#concession",
    "title": "vocabs with simple examples",
    "section": "concession",
    "text": "concession\nsomething that is allowed or given up, often in order to end a disagreement, or the act of allowing or giving this:\n\nBoth sides involved in the conflict made some concessions in yesterdayâ€™s talks.\nHe stated firmly that no concessions will be made to the terrorists.\n\nthe act of admitting defeat:\n\nThe former presidentâ€™s concession came even before all the votes had been counted.\na concession speech\n\na reduction in the usual price of something, made available to students, old people, etc.:\n\nYou can get travel concessions if you are under 26."
  },
  {
    "objectID": "posts/english/vocab-example.html#trigger",
    "href": "posts/english/vocab-example.html#trigger",
    "title": "vocabs with simple examples",
    "section": "trigger",
    "text": "trigger\na part of a gun that causes the gun to fire when pressed:\n\nItâ€™s not clear who actually pulled the trigger.\n\nan event or situation, etc. that causes something to start:\n\nThere are fears that the incident may be a trigger for more violence in the capital.\n\nsomething that causes someone to feel upset and frightened because they are made to remember something bad that has happened in the past:\n\nA trigger is something that sets off a flashback, transporting the person back to the traumatic event.\n\nto cause something to start:\n\nSome people find that certain foods trigger their headaches.\nUltraviolet-B radiation triggers the skin to produce vitamin D.\nThe racial killings at the weekend have triggered off a wave of protests.\n\nto cause a strong emotional reaction of fear, shock, anger, or worry in someone, especially because they are made to remember something bad that has happened in the past:\n\nSeeing him come towards me just triggered me and I screamed.\nHe could be triggered by a loud noise."
  },
  {
    "objectID": "posts/english/vocab-example.html#conform",
    "href": "posts/english/vocab-example.html#conform",
    "title": "vocabs with simple examples",
    "section": "conform",
    "text": "conform\nto behave according to the usual standards of behaviour that are expected by a group or society:\n\nAt our school, you were required to conform, and there was no place for originality."
  },
  {
    "objectID": "posts/english/vocab-example.html#deduce",
    "href": "posts/english/vocab-example.html#deduce",
    "title": "vocabs with simple examples",
    "section": "deduce",
    "text": "deduce\nto reach an answer or a decision by thinking carefully about the known facts:\n\nWe cannot deduce very much from these figures.\nThe police have deduced that he must have left his apartment yesterday evening."
  },
  {
    "objectID": "posts/english/vocab-example.html#antiseptic",
    "href": "posts/english/vocab-example.html#antiseptic",
    "title": "vocabs with simple examples",
    "section": "antiseptic",
    "text": "antiseptic\na chemical used for preventing infection in an injury, especially by killing bacteria:\n\nAntiseptic is used to sterilize the skin before giving an injection.\nMany of the ingredients for antiseptics come from the rainforests.\n\ncompletely free from infection:\n\nIn the 1870s and 1880s, doctors began to follow the principles of antiseptic surgery.\n\ntoo clean and showing no imagination and character:\n\nThereâ€™s an antiseptic feeling to the new town centre, with its covered shopping mall."
  },
  {
    "objectID": "posts/english/vocab-example.html#patronage",
    "href": "posts/english/vocab-example.html#patronage",
    "title": "vocabs with simple examples",
    "section": "patronage",
    "text": "patronage\nthe support given to an organization by someone:\n\nThe charity enjoys the patronage of many prominent local business people.\n\nthe power of a person to give someone an important job or position:\n\nPatronage is a potent force if used politically."
  },
  {
    "objectID": "posts/english/vocab-example.html#whip",
    "href": "posts/english/vocab-example.html#whip",
    "title": "vocabs with simple examples",
    "section": "whip",
    "text": "whip\nto bring or take something quickly:\n\nShe whipped a handkerchief out of her pocket and wiped his face.\nHe whipped the covers off the bed.\nI was going to pay but before I knew it heâ€™d whipped out his credit card.\nThey whipped my plate away before Iâ€™d even finished.\n\nto (cause something to) move quickly and forcefully:\n\nThe wind whipped across the half-frozen lake.\nA fierce, freezing wind whipped torrential rain into their faces.\n\nto beat food, especially cream, with a special piece of equipment in order to make it thick and firm:\n\nCould you whip the cream for me?\nTry whipping a little brandy or other liqueur into the cream.\nTop with whipped cream and a sprinkle of sugar.\n\nto hit a person or animal with a whip:\n\nI donâ€™t like the way the drivers whip their horses.\n\nto defeat a person or a team in a competition, especially in a sport:\n\nThey beat us last time, but we whipped them in a rematch.\nHe whipped him in their fight two years ago."
  },
  {
    "objectID": "posts/english/vocab-example.html#perquisite",
    "href": "posts/english/vocab-example.html#perquisite",
    "title": "vocabs with simple examples",
    "section": "perquisite",
    "text": "perquisite\nan advantage or something extra, such as money or goods, that you are given because of your job:\n\nThe perquisites of this job include health insurance and a performance bonus."
  },
  {
    "objectID": "posts/english/vocab-example.html#elaborate",
    "href": "posts/english/vocab-example.html#elaborate",
    "title": "vocabs with simple examples",
    "section": "elaborate",
    "text": "elaborate\ncontaining a lot of careful detail or many detailed parts:\n\nYou want a plain blouse to go with that skirt - nothing too elaborate.\nTheyâ€™re making the most elaborate preparations for the wedding.\nHe came out with such an elaborate excuse that I didnâ€™t quite believe him.\n\nto add more information to or explain something that you have said:\n\nThe congresswoman said she was resigning, but refused to elaborate on her reasons for doing so."
  },
  {
    "objectID": "posts/english/vocab-example.html#merchandise",
    "href": "posts/english/vocab-example.html#merchandise",
    "title": "vocabs with simple examples",
    "section": "merchandise",
    "text": "merchandise\ngoods that are bought and sold:\n\nShoppers complained about poor quality merchandise and high prices.\nJapan exported $117 billion in merchandise to the US in 1999.\n\nto encourage the sale of goods by advertising them or by making certain that they are noticed:\n\nShe had to merchandise the new product line."
  },
  {
    "objectID": "posts/english/vocab-example.html#dividend",
    "href": "posts/english/vocab-example.html#dividend",
    "title": "vocabs with simple examples",
    "section": "dividend",
    "text": "dividend\n(a part of) the profit of a company that is paid to the people who own shares in it:\n\nDividends will be sent to shareholders.\nIn addition to their salary, employees receive a profit-related dividend."
  },
  {
    "objectID": "posts/english/vocab-example.html#scatter",
    "href": "posts/english/vocab-example.html#scatter",
    "title": "vocabs with simple examples",
    "section": "scatter",
    "text": "scatter\nto (cause to) move far apart in different directions:\n\nThe protesters scattered at the sound of gunshots.\nThe soldiers came in and scattered the crowd.\n\nto cover a surface with things that are far apart and in no particular arrangement:\n\nScatter the powder around the plants.\nI scattered grass seed all over the lawn.\nI scattered the whole lawn with grass seed."
  },
  {
    "objectID": "posts/esl/ESL-01.html",
    "href": "posts/esl/ESL-01.html",
    "title": "The elements of statistical learning 01",
    "section": "",
    "text": "committee from a bunch of weak learners\\(G_m\\)(slightly better than rand)\n\\[\nG(x)=sign\\left(\\sum_{m=1}^Ma_mG_m(x)\\right)\n\\]\none generic method is forward-stagewise method where you compute one model \\(G_m\\) and its correspd weight \\(a_m\\) at a time (min \\(L(y_i, f_m(x_i)+\\beta G_m(x_i))\\)).\nif using MSE as the \\(L\\) loss, each time we are seeking for a model \\(\\beta G\\) that fit the residual.\n\n\niteratively fit \\(G_m\\) on a weighted dataset.\nmethod derived by using exp loss instead of the common mse ..\n\\[e^{-y_if(x_i)}\\]\nsuppose we consider scaled model (\\(range f=\\{-1,1\\}\\))\nat stage m\nwe want to opt the following\n\\[\n\\min_{(a,G)}\\sum_{i=1}^Nexp\\left(-y_i(f_{m-1}(x_i)+aG(x_i))\\right)\\\\\n=\\sum w_iexp(-y_iaG(x_i))\n\\]\nbasically using forward-stagewise method, if we fix \\(a\\)\n\\[\\begin{align}\n&exp(a)\\sum_{y_i\\neq G(x_i)}w_i+exp(-a)\\sum_{y_i=G(x_i)}w_i\\\\\n=&exp(a)\\sum_i^N w_i - exp(a)\\sum_{y_i=G(x_i)}w_i+\\dots\\\\\n=&(exp(-a)-exp(a))\\sum_{y_i=G(x_i)} w_i + exp(a)\\sum_i^Nw_i\\\\\n=&A\\sum_i^Nw_i[y_i\\neq G(x_i)]+\\dots\n\\end{align}\\]\nso actually, we are minimizing a weighted dataset using 0-1 loss\nwith this new solved \\(G\\), we can then solve for \\(a\\)\n\\[\\begin{align}\n&\\frac{d \\sum w_iexp(-aG(x_i))}{da}\\\\\n=&-\\sum w_iy_iG(x_i)exp(-ay_iG(x_i))\\\\\n=&-(exp(-a)+exp(a))\\sum_{y_i=G(x_i)} w_i + exp(a)\\sum_i^Nw_i=0\n\\end{align}\\]\nin short, AdaBoost.M1 is directly abtained from forward-stagewise method. With exp loss, we can get this neat sol.\nfurther, since \\(f_m=f_{m-1}+G_m\\), the weights \\(w_i\\) can be calculated iteratively.\n\n\n\n\\(f^\\star=arg \\min_fE_{Y|x}(e^{-Yf(x)})=\\frac{1}{2}log\\frac{Pr(Y=1|x)}{Pr(Y=-1|x)}\\), it est one half the log odds of \\(Pr(Y=1|x)\\), and \\(Pr(Y=1|x)=\\frac{1}{1+e^{-2f}}\\)\nlet \\(p(x)=Pr(Y=1|x)=\\frac{1}{1+e^{-2f}}\\) and \\(Y^\\prime =(Y+1)/2\\), we can see that cross-entropy and exp loss are actually est the same population.\n\nce: \\(-l(Y,f(x))=log(1+e^{-2Yf(x)})=Y^\\prime logp(x)+(1-Y^\\prime)log(1-p(x))\\), (f is softmaxed before output)\nexp: \\(e^{-Yf}\\)\nsame Pr, and f\n\n\n\n\n\n\nhere robustness means being disturbed less by outlier samples.\n\n\n\nsquared-error -> \\(f(x)=\\mathbf{E}(Y|x)\\), more focus on obs with large absolute residuals during fitting process, far less robust, bad on long-tailed error distrb, gross outliers\nabsolute loss -> median\nHuber loss. \\([y-f(x)]^2 \\text{ for abs residual } \\leq \\delta \\text{ and } 2\\delta|y-f(x)|-\\delta^2 \\text{ otherwise }\\)\n\n\n\n\nloss and robustness on regression prob\n\n\n\n\n\nwe consider two-class classification problem\nin regression problem, \\(y-f(x)\\) is considered as the margin\nin classif.., \\(yf(x)\\) plays the same role. where \\(y\\in\\{-1,1\\}\\)\n\n\n\nLoss functions for two-class classification.\n\n\n\n\n\nsquared-error, and exp loss are not robust, but give rise to simple boosting algorithms."
  },
  {
    "objectID": "posts/esl/ESL-01.html#specific-boosting-example",
    "href": "posts/esl/ESL-01.html#specific-boosting-example",
    "title": "The elements of statistical learning 01",
    "section": "specific boosting example",
    "text": "specific boosting example\n\nboosting tree\nregion \\(R_j,\\, j=1,\\dots,J\\)\n\\(x\\in R_j\\mathbb{R}ightarrow f(x)=y_j\\)"
  },
  {
    "objectID": "posts/esl/ESL-02.html",
    "href": "posts/esl/ESL-02.html",
    "title": "unsupervised model related",
    "section": "",
    "text": "quick notes while reading unsupervised model related part in ESL"
  },
  {
    "objectID": "posts/esl/ESL-02.html#knn",
    "href": "posts/esl/ESL-02.html#knn",
    "title": "unsupervised model related",
    "section": "KNN:",
    "text": "KNN:\n\ntangent distance\nInvariant Metrics and Tangent Distance (Simard et al., 1993)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#k-means",
    "href": "posts/esl/ESL-02.html#k-means",
    "title": "unsupervised model related",
    "section": "K-means:",
    "text": "K-means:\n\nfind clusters,\nrecompute the cluster centers\n\n\nK-means for classification\n\napply K-means for each class, using R prototypes per class (K in total)\nassign a label for each prototypes (K x R in total)\nclassify new x to the nearest prototype"
  },
  {
    "objectID": "posts/esl/ESL-02.html#learning-vector-quantization",
    "href": "posts/esl/ESL-02.html#learning-vector-quantization",
    "title": "unsupervised model related",
    "section": "Learning Vector Quantization",
    "text": "Learning Vector Quantization\n\ncons of vanilla K-means classification:\n\nother samples of different class wonâ€™t involve in the position of other class\n\nfor a random training sample, move the closest prototype a bit (towards or away)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#gaussian-mixture",
    "href": "posts/esl/ESL-02.html#gaussian-mixture",
    "title": "unsupervised model related",
    "section": "Gaussian mixture",
    "text": "Gaussian mixture\n\nassign weight for each cluster (gaussian density)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#discriminant-adaptive-nearest-neighbor-dann",
    "href": "posts/esl/ESL-02.html#discriminant-adaptive-nearest-neighbor-dann",
    "title": "unsupervised model related",
    "section": "discriminant adaptive nearest-neighbor (DANN)",
    "text": "discriminant adaptive nearest-neighbor (DANN)\n\n\\(\\varSigma=W^{-1/2}[B^*+\\epsilon I]W^{-1/2}\\)\n\\(\\varSigma^{1/2} (x-x_0)=\\textrm{circle}\\)\n\\(W\\) normalization, spheres the data\n\\(B\\) stretch, assign larger weights to the directions with larger covariance."
  },
  {
    "objectID": "posts/esl/ESL-02.html#market-basket-analysis",
    "href": "posts/esl/ESL-02.html#market-basket-analysis",
    "title": "unsupervised model related",
    "section": "Market Basket Analysis",
    "text": "Market Basket Analysis\n\nThe Apriori Algorithm\ndiscover association rules with high support values and confidence"
  },
  {
    "objectID": "posts/esl/ESL-02.html#unsupervised-as-supervised-learning",
    "href": "posts/esl/ESL-02.html#unsupervised-as-supervised-learning",
    "title": "unsupervised model related",
    "section": "Unsupervised as Supervised Learning",
    "text": "Unsupervised as Supervised Learning\n\nreference density function"
  },
  {
    "objectID": "posts/esl/ESL-02.html#generalized-association-rules-not-understand",
    "href": "posts/esl/ESL-02.html#generalized-association-rules-not-understand",
    "title": "unsupervised model related",
    "section": "Generalized Association Rules (NOT UNDERSTAND)",
    "text": "Generalized Association Rules (NOT UNDERSTAND)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#clustering-analysis",
    "href": "posts/esl/ESL-02.html#clustering-analysis",
    "title": "unsupervised model related",
    "section": "Clustering Analysis",
    "text": "Clustering Analysis\n\nDissimilarity Based on Attributes (dissimilarity defined individually on each attributes)\n\nQuantitative variables:\n\n\\(\\ell(\\textrm{abs}(x_i-x_j))\\) \\(\\rightarrow\\) squared-error loss, identity (just \\(\\textrm{abs}\\))\n\n\n\n\ncorreltation\n\nOrdinal variables\nCategorical variables\n\n\n\nObject Dissimilarity (combining the p-individual attribute dissimilarities)\n\nweighted average"
  },
  {
    "objectID": "posts/calculus/calculus-02.html",
    "href": "posts/calculus/calculus-02.html",
    "title": "calculus review 02",
    "section": "",
    "text": "can be used for computing ranks, solving systems of linear equations, etc.\nGiven \\(\\mathbf{A}_{m\\times n}\\mathbf{x}=\\mathbf{b}\\)\n\\([\\mathbf{A}\\mid\\mathbf{b}]\\) row reduces to \\([\\tilde{\\mathbf{A}}\\mid\\tilde{\\mathbf{b}}]\\), determine the case of the solutions.\n\n\n\n\n\n\nextend\n\n\n\nsquare matrix can be approximated by invertible matrices (replace zeros in the diagonal entries of the row reduced matrix with \\(1/n\\)).\n\n\n\n\nthe dim of the image\nrow rank = column rank"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#vector-space",
    "href": "posts/calculus/calculus-02.html#vector-space",
    "title": "calculus review 02",
    "section": "Vector Space",
    "text": "Vector Space\n\nâ€œcontrete to abstractâ€ function \\(\\Phi_{v}\\)\nA linear transformation from contrete \\(\\mathbb{R}^n\\) coordinate representation to abstract vector space \\(V\\)\n\\[\n\\Phi_{\\{\\mathbf{v}\\}}(\\mathbf{a})=\\Phi_{\\{\\mathbf{v}\\}}\\left(\\begin{matrix}a_1\\\\\\vdots\\\\a_n\\end{matrix}\\right)=a_1\\mathbf{v_1}+\\cdots+a_n\\mathbf{v_n},\n\\]\nwhere \\(\\{\\mathbf{v}\\}\\) is the set of base vectors of \\(V\\).\n\n\nChange of bases \\(P_{\\{\\mathbf{v}'\\}\\to \\{\\mathbf{v}\\}}\\)\n\\[[P_{\\{\\mathbf{v}'\\}\\to\\{\\mathbf{v}\\}}]\\mathbf{a}=\\mathbf{b},\\]\nwhere \\(\\sum_ia_i\\mathbf{v}_i'=\\sum_ib_i\\mathbf{v}_i\\)\nIn other words,\n\\[P_{\\{\\mathbf{v}'\\}\\to \\{\\mathbf{v}\\}}=\\Phi_{\\{\\mathbf{v}'\\}}^{-1}\\Phi_{\\{\\mathbf{v}\\}}\\]"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#eigenvectors-and-eigenvalues",
    "href": "posts/calculus/calculus-02.html#eigenvectors-and-eigenvalues",
    "title": "calculus review 02",
    "section": "Eigenvectors and Eigenvalues",
    "text": "Eigenvectors and Eigenvalues\neigenvectors with different eigenvalues are linearly independent.\ndiagnolization"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#newtons-method",
    "href": "posts/calculus/calculus-02.html#newtons-method",
    "title": "calculus review 02",
    "section": "Newtonâ€™s Method",
    "text": "Newtonâ€™s Method\nwhy talking about Newtonâ€™s Method?\na convenient and practical way to introduce implicit function and inverse function\n\ndef\ntarget: solve for a solution \\(f(x)=0\\)\nstart at \\(a_0\\)\niterate with \\(a_i=a_{i-1}-[Df(a_{i-1})]^{-1}f(a_{i-1})\\)\n\n\n\n\n\n\nNote\n\n\n\nnotes: since \\(f(x)\\approx f(a)+[Df(a)](x-a)\\), letting \\(f(x)=0\\) will give us the next guess.\n\n\nThere is actually a sufficient condition to ensure the convergence.\n\n\nKantorovichâ€™s theorem\nlet \\(a_0\\in\\mathbb{R}^n\\), \\(U\\): open neighborhood of \\(a_0\\) in \\(\\mathbb{R}^n\\), \\(f:U\\to\\mathbb{R}^n\\)\ndef \\(h_0=-[Df(a_0)]^{-1}f(a_0)\\), \\(a_1=a_0+h_0\\), \\(U_1=B_{|h_0|}(a_1)\\), \\(M\\) as the Lipschitz ratio of \\(Df(x)\\) in \\(U_1\\)\ntheorem: if \\(|f(a_0)||[Df(a_0)]^{-1}|^2M\\le\\frac{1}{2}\\), then \\(f(x)=0\\) has a unique solution in the closed ball \\(\\overline{U_1}\\) and Newtonâ€™s Method converges to it with initial guess \\(a_0\\).\n\n\n\n\n\n\nNote\n\n\n\nnotes 1: in short, \\(a_1\\) is the next guess\nnotes 2: a valid value of Lipschitz ratio\nIf \\(|D_kD_jf_i(x)|\\le c_{i,j,k}\\)\nthen\n\\[\\left(\\sum_{1\\le i,j,k\\le n}c_{i,j,k}^2\\right)^{1/2}\\]\nis valid\n\n\n\n\nproposition\ndef \\(U_0=\\{x\\mid |x-a_0|\\lt 2|h_0|\\}\\)\nIf \\(Df\\) satisfy Lipschitz condition in \\(U_0\\), then \\(f(x)=0\\) has a unique solution in \\(\\overline{U_0}\\) and Newtonâ€™s Method converges to it.\n\n\n\n\n\n\nNote\n\n\n\nIf the product is strictly less than \\(1/2\\), then Newtonâ€™s Method superconverges.\nIf \\(|h_n|\\le\\frac{1}{c}\\), for \\(c\\) some constant depend on \\(f\\), then\n\\[|h_{n+m}|\\le \\frac{1}{c}\\cdot\\left(\\frac{1}{2}\\right)^{2m},\\]\nwhere \\(h_i=a_{i+1}-a_{i}\\).\n\n\n\n\nstronger version\nreplace all lengths of matrices with norms.\n\n\n\n\n\n\nNote\n\n\n\nnorm of a matrix \\(A\\)\n\\[\\lVert A\\rVert=\\sup_{|x|=1} |A\\mathbf{x}|\\]\neasy to check: \\(\\lVert A\\rVert\\le |A|\\)"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#inverse-function-theorem",
    "href": "posts/calculus/calculus-02.html#inverse-function-theorem",
    "title": "calculus review 02",
    "section": "inverse function theorem",
    "text": "inverse function theorem\n\\(f\\) is continously differentiable\n\\(Df\\) is invertible at \\(x_0\\)\nthen \\(f\\) is locally invertible, with differentiable inverse in some neighborhood of \\(f(x_0)\\)\n\n\n\n\n\n\nNote\n\n\n\nthe vigorous version is too lengthy, I am not gonna put it here.\nAs you might expect, the locality can actually be quantified by Kantorovichâ€™s theorem."
  },
  {
    "objectID": "posts/calculus/calculus-02.html#implicit-function-theorem",
    "href": "posts/calculus/calculus-02.html#implicit-function-theorem",
    "title": "calculus review 02",
    "section": "implicit function theorem",
    "text": "implicit function theorem\n\\(U\\): a open subset of \\(\\mathbb{R}^{n+m}\\)\n\\(\\mathbf{F}:U\\to\\mathbb{R}^n\\) a \\(C^1\\) mapping s.t. \\(\\mathbf{F}(\\mathbf{c})=0\\) and \\([D\\mathbf{F}(c)]\\) is onto\nthen the system of linear equations \\(D\\mathbf{F}(\\mathbf{x})=0\\) has n pivotal vars and m nonpivotal vars. there exists a neighborhood of \\(\\mathbf{c}\\) where \\(\\mathbf{F}(\\mathbf{c})=0\\) implicitly defines the n passive vars as a function \\(\\mathbf{g}\\) of the m active vars.\n\n\n\n\n\n\nNote\n\n\n\nnotes 1: \\(\\mathbf{F}\\) behaves similar to \\(D\\mathbf{F}\\) locally.\nin a normal system of linear equations \\(\\mathbf{Ax}=0\\), where \\(\\mathbf{x}\\in\\mathbb{R}^{m+n}\\), \\(\\textrm{rank }\\mathbf{A}=n\\), the dimension of the kernel space of \\(\\mathbf{A}\\) is m which corresponds to the m active vars (the base of the kernel).\nnotes 2: again, the rigorous version is a little lengthy.\n\n\nnot like the existence theorem, which only claims the existence of the inverse function or the implicit function, using Newtonâ€™s Method gives us a more practical way to find the function and a more quantified result."
  },
  {
    "objectID": "posts/calculus/calculus-01.html",
    "href": "posts/calculus/calculus-01.html",
    "title": "calculus review 01",
    "section": "",
    "text": "linear transformations\nmeasure of matrices: \\(\\left \\rVert A\\right \\lVert_F^2\\)\ntriangle inq in matrices: \\(\\rVert AB \\lVert\\le \\rVert A\\lVert\\rVert B\\lVert\\)\nthe neighborhood of \\(x\\), exist an open ball init\nclosure \\(\\bar{A}\\), the smallest close-set that contains A\ninterior \\(\\mathring{A}\\), the largest open set that A contains\nthe boundary of subset, \\(\\partial A\\)\n\nconvergence of a sequence, in terms of coordinates\nlimits of multivariable functions: continuity is preserved under dot product operation\ncontinuity: the preimage of a neighborhood of \\(f(x)\\) is also a neighborhood of x\nuniform continuity: linear transformations are uniform continuity\nconvergence of the sum of series (vectors): absolute(norm in vector cases) convergence implies convergence\ncomplex exponentials\n\ncomplex exponential series converges: \\(e^z=1+z+\\frac{z^2}{2!}+\\dots=\\sum_{k=0}^\\infty \\frac{z^k}{k!}\\), since the absolute series converges\n\neuler formular: \\(e^{it}=cost+isint\\)\ngeometric series of matrices:\n\n\\(S=I+A+A^2+\\dots\\) converges to \\((1-A)^{-1}\\) if \\(\\lVert A \\rVert \\lt 1\\)\nthe set of invertible n by n matrices is open\n\nbounded: subset \\(X\\subset \\mathbb{R}^n\\) is bounded if it is contained in some ball centered at the origin\ncompact: nonempty subset \\(C\\subset \\mathbb{R}^n\\) is compact if it is closed and bounded"
  },
  {
    "objectID": "posts/calculus/calculus-01.html#important-theorem",
    "href": "posts/calculus/calculus-01.html#important-theorem",
    "title": "calculus review 01",
    "section": "important theorem",
    "text": "important theorem\n\nBolzano-Weierstrass theorem: a compact set C contains a seq, then that seq has a convergent sub seq whose limit is in C\na continuous function on a compact set achieves its minimum and maximum\nthe mean value theorem\ncontinuity on a compact set is uniform continuity\nthe fundamental theorem of algebra"
  },
  {
    "objectID": "posts/calculus/calculus-01.html#differentiable",
    "href": "posts/calculus/calculus-01.html#differentiable",
    "title": "calculus review 01",
    "section": "differentiable",
    "text": "differentiable\n\ndef\ndifferentiable means \\(f\\) can be well approximated at \\(x=a\\) by a linear transformation\nlet \\(f:\\mathcal{U}\\to\\mathbb{R}^m\\) where \\(\\mathcal{U}\\) is a open subset of \\(\\mathbb{R}^n\\)\n\\(D_if(a)\\) means the partial derivative on the i-th value. It is an m by 1 vertical vector\nwe can treat it like the mapped element from \\(e_i\\)\n\\(Df(a)\\) is an m by n matrix where we stack \\(D_i\\)â€™s in a row\nthe precise def of the derivative is as below:\n\\(Df(a)\\) is a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\), s.t.\n\\[\n\\lim_{h\\to 0}\\frac{1}{|h|}\\left( (f(a+h)-f(a)) - [Df(a)]h\\right )=0\n\\]\n\n\nJacobian matrix\nJacobian matrix now fits this definition and donâ€™t need for any transpose.\nnote the existence of Jacobian matrix does not mean differentiable, except that we let \\(f\\) to be continuously differentiable. I wonâ€™t present any examples of why and how. We just need to focus on the \\(C^1\\) functions from now on :\n\n\nfor some cases where the domain is not the subspace of Euclidiean space, we may need to use the definition of derivative instead of the Jacobian matrix.\ne.g.Â 1\n\\(\\mathcal{S}:Mat(n,n)\\to Mat(n,n)\\) given by \\(S(A)=A^2\\)\n\\[[\\mathbf{D}S(A)]H=AH+HA\\]\ne.g.Â 2\n\\(f(A)=A^{-1}\\) defined on invertible n by n matrices\n\\[[\\mathbf{D}f(A)]H=-A^{-1}HA^{-1}\\]\n\n\n\nabout gradient\n\\[\ngrad\\ f(a)=\\vec{\\nabla} f(a)=[D_1f(a),\\dots, D_nf(a)]^\\intercal\n\\]\nwhere \\(f:\\mathbb{R}^n\\to \\mathbb{R}\\)\nFrom my POV, gradient is the solution of the fastest increasing direction at some point \\(a\\) and it happens to be the (transpose of) derivative. note the transpose has its meaning, it makes the grad lying in the domain.\nfurthermore, from how we get the solution, we should have an inner-product defined on the domain space, which is not the case in the def of derivative.\n\n\ndirectional derivative\ndirectional derivative of \\(f\\) at \\(a\\) in the derection \\(\\vec{v}\\)\n\\[\n[Df(a)]\\vec{v}\n\\]\nthis is how linear transformation works.\n\n\nrules\n\nconstant function: 0\nlinear function: itself\n\\(\\mathbf{f}=(f_1,\\dots,f_m)^\\intercal\\) then\n\\[[Df(a)]\\vec{v}=\\left[[Df_1(a)]\\vec{v},\\cdots,[Df_m(a)]\\vec{v}\\right]^\\intercal\\]\n\\([D(f+g)(a)]=[Df(a)]+[Dg(a)]\\)\n\\(f:U\\to \\mathbb{R}\\) and \\(\\mathbf{g}:U\\to \\mathbb{R}^m\\)\n\\[\n[D(f\\mathbf{g})]\\vec{v}=f(a) [D\\mathbf{g}(a)]\\vec{v}+([Df(a)]\\vec{v})\\mathbf{g}(a)\n\\]\n\\[\\left[D\\left(\\frac{\\mathbf{g}}{f}\\right)(a)\\right]\\vec{v}=\\frac{[D\\mathbf{g}(a)]\\vec{v}}{f(a)}-\\frac{([Df(a)]\\vec{v})g(a)}{f^2(a)}\\]\ndot product \\(\\mathbf{f}:U\\to \\mathbb{R}^m\\)\n\\[\n[D(\\mathbf{f}\\cdot\\mathbf{g})]\\vec{v}=\\mathbf{f}(a)\\cdot[D\\mathbf{g}(a)]\\vec{v}+([D\\mathbf{f}(a)]\\vec{v})\\cdot\\mathbf{g}(a)\n\\]\nchain rule\n\\[[D(f\\circ g)(a)]=[Df(g(a))][Dg(a)]\\]\n\n\nref\n\nVector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"
  },
  {
    "objectID": "posts/calculus/calculus-03.html",
    "href": "posts/calculus/calculus-03.html",
    "title": "calculus review 03",
    "section": "",
    "text": "New stuff (to me)\n\n\n\n\n\n\n\n\nNote\n\n\n\nmanifold: a subset \\(M\\subset \\mathbb{R}^n\\) is a smooth \\(k\\)-dimensional manifold\nif locally it is the graph of a \\(C^1\\) mapping \\(f\\) expressing \\(n-k\\) variables as functions of the other \\(k\\) variables\n\n\nThe definition seems highly related to the implicit function theorem\nTherefore, we can quickly catch the spirit below\nlet \\(\\mathbf{F}:U\\rightarrow \\mathbb{R}^{n-k}\\) be a \\(C^1\\) mapping, where \\(U\\subset\\mathbb{R}^n\\).\n\n\n\n\n\n\nNote\n\n\n\n\nSome subset \\(M\\) of the domain \\(U\\) is a \\(k\\)-dimensional manifold if \\(\\mathbf{F}(z)=0\\) and \\([D\\mathbf{F}(z)]\\) is onto for every \\(z\\in M\\).\n(converse) if \\(M\\) is a smooth \\(k\\)-dimensional manifold, then for every \\(z\\in M\\), there exists \\(\\mathbf{F}\\) s.t., \\([D\\mathbf{F}(z)]\\) is onto and \\(\\mathbf{F}(y)=0\\) with a neighborhood of \\(z\\) as the domain\n\n\n\n\\(\\star\\) it says we can virtually claim a manifold by \\(\\mathbf{F}(z)=0\\)\nintuitively, the definition of manifold should not depend on the coordinates\n\n\n\n\n\n\nNote\n\n\n\n\\(k\\)-dimensional manifold \\(M\\subset\\mathbb{R}^m\\), \\(f\\) is a mapping with some properties 11Â  \\(f:U\\rightarrow\\mathbb{R}^m\\) where \\(U\\) is an open subset of \\(\\mathbb{R}^n\\) and \\([Df(x)]\\) is surjective at \\(\\forall x\\in f^{-1}(M)\\)\nthen \\(f^{-1}(M)\\) is a submanifold of \\(\\mathbb{R}^n\\) of dimension \\(k+n-m\\)\n\n\nas a corollary, manifolds are independent of coordinates if \\(f\\) is an affine transformation\n\n\n\nparametrization is useful for analysing manifolds since taking a manifolds as domain directly is rather cumbersome\n\n\n\n\n\n\nNote\n\n\n\na parametrization of a \\(k\\)-dimensional manifold \\(M\\subset\\mathbb{R}^n\\) is a mapping \\(\\gamma:U\\subset\\mathbb{R}^k\\rightarrow M\\), s.t.,\n\n\\(U\\) is open\n\\(\\gamma\\) is \\(C^1\\), one to one, and onto \\(M\\)\n\\([D\\gamma(u)]\\) is one to one for every \\(u\\in U\\)\n\n\n\n\nas a comparison, linear algebra and differential calculus have mucn in common\n\nrow reduction \\(\\leftrightarrow\\) newtonâ€™s method\nkernels \\(\\leftrightarrow\\) defining manifolds by equations ( e.g., \\(f(x)=0\\) )\nimages \\(\\leftrightarrow\\) defining manifolds by a parametrization\n\n\n\n\n\ntangent space is similar to the linear approximation of a function \\(f\\), but it is used on a manifold.\n\n(derivative) replace a function \\(f\\) locally by a linear map\n(tangent space) replace a manifold locally by a linear space\n\nfor a manifold \\(M=\\left\\{\\left(\\begin{matrix} x\\\\y \\end{matrix}\\right)\\in\\mathbb{R}^n\\mid f(x)=y\\right\\}\\)\nfix \\(z_0\\in M\\).\nthe change of \\(z\\) should be approximated by a linear mapping\n\\[\ny-y_0=[Df(x_0)](x-x_0)\n\\]\nin short \\(\\Delta y=[Df(x_0)]\\Delta x\\)\n\n\n\n\n\n\nNote\n\n\n\nthe graph of \\([Df(x_0)]\\) is the tangent space, which is denoted as \\(T_{z_0}M\\)\nthe linear approximation to the graph is the graph of the linear approximation\n\n\n\n\n\nif you have an equation form \\(F(x)=0\\) of the manifold, then you are lucky.\n\n\n\n\n\n\nNote\n\n\n\nthe kernel space of the derivative of \\(F\\) is the tangent space.\n\\[\nT_{z_0}M=\\ker [DF(z_0)]\n\\]\n\n\nagain, this will certainly remind you the implicit function theorem, where we also claim the derivative of the implicit function\nhere is another approach to get the tangent space\n\n\n\n\n\n\nNote\n\n\n\nLet \\(U\\subset\\mathbb{R}^k\\) be open, and let \\(\\gamma:U\\rightarrow\\mathbb{R}^n\\) be a parametrization of manifold \\(M\\), then\n\\[\nT_{\\gamma(u)}M=\\textrm{img}[D\\gamma(u)].\n\\]\n\n\nit might look strange at first â€” why taking the image instead of the kernel like above.\nquick calculation:\n\\[\n[D(F\\circ\\gamma)(u)]=[DF(\\gamma(u))]\\circ[D\\gamma(u)]=[0]\n\\]\nderivative is not quite like what we think as the origin function\nhere \\(\\textrm{img}[D\\gamma(u)]\\) and \\(\\textrm{img}[DF(\\gamma(u))]\\) are orthogonal complementary subspaces\n\n\n\nTBD :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJan 5, 2023\n\n\nQuarto Basics\n\n\nhello world,test\n\n\n\n\nSep 11, 2022\n\n\ncalculus review 03\n\n\ncalculus,review\n\n\n\n\nSep 8, 2022\n\n\nsome papers about multi-view-clustering\n\n\npaper reading,clustering,note\n\n\n\n\nSep 8, 2022\n\n\nunsupervised model related\n\n\nESL,note\n\n\n\n\nAug 18, 2022\n\n\nvocabs with simple examples\n\n\nEnglish,vocab,note\n\n\n\n\nAug 12, 2022\n\n\nPyTorch Notes\n\n\nPyTorch,programming\n\n\n\n\nAug 9, 2022\n\n\nNotes about Technical Writing\n\n\nwriting,note\n\n\n\n\nJul 5, 2022\n\n\nprobability review 01\n\n\nprobability\n\n\n\n\nJun 27, 2022\n\n\ncalculus review 02\n\n\ncalculus,review\n\n\n\n\nJun 8, 2022\n\n\nparallel algorithm course 10\n\n\nparallel algorithm,note\n\n\n\n\nJun 1, 2022\n\n\nparallel algorithm course 09\n\n\nparallel algorithm,note\n\n\n\n\nMay 11, 2022\n\n\nparallel algorithm course 08\n\n\nparallel algorithm,note\n\n\n\n\nApr 27, 2022\n\n\nparallel algorithm course 07\n\n\nparallel algorithm,note\n\n\n\n\nApr 23, 2022\n\n\nmidterm exam\n\n\nparallel algorithm,note,exam\n\n\n\n\nApr 20, 2022\n\n\nparallel algorithm course 06\n\n\nparallel algorithm,note\n\n\n\n\nApr 6, 2022\n\n\nparallel algorithm course 05\n\n\nparallel algorithm,note\n\n\n\n\nApr 2, 2022\n\n\nInequality\n\n\ninequality\n\n\n\n\nMar 30, 2022\n\n\nparallel algorithm course 04\n\n\nparallel algorithm,note\n\n\n\n\nMar 30, 2022\n\n\nUnderstanding Machine Learning 06\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 28, 2022\n\n\ncalculus review 01\n\n\ncalculus,review\n\n\n\n\nMar 23, 2022\n\n\nparallel algorithm course 03\n\n\nparallel algorithm,note\n\n\n\n\nMar 21, 2022\n\n\nUnderstanding Machine Learning 05\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 16, 2022\n\n\nparallel algorithm course 02\n\n\nparallel algorithm,note\n\n\n\n\nMar 16, 2022\n\n\nUnderstanding Machine Learning 04\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 11, 2022\n\n\nUnderstanding Machine Learning 03\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 9, 2022\n\n\nparallel algorithm course 01\n\n\nparallel algorithm,note\n\n\n\n\nMar 8, 2022\n\n\nUnderstanding Machine Learning 02\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 8, 2022\n\n\nvim techs\n\n\nvim,programming\n\n\n\n\nMar 7, 2022\n\n\nThe elements of statistical learning 01\n\n\nESL,note\n\n\n\n\nMar 3, 2022\n\n\nMoCo\n\n\npaper reading\n\n\n\n\nMar 3, 2022\n\n\nUnderstanding Machine Learning 01\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 1, 2022\n\n\nmy first post\n\n\ntest\n\n\n\n\n\n\nNo matching items"
  }
]