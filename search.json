[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMSR AI Seminar: Why Does Deep Learning Perform Deep Learning?\n\n\nnote,research,webinar\n\n\n\n\nFeb 6, 2023\n\n\nTotal Commander Tips\n\n\nnote,tool,total commander\n\n\n\n\nSep 11, 2022\n\n\ncalculus review 03\n\n\ncalculus,review\n\n\n\n\nSep 8, 2022\n\n\nsome papers about multi-view-clustering\n\n\npaper reading,clustering,note\n\n\n\n\nSep 8, 2022\n\n\nunsupervised model related\n\n\nESL,note\n\n\n\n\nAug 18, 2022\n\n\nvocabs with simple examples\n\n\nEnglish,vocab,note\n\n\n\n\nAug 12, 2022\n\n\nPyTorch Notes\n\n\nPyTorch,programming\n\n\n\n\nAug 9, 2022\n\n\nNotes about Technical Writing\n\n\nwriting,note\n\n\n\n\nJul 5, 2022\n\n\nprobability review 01\n\n\nprobability\n\n\n\n\nJun 27, 2022\n\n\ncalculus review 02\n\n\ncalculus,review\n\n\n\n\nJun 8, 2022\n\n\nparallel algorithm course 10\n\n\nparallel algorithm,note\n\n\n\n\nJun 1, 2022\n\n\nparallel algorithm course 09\n\n\nparallel algorithm,note\n\n\n\n\nMay 11, 2022\n\n\nparallel algorithm course 08\n\n\nparallel algorithm,note\n\n\n\n\nApr 27, 2022\n\n\nparallel algorithm course 07\n\n\nparallel algorithm,note\n\n\n\n\nApr 23, 2022\n\n\nmidterm exam\n\n\nparallel algorithm,note,exam\n\n\n\n\nApr 20, 2022\n\n\nparallel algorithm course 06\n\n\nparallel algorithm,note\n\n\n\n\nApr 6, 2022\n\n\nparallel algorithm course 05\n\n\nparallel algorithm,note\n\n\n\n\nApr 2, 2022\n\n\nInequality\n\n\ninequality\n\n\n\n\nMar 30, 2022\n\n\nUnderstanding Machine Learning 06\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 30, 2022\n\n\nparallel algorithm course 04\n\n\nparallel algorithm,note\n\n\n\n\nMar 28, 2022\n\n\ncalculus review 01\n\n\ncalculus,review\n\n\n\n\nMar 23, 2022\n\n\nparallel algorithm course 03\n\n\nparallel algorithm,note\n\n\n\n\nMar 21, 2022\n\n\nUnderstanding Machine Learning 05\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 16, 2022\n\n\nUnderstanding Machine Learning 04\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 16, 2022\n\n\nparallel algorithm course 02\n\n\nparallel algorithm,note\n\n\n\n\nMar 11, 2022\n\n\nUnderstanding Machine Learning 03\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 9, 2022\n\n\nparallel algorithm course 01\n\n\nparallel algorithm,note\n\n\n\n\nMar 8, 2022\n\n\nUnderstanding Machine Learning 02\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 8, 2022\n\n\nvim techs\n\n\nvim,programming\n\n\n\n\nMar 7, 2022\n\n\nThe elements of statistical learning 01\n\n\nESL,note\n\n\n\n\nMar 3, 2022\n\n\nUnderstanding Machine Learning 01\n\n\nunderstanding machine learning,note\n\n\n\n\nMar 3, 2022\n\n\nMoCo\n\n\npaper reading\n\n\n\n\nMar 1, 2022\n\n\nmy first post\n\n\ntest\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "",
    "text": "record video link on bilibili üëâ MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#main-question",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#main-question",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "main question",
    "text": "main question\nhow does ‚Äúdeep layers‚Äù work?\nDeep learning = hierarchical feature learning"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#observation",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#observation",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "observation",
    "text": "observation\n\nadding more layers and train holistically will improve the accuracy, though the previous layers are already fully converged.\nYou can‚Äôt expect what it learns from what you build"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#focused-object",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#focused-object",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "focused object",
    "text": "focused object\nwe only consider densenet with quadratic activation function\n\n\n\ndensenet"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#proof-target",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#proof-target",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "proof target",
    "text": "proof target\ndensenet with wider layers will learn a target densenet effeciently in arbitrary accuracy\n\n\n\n\n\n\nNote\n\n\n\n\n‚Äúwider‚Äù means overparameterize\n‚Äúefficient‚Äù means converging with arbitrary accuracy \\(\\epsilon\\) using \\(\\text{poly}(d,\\frac{1}{\\epsilon})\\) samples, \\(d\\) dimensions"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#assumptions",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#assumptions",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "assumptions",
    "text": "assumptions\n\nweight matrices in the target net are well-conditioned: not degenerated. the output will be a \\(2^L\\) degree poly\ninformation gap: let \\(a_i\\) be the coefficient in the linear combination of output, \\(a_i >> a_{i+1} >> 1/d^{0.01}\\). note: \\(G(x)=\\sum_i^La_iL_i\\), where \\(L_i\\) is the sum of the output nodes of layer \\(i\\)\n\\(L\\approx O(\\log\\log d)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nshallow model will not learn efficiently, usually \\(d^{2^L}\\) samples, while deep model uses \\(2^{2^L}\\) samples which is \\(\\text{poly}(d)\\)"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#intuition-proof",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#intuition-proof",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "intuition proof",
    "text": "intuition proof\n\nStep 1: about overparameterization\nIf we wish to learn \\(G(x)=x_1^2+x_2^2+\\alpha(x_1^4+x_2^4)\\), \\(\\alpha=0.1\\)\nWe hope the first layer to learn \\(x_1^2\\), \\(x_2^2\\), second layer to learn \\(\\alpha(x_1^4+x_2^4)\\)\nbut first layer may give an output which we cannot reconstruct \\(x_1^4+x_2^4\\) from\nsolution: over-parametrization and Gaussian random init\n\n\n\n\n\n\nconclusion\n\n\n\nrich representation for the next layer (not necessary useful for current layer)\n\n\n\n\nStep 2\nIf we wish to learn \\(G(x)=x_1^2+x_2^2+\\alpha((x_1^2+x_3)^2+(x_2^2+x_4)^2)\\), \\(\\alpha=0.1\\)\nChances are that the first layer learns \\((x_1+\\alpha x_3)^2+(x_2+\\alpha x_4)^2\\) from which the next layer cannot reconstruct the remaining terms\n\n\n\n\n\n\nclaim\n\n\n\nlayer-wise training overfits to higher-level signals, not noise\n\n\nsolution: training both layers together, second layer will fix the first layer\n\n\nStep 3\nfirst layer: \\(\\alpha\\) close (to the ground-truth poly)\n\\(\\xrightarrow{learn}\\) second layer (learns the residual): \\(\\alpha^2\\) close\n\\(\\xrightarrow{correction}\\) first layer (correction): \\(\\alpha^2\\) close\n\\(\\xrightarrow{re-learn}\\) second layer: \\(\\alpha^4\\) close\n‚Ä¶\nmore layers means more corrections to the previous layers\n\n\n\n\n\n\nclaim\n\n\n\nlayers are learned simoutaneously\n\n\nimportant: backward feature correction"
  },
  {
    "objectID": "posts/research/MSR-AI-Seminar-Feb-8.html#feature-visualization",
    "href": "posts/research/MSR-AI-Seminar-Feb-8.html#feature-visualization",
    "title": "MSR AI Seminar: Why Does Deep Learning Perform Deep Learning?",
    "section": "feature visualization:",
    "text": "feature visualization:\n\n\n\nBy Google AI, 2017\n\n\n\nfind the picture that activates a specific neuron the most by Gradient Descent\nweakness: relies on strong regularization to make it more like a image, otherwise a meaningless noise picture\n\n\n\n\n\n\nrelated\n\n\n\nadversarial perturbation"
  },
  {
    "objectID": "posts/inequality.html",
    "href": "posts/inequality.html",
    "title": "Inequality",
    "section": "",
    "text": "I found a great book on concentration inequalities\nConcentration inequalities: A nonasysmtotic theory of independence by Boucheron\nhope I can read this book thoroughly one day :/"
  },
  {
    "objectID": "posts/tools/tc.html",
    "href": "posts/tools/tc.html",
    "title": "Total Commander Tips",
    "section": "",
    "text": "xbeta blog about TC\nA famous introductory blog to Total Commander.\nTotal Commander is a powerful customizable file manager tool. It has enormous features that are totally beyond one‚Äôs imagination."
  },
  {
    "objectID": "posts/tools/tc.html#config-files",
    "href": "posts/tools/tc.html#config-files",
    "title": "Total Commander Tips",
    "section": "Config files",
    "text": "Config files\n.ini config file is recommended to be put under the installation directory.\nFor better management.\nAlso set UseIniInProgramDir=7, ignore register file.\nxbeta‚Äôs config file for learning."
  },
  {
    "objectID": "posts/tools/tc.html#location-and-selection",
    "href": "posts/tools/tc.html#location-and-selection",
    "title": "Total Commander Tips",
    "section": "location and selection",
    "text": "location and selection\n\ndirectory hotlist\nCtrl+d x\n\nhotlist = most used directorys\nextremely fast to navigate\n\n\n\n\n\n\n\nTip\n\n\n\n\nuse subdirectory in hotlist\ntidy up the hotlist periodically, e.g., make a Ctrl+d n (now) sublist for most frequent dirs recently.\n\n\n\n\n\nchange disk\nAlt+F1 for left panel, Alt+F2 for right panel\n\n\ncommon way of changing dir\n\nEnter or Backspace\nCtrl+\\ to root dir\n\n\n\nquick search\nmy config:\nConfiguration -> Quick Search\n\ntick Letters only\nuncheck Beginning (...)\ncheck Ending (...)\n\n\n\nHistory\nAlt+‚Üì\n\n\nSorting\nSort with multiple keys.\nE.g.,\n\nFirst click Ext\nThen, click Date, Name while pressing Ctrl\n\n\n\nFiltering\nShow only the desired files\nShow -> Custom...\n\n\nColoring\nConfiguration -> Color -> Define colors by file type...\n\n2days as red\nexe as purple\n7days as blue\n\n\n\nSelecting\nRecommend to use the default NC mode (right single click to select)\n\nConfiguration -> Operation\n\nLong right press for right menu.\n\n\nmultiple selection\n\nAlt+'+' ('+' in the numeric keypad of keyboard): also select files with the same extension (you have already been selecting a file)\n* (also in the numeric keypad): reverse selection\n+ and -: also select or unselect some files (pops up a window for you to choose)\nInsert for a single selection, function like right single click and then ‚Üì.\n\n\n\n\nBranch View\nCtrl+b: show all files in current and sub- directories."
  },
  {
    "objectID": "posts/tools/tc.html#pack-unpack",
    "href": "posts/tools/tc.html#pack-unpack",
    "title": "Total Commander Tips",
    "section": "Pack / Unpack",
    "text": "Pack / Unpack\nPack:\n\nAlt+F5 / Ctrl+Alt+F5\nTo save the archive in the current dir, set CA+F5=cm_PackFiles in [Shortcuts] session of wincmd.ini file. Ctrl+Alt+F5\n\nUnpack:\n\nAlt+F9, then clear the input field (empty means current folder)\ncheck Unpack each archive ... to save in a subfolder\n\n\n\n\n\n\n\nTip\n\n\n\nto make it a default manner (save in a subfolder instead of current folder), set UnZIPSeperateSubdirs=-2 in [Packer] section of wincmd.ini.\n\n\nBrowsing packed file:\n\nbrowse like a normal folder\nCtrl + ‚Üí or Ctrl + ‚Üê or Ctrl + PageDown: open in right/left/current panel"
  },
  {
    "objectID": "posts/tools/tc.html#rename",
    "href": "posts/tools/tc.html#rename",
    "title": "Total Commander Tips",
    "section": "rename",
    "text": "rename\nShift+F6 is the default (change filename), press again to include extension\nuse F2 is recommended.\nadd F2=cm_RenameOnly in [Shortcuts] section"
  },
  {
    "objectID": "posts/tools/tc.html#addons",
    "href": "posts/tools/tc.html#addons",
    "title": "Total Commander Tips",
    "section": "Addons",
    "text": "Addons\n\nxbeta plugin collections\nofficial addon collections\n\n\nmy config file, mod a little from xbeta\nfor more details and usage, please refer to the xbeta blog or F1 help manual."
  },
  {
    "objectID": "posts/english/vocab-example.html",
    "href": "posts/english/vocab-example.html",
    "title": "vocabs with simple examples",
    "section": "",
    "text": "some words with explanations (in Eng.) and simple examples\nall the contents are from Combridge Dictionary, I just did some picking up"
  },
  {
    "objectID": "posts/english/vocab-example.html#relate",
    "href": "posts/english/vocab-example.html#relate",
    "title": "vocabs with simple examples",
    "section": "relate",
    "text": "relate\nto be able to understand a situation or someone‚Äôs feelings because you have experienced something similar yourself:\n\nI often wake very early - I‚Äôm sure most readers over 50 can relate\n\nto tell a story or describe a series of events:\n\nShe related the events of the previous week to the police."
  },
  {
    "objectID": "posts/english/vocab-example.html#persist",
    "href": "posts/english/vocab-example.html#persist",
    "title": "vocabs with simple examples",
    "section": "persist",
    "text": "persist\nIf an unpleasant feeling or situation persists, it continues to exist:\n\nIf the pain persists, consult a doctor.\nThe cold weather is set to persist throughout the week.\n\nto try to do or continue doing something in a determined but often unreasonable way:\n\nIf he persists in asking awkward questions, then send him to the boss"
  },
  {
    "objectID": "posts/english/vocab-example.html#holistically",
    "href": "posts/english/vocab-example.html#holistically",
    "title": "vocabs with simple examples",
    "section": "holistically",
    "text": "holistically\nin a way that deals with or treats the whole of something or someone and not just a part:\n\nThe problem needs to be addressed holistically.\nNutrition is being viewed more holistically as a health issue."
  },
  {
    "objectID": "posts/english/vocab-example.html#entice",
    "href": "posts/english/vocab-example.html#entice",
    "title": "vocabs with simple examples",
    "section": "entice",
    "text": "entice\nto persuade someone to do something by offering them something pleasant:\n\nThe adverts entice the customer into buying things they don‚Äôt really want.\nPeople are being enticed away from the profession by higher salaries elsewhere.\nA smell of coffee in the doorway enticed people to enter the shop."
  },
  {
    "objectID": "posts/english/vocab-example.html#embrace",
    "href": "posts/english/vocab-example.html#embrace",
    "title": "vocabs with simple examples",
    "section": "embrace",
    "text": "embrace\nto accept something enthusiastically:\n\nThis was an opportunity that he would embrace.\n\nto hold someone tightly with both arms to express love, liking, or sympathy, or when greeting or leaving someone:\n\nShe saw them embrace on the station platform.\n\nto include something, often as one of a number of things:\n\nLinguistics embraces a diverse range of subjects such as phonetics and stylistics."
  },
  {
    "objectID": "posts/english/vocab-example.html#bolster",
    "href": "posts/english/vocab-example.html#bolster",
    "title": "vocabs with simple examples",
    "section": "bolster",
    "text": "bolster\nto support or improve something or make it stronger:\n\nMore money is needed to bolster the industry."
  },
  {
    "objectID": "posts/english/vocab-example.html#surveillance",
    "href": "posts/english/vocab-example.html#surveillance",
    "title": "vocabs with simple examples",
    "section": "surveillance",
    "text": "surveillance\nthe careful watching of a person or place, especially by the police or army, because of a crime that has happened or is expected:\n\nThe police have kept the nightclub under surveillance because of suspected illegal drug activity."
  },
  {
    "objectID": "posts/english/vocab-example.html#antibiotic",
    "href": "posts/english/vocab-example.html#antibiotic",
    "title": "vocabs with simple examples",
    "section": "antibiotic",
    "text": "antibiotic\na medicine that can destroy harmful bacteria or limit their growth\n\nI‚Äôm taking antibiotics for a throat infection."
  },
  {
    "objectID": "posts/english/vocab-example.html#radiate",
    "href": "posts/english/vocab-example.html#radiate",
    "title": "vocabs with simple examples",
    "section": "radiate",
    "text": "radiate\nto produce heat and light\n\nThe planet Jupiter radiates twice as much heat from inside as it receives from the Sun.\n\nto show an emotion or quality, or (of an emotion or quality) to be shown or felt:\n\nHe was radiating joy and happiness."
  },
  {
    "objectID": "posts/english/vocab-example.html#voyage",
    "href": "posts/english/vocab-example.html#voyage",
    "title": "vocabs with simple examples",
    "section": "voyage",
    "text": "voyage\na long journey, especially by ship\n\nHe was a young sailor on his first sea voyage.\n\nto travel\n\nIn their little boat they planned to voyage to distant lands."
  },
  {
    "objectID": "posts/english/vocab-example.html#invert",
    "href": "posts/english/vocab-example.html#invert",
    "title": "vocabs with simple examples",
    "section": "invert",
    "text": "invert\nto turn sth. upside down or change the order of two things\n\nIn some languages, the word order in questions is inverted"
  },
  {
    "objectID": "posts/english/vocab-example.html#constitution",
    "href": "posts/english/vocab-example.html#constitution",
    "title": "vocabs with simple examples",
    "section": "constitution",
    "text": "constitution\nthe set of political principles\nthe general state of someone‚Äôs health:\n\nHe has a very strong constitution.\n\nhow something is made up of different parts:\n\nthe constitution of a chemical compound"
  },
  {
    "objectID": "posts/english/vocab-example.html#stuffy",
    "href": "posts/english/vocab-example.html#stuffy",
    "title": "vocabs with simple examples",
    "section": "stuffy",
    "text": "stuffy\nA stuffy room or building is unpleasant because it has no fresh air:\n\nIt‚Äôs really hot and stuffy in here - let‚Äôs open the window.\n\nold-fashioned, formal, and boring:\n\nShe is trying to promote a less stuffy image of librarians.\n\nstuffy nose"
  },
  {
    "objectID": "posts/english/vocab-example.html#propagate",
    "href": "posts/english/vocab-example.html#propagate",
    "title": "vocabs with simple examples",
    "section": "propagate",
    "text": "propagate\nproduce a new plant using a parent plant (or animal)\nto spread opinions, lies, or beliefs among a lot of people:\n\nThe government have tried to propagate the belief that this is a just war.\n\nto send out or spread light or sound waves, movement, etc., or to be sent out or spread:\n\nHow are sound waves propagated?"
  },
  {
    "objectID": "posts/english/vocab-example.html#penalize",
    "href": "posts/english/vocab-example.html#penalize",
    "title": "vocabs with simple examples",
    "section": "penalize",
    "text": "penalize\nto cause someone a disadvantage:\n\nThe present tax system penalizes poor people.\n\nto punish someone, esp.¬†for breaking the law or a rule:\n\nThe new law penalizes the taxpayers who can least afford to pay."
  },
  {
    "objectID": "posts/english/vocab-example.html#graze",
    "href": "posts/english/vocab-example.html#graze",
    "title": "vocabs with simple examples",
    "section": "graze",
    "text": "graze\nto break the surface of the skin by rubbing against something rough:\n\nHe fell down and grazed his knee.\n\nIf an object grazes something, it touches its surface lightly when it passes it:\n\nThe aircraft‚Äôs landing gear grazed the treetops as it landed.\n\neat grass\ninjury"
  },
  {
    "objectID": "posts/english/vocab-example.html#sustain",
    "href": "posts/english/vocab-example.html#sustain",
    "title": "vocabs with simple examples",
    "section": "sustain",
    "text": "sustain\nto cause or allow something to continue for a period of time:\n\nThe economy looks set to sustain its growth into next year.\n\nto keep alive\n\nThe soil in this part of the world is not rich enough to sustain a large population.\n\nto suffer or experience, especially damage or loss:\n\nShe sustained multiple injuries in the accident.\n\nto support emotionally:\n\nShe was sustained by the strength of her religious faith."
  },
  {
    "objectID": "posts/english/vocab-example.html#steer",
    "href": "posts/english/vocab-example.html#steer",
    "title": "vocabs with simple examples",
    "section": "steer",
    "text": "steer\nto control the direction of a vehicle\n\nThis car is very easy to steer.\n\nIf a vehicle steers, it follows a particular route or direction:\n\nThe ship passed Land‚Äôs End, in Cornwall, then steered towards southern Ireland.\n\nto take or make sb./sth. go in the direction you want them\n\nShe steered her guests into the dining room."
  },
  {
    "objectID": "posts/english/vocab-example.html#comprise",
    "href": "posts/english/vocab-example.html#comprise",
    "title": "vocabs with simple examples",
    "section": "comprise",
    "text": "comprise\nto have things or people as parts or members; to consist of:\n\nThe course comprises a class book, a practice book, and a CD.\n\nto be the parts or members of something; to make up something:\n\nItalian students comprise 60 percent of the class.\nThe class is comprised mainly of Italian and French students."
  },
  {
    "objectID": "posts/english/vocab-example.html#prosper",
    "href": "posts/english/vocab-example.html#prosper",
    "title": "vocabs with simple examples",
    "section": "prosper",
    "text": "prosper\nto be or become successful, especially financially:\n\nLots of microchip manufacturing companies prospered at that time."
  },
  {
    "objectID": "posts/english/vocab-example.html#compromise",
    "href": "posts/english/vocab-example.html#compromise",
    "title": "vocabs with simple examples",
    "section": "compromise",
    "text": "compromise\nan agreement in an argument in which the people involved reduce their demands or change their opinion in order to agree:\n\nIt is hoped that a compromise will be reached in today‚Äôs talks.\nIn a compromise between management and unions, a four percent pay rise was agreed in return for an increase in productivity.\n\nto allow your principles to be less strong or your standards or morals to be lower:\n\nDon‚Äôt compromise your beliefs/principles for the sake of being accepted.\nIf we back down on this issue, our reputation will be compromised.\n\nto risk having a harmful effect on something:\n\nWe would never compromise the safety of our passengers."
  },
  {
    "objectID": "posts/english/vocab-example.html#manufacture",
    "href": "posts/english/vocab-example.html#manufacture",
    "title": "vocabs with simple examples",
    "section": "manufacture",
    "text": "manufacture\nto produce goods in large numbers, usually in a factory using machines:\n\nHe works for a company that manufactures car parts.\nThe report notes a rapid decline in manufactured goods.\n\nto invent something, such as an excuse or story, in order to deceive someone:\n\nShe insisted that every scandalous detail of the story had been manufactured.\n\nthe process of producing goods:\n\nOil is used in the manufacture of a number of fabrics."
  },
  {
    "objectID": "posts/english/vocab-example.html#revelation",
    "href": "posts/english/vocab-example.html#revelation",
    "title": "vocabs with simple examples",
    "section": "revelation",
    "text": "revelation\nthe act of making something known that was secret, or a fact that is made known:\n\na moment of revelation\nHis wife divorced him after the revelation that he was having an affair."
  },
  {
    "objectID": "posts/english/vocab-example.html#impart",
    "href": "posts/english/vocab-example.html#impart",
    "title": "vocabs with simple examples",
    "section": "impart",
    "text": "impart\nto communicate information to someone:\n\nto impart the bad news\nI was rather quiet as I didn‚Äôt feel I had much wisdom to impart on the subject.\n\nto give something a particular feeling, quality, or taste:\n\nPreservatives can impart colour and flavour to a product."
  },
  {
    "objectID": "posts/english/vocab-example.html#concession",
    "href": "posts/english/vocab-example.html#concession",
    "title": "vocabs with simple examples",
    "section": "concession",
    "text": "concession\nsomething that is allowed or given up, often in order to end a disagreement, or the act of allowing or giving this:\n\nBoth sides involved in the conflict made some concessions in yesterday‚Äôs talks.\nHe stated firmly that no concessions will be made to the terrorists.\n\nthe act of admitting defeat:\n\nThe former president‚Äôs concession came even before all the votes had been counted.\na concession speech\n\na reduction in the usual price of something, made available to students, old people, etc.:\n\nYou can get travel concessions if you are under 26."
  },
  {
    "objectID": "posts/english/vocab-example.html#trigger",
    "href": "posts/english/vocab-example.html#trigger",
    "title": "vocabs with simple examples",
    "section": "trigger",
    "text": "trigger\na part of a gun that causes the gun to fire when pressed:\n\nIt‚Äôs not clear who actually pulled the trigger.\n\nan event or situation, etc. that causes something to start:\n\nThere are fears that the incident may be a trigger for more violence in the capital.\n\nsomething that causes someone to feel upset and frightened because they are made to remember something bad that has happened in the past:\n\nA trigger is something that sets off a flashback, transporting the person back to the traumatic event.\n\nto cause something to start:\n\nSome people find that certain foods trigger their headaches.\nUltraviolet-B radiation triggers the skin to produce vitamin D.\nThe racial killings at the weekend have triggered off a wave of protests.\n\nto cause a strong emotional reaction of fear, shock, anger, or worry in someone, especially because they are made to remember something bad that has happened in the past:\n\nSeeing him come towards me just triggered me and I screamed.\nHe could be triggered by a loud noise."
  },
  {
    "objectID": "posts/english/vocab-example.html#conform",
    "href": "posts/english/vocab-example.html#conform",
    "title": "vocabs with simple examples",
    "section": "conform",
    "text": "conform\nto behave according to the usual standards of behaviour that are expected by a group or society:\n\nAt our school, you were required to conform, and there was no place for originality."
  },
  {
    "objectID": "posts/english/vocab-example.html#deduce",
    "href": "posts/english/vocab-example.html#deduce",
    "title": "vocabs with simple examples",
    "section": "deduce",
    "text": "deduce\nto reach an answer or a decision by thinking carefully about the known facts:\n\nWe cannot deduce very much from these figures.\nThe police have deduced that he must have left his apartment yesterday evening."
  },
  {
    "objectID": "posts/english/vocab-example.html#antiseptic",
    "href": "posts/english/vocab-example.html#antiseptic",
    "title": "vocabs with simple examples",
    "section": "antiseptic",
    "text": "antiseptic\na chemical used for preventing infection in an injury, especially by killing bacteria:\n\nAntiseptic is used to sterilize the skin before giving an injection.\nMany of the ingredients for antiseptics come from the rainforests.\n\ncompletely free from infection:\n\nIn the 1870s and 1880s, doctors began to follow the principles of antiseptic surgery.\n\ntoo clean and showing no imagination and character:\n\nThere‚Äôs an antiseptic feeling to the new town centre, with its covered shopping mall."
  },
  {
    "objectID": "posts/english/vocab-example.html#patronage",
    "href": "posts/english/vocab-example.html#patronage",
    "title": "vocabs with simple examples",
    "section": "patronage",
    "text": "patronage\nthe support given to an organization by someone:\n\nThe charity enjoys the patronage of many prominent local business people.\n\nthe power of a person to give someone an important job or position:\n\nPatronage is a potent force if used politically."
  },
  {
    "objectID": "posts/english/vocab-example.html#whip",
    "href": "posts/english/vocab-example.html#whip",
    "title": "vocabs with simple examples",
    "section": "whip",
    "text": "whip\nto bring or take something quickly:\n\nShe whipped a handkerchief out of her pocket and wiped his face.\nHe whipped the covers off the bed.\nI was going to pay but before I knew it he‚Äôd whipped out his credit card.\nThey whipped my plate away before I‚Äôd even finished.\n\nto (cause something to) move quickly and forcefully:\n\nThe wind whipped across the half-frozen lake.\nA fierce, freezing wind whipped torrential rain into their faces.\n\nto beat food, especially cream, with a special piece of equipment in order to make it thick and firm:\n\nCould you whip the cream for me?\nTry whipping a little brandy or other liqueur into the cream.\nTop with whipped cream and a sprinkle of sugar.\n\nto hit a person or animal with a whip:\n\nI don‚Äôt like the way the drivers whip their horses.\n\nto defeat a person or a team in a competition, especially in a sport:\n\nThey beat us last time, but we whipped them in a rematch.\nHe whipped him in their fight two years ago."
  },
  {
    "objectID": "posts/english/vocab-example.html#perquisite",
    "href": "posts/english/vocab-example.html#perquisite",
    "title": "vocabs with simple examples",
    "section": "perquisite",
    "text": "perquisite\nan advantage or something extra, such as money or goods, that you are given because of your job:\n\nThe perquisites of this job include health insurance and a performance bonus."
  },
  {
    "objectID": "posts/english/vocab-example.html#elaborate",
    "href": "posts/english/vocab-example.html#elaborate",
    "title": "vocabs with simple examples",
    "section": "elaborate",
    "text": "elaborate\ncontaining a lot of careful detail or many detailed parts:\n\nYou want a plain blouse to go with that skirt - nothing too elaborate.\nThey‚Äôre making the most elaborate preparations for the wedding.\nHe came out with such an elaborate excuse that I didn‚Äôt quite believe him.\n\nto add more information to or explain something that you have said:\n\nThe congresswoman said she was resigning, but refused to elaborate on her reasons for doing so."
  },
  {
    "objectID": "posts/english/vocab-example.html#merchandise",
    "href": "posts/english/vocab-example.html#merchandise",
    "title": "vocabs with simple examples",
    "section": "merchandise",
    "text": "merchandise\ngoods that are bought and sold:\n\nShoppers complained about poor quality merchandise and high prices.\nJapan exported $117 billion in merchandise to the US in 1999.\n\nto encourage the sale of goods by advertising them or by making certain that they are noticed:\n\nShe had to merchandise the new product line."
  },
  {
    "objectID": "posts/english/vocab-example.html#dividend",
    "href": "posts/english/vocab-example.html#dividend",
    "title": "vocabs with simple examples",
    "section": "dividend",
    "text": "dividend\n(a part of) the profit of a company that is paid to the people who own shares in it:\n\nDividends will be sent to shareholders.\nIn addition to their salary, employees receive a profit-related dividend."
  },
  {
    "objectID": "posts/english/vocab-example.html#scatter",
    "href": "posts/english/vocab-example.html#scatter",
    "title": "vocabs with simple examples",
    "section": "scatter",
    "text": "scatter\nto (cause to) move far apart in different directions:\n\nThe protesters scattered at the sound of gunshots.\nThe soldiers came in and scattered the crowd.\n\nto cover a surface with things that are far apart and in no particular arrangement:\n\nScatter the powder around the plants.\nI scattered grass seed all over the lawn.\nI scattered the whole lawn with grass seed."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html",
    "href": "posts/understanding-machine-learning/UML-05.html",
    "title": "Understanding Machine Learning 05",
    "section": "",
    "text": "here we go to one of the famous theories, the VC dimension\nbefore going a little deeper, we will have a look at the motivation\n\n\nfirst, finite is not a sufficient and necessary condition of PAC learnability. exercise 3.3 is a simple example. Moreover, in the last post (No-Free-Lunch theorem), we have seen H that contains all possible functions is not PAC learnable. When we rethink the proof, we may notice that the construction of set \\(C\\) is the key point. Since we are considering all possible functions, the error of different f‚Äôs can cancel, resulting in a large error.\nborrow the idea, if we can find a subset \\(C\\) of domain \\(\\mathcal{X}\\), and if H contains all the functions when taking \\(C\\) as the domain, then it will cause a considerable risk using the same proof\nfurther, if such kind of \\(C\\) is infinitely large, then \\(H\\) is not learnable\nthe thoughts above give us the basic idea of how the VC dimension comes\n\n\n\n\nDef. restriction of H to C\nhere, \\(\\mathcal{C}=\\{c_1,c_2,\\dots,c_m\\}\\subseteq\\mathcal{X}\\)\nand \\(\\mathcal{H}_C=\\{h(c_1),\\dots,h(c_m)\\}\\)\n\nDef. Shattering\nH shatters C \\(\\iff\\) \\(|H_C|=2^{|C|}\\)\n\nDef. VC-dimension\nthe VC-dimension of H \\(\\coloneq\\max_C\\{|H_C|:\\text{H shatters C}\\}\\)\n\na simple method to show VCdim=d we need to show that\n\n\\(\\exists C\\) of size d that is shattered by H\nevery C of size d+1 is not shattered by H\n\n\nsince \\(2^{|VCdim(H)|}\\le |H|\\), we have a loose upper bound of VC dim which is \\(log_2(|H|)\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html#fundamental-theorem-of-pac-learning",
    "href": "posts/understanding-machine-learning/UML-05.html#fundamental-theorem-of-pac-learning",
    "title": "Understanding Machine Learning 05",
    "section": "fundamental theorem of PAC learning",
    "text": "fundamental theorem of PAC learning\nthe followings are eq\n\nuniform convergence\nPAC learnable on every ERM learner\nagnostic PAC learnable on every ERM learner\nfinite VC dimension\n\n\nfrom 4. to 1. is non-trivial. here we will go deeper into that\nwe need to find \\(m_H^U\\) s.t. for any S that \\(|S|\\ge m_H^U\\) is \\(\\frac{\\epsilon}{2}\\)-representative (i.e.¬†\\(L_D(h)-L_S(h)|\\le \\frac{\\epsilon}{2}\\))\nintuitively, if \\(m\\) is larger than \\(d\\), where \\(d\\) is the VC dimension, \\(H_C\\) will only take small part of \\(2^C\\) (in fact that is polynomial large w.r.t. \\(|C|\\)), the estimation error could be bounded by \\(o(m)\\)\n\nSauer‚Äôs Lemma\n\nDef. Growth Function\n\\[\\tau_H(m)=\\max_{C\\subset \\mathcal{X}:|C|=m}|H_C|.\\]\n\naccording to Sauer‚Äôs lemma, \\(\\tau_H(m)\\le \\sum_{i=0}^d\\tbinom{n}{i}\\), if \\(m\\gt d+1\\), we have a looser but neat form \\(\\tau_H(m)\\ge (em/d)^d\\)\n\nproof\nbasic idea\nwe prove a stronger claim, \\(\\forall C=\\{c_1,\\dots,c_m\\}\\)\nwe have \\(\\forall H,\\ |H_C|\\le |\\{B\\subseteq C: \\text{H shatters B}\\}|\\)\ninduction on m\nwhen m=1, both sides are equal\nwhen m=k+1, suppose the claim holds for \\(m\\le k\\).\nto use the claim, we need to split \\(C\\) as \\(\\{c_1\\}\\) and ${c_1,, c_m} which is denoted as \\(C'\\)\nwe need to find such \\(H'\\) that can naturally convert from \\(C'\\) to \\(C\\) but still holds the shattering property\nnote:\n\\[\nH_C=H_{C'}\\oplus H'_{C'}\n\\]\nwhere \\(H'_{C'}=\\{\\exists f(c_1)=1\\land g(c_1)=0\\land f_{C'}=g_{C'}, \\text{ where }f,g\\in H\\}\\), \\(\\oplus\\) means direct sum (\\(H_{C'}\\cap H'_{C'}=\\emptyset\\))\n\nif \\(f_{C'}\\) and \\(g_{C'}\\) are exactly the same, they represent the same function in \\(H_{C'}\\), and hence only count once in \\(H_{C'}\\) which should be treated separatedly in \\(H_C\\).\n\nso we have that\n\\[\n\\begin{aligned}\n    |H_C|&=|H_{C'}|+|H'_{C'}|\\\\\n    &\\le |\\{B\\subseteq C':\\text{H shatters B}\\}|+|\\{B\\subseteq C':\\text{H' shatters B}\\}|\n\\end{aligned}\n\\]\nnote that \\(H'\\) shatters B \\(\\iff\\) H shatters \\(\\{c_1\\}+B\\)\nso RHS \\(\\le |\\{B\\subseteq C:\\text{H shatters B}\\}|\\) \\(\\blacksquare\\)\n\n\n\nuniform convergence\nwe will give the upper bound without proof :(\nfor every \\(\\delta\\), with prob \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\n|L_D(h)-L_S(h)|\\le \\frac{4+\\sqrt{log(\\tau_H(2m))}}{\\delta \\sqrt{2m}}\n\\]\nRHS \\(\\in o(m)\\)\n\nI‚Äôll add the proof when I master this math skill :("
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-05.html#exercise",
    "href": "posts/understanding-machine-learning/UML-05.html#exercise",
    "title": "Understanding Machine Learning 05",
    "section": "exercise",
    "text": "exercise\n\n6.1. monotonicity of VC dim\n6.2. finite X, \\(k\\lt |\\mathcal{X}|\\), compute the VC dim\n\n\\(H_{=k}^\\mathcal{X}=\\{h\\in \\{0,1\\}^\\mathcal{X}:|x:h(x)=1|=k\\}\\). VC dim=\\(\\min\\{k,|\\mathcal{X}|-k\\}\\), otherwise you can‚Äôt assign all one (zero) to these instances\n\\(H_{\\text{at most k}}^\\mathcal{X}=\\{|x:h(x)=1|\\le k \\text{ or }|x:h(x)=0|\\le k\\}\\). VC dim=k.\n\n6.3. parity function \\(h_I\\) (h computes the parity of bits at \\(I\\) ). finte H, \\(d\\le n\\), and easy to construct such \\(C\\) that \\(|C|=n\\)\n6.4. skip\n6.5. The VC dim of axis-aligned rectangles in d-dim is 2d. Since we need 2d points to construct a rectangle that assigns all points as one if there are 2d+1 points, one of them must stay inside the box (or lie on the border with no points in the box). With that point assigned as zero, others assigned as one, no function will satisfy\n6.6. VC-dimension of Boolean conjuntions \\(H_{con}^d\\): \\(f(x)=x_{i_1}\\land \\dots \\land x_{i_k}\\)\n\nprove \\(|H_{con}^d|\\le 3^d + 1\\). each \\(x_i\\) has three states, not chosen, origin \\(x_i\\), inverse \\(\\bar{x}_i\\), so should be \\(= 3^d\\) ??\nprove \\(VCdim(H)\\le d log 3\\). \\(VDdim\\le log(|H|)=dlog3\\)\nshow that \\(H_{con}\\) shatters \\(\\{e_i:i\\le d\\}\\), seems easy\nshow that \\(VCdim(H)\\le d\\). if \\(d+1\\), consider hypothesis \\(h_i(c_j) = \\begin{cases}  0 &\\text{if } i=j \\\\  1 &\\text{otherwise}  \\end{cases}\\) .that means each \\(c_i\\) has at least one bit that is different from others, which is a contradiction\n\\(H_{mcond}^d\\) which do not contain negations, empty conjunction is considered as all positive, the vc dim of \\(H_{mcond}^d\\) with all negative h =d.¬†first \\(VCdim(H_{mcond})\\le d\\). consider \\(x_i\\) is all one but zero at index i, \\(i\\le d\\)\n\n6.7. skip\n6.8. show vcdim of \\(H=\\{x\\mapsto \\lceil sin(\\theta x) \\rceil: \\theta \\in \\mathbb{R}\\}\\) is infinity.\n\nhint: if \\(0.x_1x_2x_3\\dots\\) is the binary expansion of \\(x\\in (0,1)\\), then for any natual number m, \\(\\lceil sin(2^m\\pi x) \\rceil =(1-x_m)\\), provided that \\(\\exist k\\ge m\\) s.t. \\(x_k=1\\).\nwith the hint, that is easy. Just construct a huge matrix with each row representing a binary expansion of \\(x_i\\) and the columns running through all possible combinations.\n\n6.9. skip\n6.10. skip\n6.11. VC of union. \\(H_1,\\dots,H_r\\).\n6.12. Dudley classes.\n\n\\(VCdim(POS(\\mathcal{F}+g))=VCdim(POS(\\mathcal{F}))\\). ‚Äú\\(\\mathbb{R}ightarrow\\)‚Äù, \\((f_1+g)-(f_2+g)\\).\n\\(VCdim(POS(\\mathcal{F}))=\\dim(\\mathcal{F})\\). half space, full rank, bla bla bla\nexamples of Dudley classes\n\n\nbtw, Dudley class seems to be a fascinating and important topic"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-03.html",
    "href": "posts/understanding-machine-learning/UML-03.html",
    "title": "Understanding Machine Learning 03",
    "section": "",
    "text": "Before, the way of choosing \\(m_H\\) and the effect of \\(\\delta\\) was related to the learner. However, we can borrow the concept of uniform convergence from analysis to make it independent of what learner you use.\nhere, we can treat \\(L_S(h)\\) as \\(\\sum_i^nf_i(x)\\) since they are both intermediate vals during the convergence, and \\(L_D(h)\\) to be the final end\n\n\n\\(S\\) is \\(\\epsilon\\)-representation sample (w.r.t domain, \\(H\\), \\(l\\) and \\(D\\)) if\n\\[\n\\forall h\\in\\mathcal{H},\\, |L_S(h)-L_D(h)|\\le \\epsilon\n\\]\nlemma\nif S is \\(\\frac{\\epsilon}{2}\\)-representative, then \\(\\forall h_S\\in\\argmin_{h\\in\\mathcal{H}}L_S(h)\\)\n\\[\nL_D(h_S)\\le \\min_{h\\in\\mathcal{H}}L_D(h)+\\epsilon\n\\]\nthrough this lemma, we can immediately have\n\\(S\\) is \\(\\frac{\\epsilon}{2}\\)-representative with prob \\(1-\\delta\\) \\(\\implies\\) Agnostic PAC learnability\n\n\n\nH has the uniform convergence \\(\\coloneqq\\) exists a func \\(m_H^{UC}(\\epsilon, \\delta)\\), for every \\(D\\), if \\(|S|\\gt m_H^{UC}\\) then with \\(1-\\delta\\) prob, it is \\(\\epsilon\\)-representative\nIt seems stronger than the original agnostic PAC, just like the rel between uniform conv and normal conv in analysis. normal conv only cares the situation in a certain area (here the decider generated by the learner), while uni conv holds on the whole area (all \\(h\\in\\mathcal{H}\\))\n\n\n\\(m_H\\le m_H^{UC}\\) if \\(H\\) has the uni conv property"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-03.html#situation-of-the-finite-h-class",
    "href": "posts/understanding-machine-learning/UML-03.html#situation-of-the-finite-h-class",
    "title": "Understanding Machine Learning 03",
    "section": "situation of the finite H class",
    "text": "situation of the finite H class\nneed to find \\(m\\), so that\n\\[\nD^m(\\{S:\\forall h\\in \\mathcal{H},|L_S(h)-L_D(h)|\\le\\epsilon\\})\\ge 1-\\delta\n\\]\nand we may convert it into a more familiar form (convenient for using inequalities)\n\\[\nD^m(\\{S:\\exists h\\in \\mathcal{H},|L_S(h)-L_D(h)|\\gt\\epsilon\\})\\lt \\delta\n\\]\nusing union bound and Hodeffing inequalities (note that \\(L_D(h)=\\mathbb{E}_{S\\sim D^m}(L_S(h))\\)), we have\n\\[\nLHS\\le\\sum_{h\\in\\mathcal{H}}2exp(-2m\\epsilon^2)\n\\]\nas a corollary, we have the upper bound for finite hypothesis class which is agnostic PAC learnable\n\\[\nm_H^{UC}(\\epsilon,\\delta)\\le\\left\\lceil\\frac{log(2|\\mathcal{H}|/\\delta)}{2\\epsilon^2}\\right\\rceil\n\\]\n\n\n\n\n\n\nsummary\n\n\n\nif uni conv holds, then in most cases, the empirical risks of h in H will faithfully represent their true risks\n\n\n\nexercises\n\n4.1\n\n\n\\(\\forall \\epsilon,\\delta\\gt 0,\\exists m(\\epsilon,\\delta)\\,s.t.\\) \\[\n\\forall m\\ge m(\\epsilon,\\delta),\\,\\mathcal{P}_{S\\sim D^m}[L_D(A(S))\\gt\\epsilon]\\lt\\delta\\]\n\\(\\lim_{m\\to \\infty}\\mathbb{E}_{S\\sim D^m}[L_D(A(S))]=0\\)\n\n1 \\(\\iff\\) 2"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html",
    "href": "posts/understanding-machine-learning/uml-06.html",
    "title": "Understanding Machine Learning 06",
    "section": "",
    "text": "we will summarize different kinds of defs of learnability first"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#learnability",
    "href": "posts/understanding-machine-learning/uml-06.html#learnability",
    "title": "Understanding Machine Learning 06",
    "section": "learnability",
    "text": "learnability\n\nPAC learnability\nH is PAC learnable if the realizability assumption holds and\n\\(\\forall \\epsilon,\\delta\\gt 0\\), there exists a learner A and \\(m_H(\\epsilon,\\delta)\\) s.t. if \\(m\\ge m_H\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le \\epsilon\n\\]\n\n\nagnostic PAC learnability\nH is agnostic PAC learnable if\n\\(\\forall \\epsilon,\\delta\\gt 0\\), there exists a learner A and \\(m_H(\\epsilon,\\delta)\\) s.t. if \\(m\\ge m_H\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le \\min_{h\\in H}\\{L_D(h)\\}+\\epsilon\n\\]\n\n\nuniform convergence property\nH enjoys the uniform convergence property if\n\\(\\forall \\epsilon,\\delta \\gt 0\\), there exists \\(m_H^{UC}(\\epsilon, \\delta)\\) s.t. if \\(m\\ge m_H^{UC}\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\), \\(\\forall h\\in H\\)\n\\[\n|L_S(h) - L_D(h)|\\le\\epsilon\n\\]\nthese defination of learnibility are equal according to the fundamental theory\n\nhere we have another def which has different power with the above"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#nonuniform-learnability",
    "href": "posts/understanding-machine-learning/uml-06.html#nonuniform-learnability",
    "title": "Understanding Machine Learning 06",
    "section": "nonuniform learnability",
    "text": "nonuniform learnability\nwe allow \\(m_H\\) to be non-uniform over h\nH is nonuniform learnable if\n\\(\\forall \\epsilon,\\delta\\gt 0\\) there exists a learner A and \\(m_H^{NU}(\\epsilon,\\delta,h)\\), s.t. for all \\(h\\in H\\), if \\(m\\ge m_H^{NU}(\\epsilon,\\delta,h)\\)\nfor any D, with probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le L_D(h)+\\epsilon\n\\]\nnote when m is decided, and it is that makes non-uniform learnability weaker than PAC\n\nproperty\nH is nonuniform learnable iff H can be expressed as a union of countable \\(H_i\\) with uniform convergence property\nwe can easily construct H that is nonuniform learnable but not PAC learnable which means PAC learnability is stronger\n\n\ngeneric learner\nERM is a fittable learer for PAC learnability\nSRM (structural risk minimization) is a fittable learner for NU learnability\nSRM requires us to provide more prior knowledge on the priority (weights) of \\(H_i\\)‚Äôs.\ndef \\(\\epsilon_n(m,\\delta)=\\min\\{\\epsilon\\in (0,1):m_{H_n}^{UC}(\\epsilon,\\delta)\\le m\\}\\) which means the minimum est error we can get with m samples\nso given m samples\n\\[\n\\forall h\\in H_n,|L_D(h)-L_S(h)|\\le \\epsilon_n(m,\\delta)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nnote m here can be varied (I‚Äôm not so sure about this) or large enough (s.t. \\(\\forall n,\\epsilon_n(m,\\delta)\\le \\epsilon\\)) during training\n\n\nwhen we put this on a larger range (\\(H_n\\rightarrow H\\)) directly, we can‚Äôt garantee we satisfy the \\(\\delta\\) constraint, since each one has relatively low confidence \\(1-\\delta\\). So we have to split the confidence to each \\(H_i\\), that is providing weights \\(\\sum_{n\\in\\mathbb{N}}w(n)\\le 1\\) on each \\(\\delta\\) (use union bound inequality to merge it back)\nto put it formally\ngiven \\(H=\\cup_{n\\in\\mathbb{N}}H_n\\) and \\(\\sum_{n\\in\\mathbb{N}}w(n)\\le 1\\), where \\(H_i\\) satisfy UC property, then\nwith probatility of at least \\(1-\\delta\\) over the choice \\(S\\sim D^m\\)\nfor any \\(n\\in\\mathbb{N}\\) and \\(h\\in H_n\\)\n\\[\n|L_D(h)-L_S(h)|\\le \\epsilon_n(m,w(n)\\cdot \\delta)\n\\]\nwhich implies for \\(\\forall h\\in H\\)\n\\[\nL_D(h)\\le L_S(h)+\\min_{n:h\\in H_n}\\epsilon_n(m,w(n)\\cdot\\delta)\n\\]\nif we make it simpler (but looser), let \\(n(h)=min\\{n:h\\in H_n\\}\\)\n\\[\nL_D(h)\\le L_S(h)+\\epsilon_n(m,w(n(h))\\cdot\\delta)\n\\]\nSO, SRM is to minimizing the RHS\nwe can proof that \\(L_D(A(S))\\le L_D(h)+\\epsilon\\) with p at least \\(1-\\delta\\) over the choice of S (if \\(m\\ge m_{H_{n(h)}}^{UC}(\\epsilon/2,w(n(h))\\cdot \\delta)\\))\n\nin fact, any converged sumations should be ok for w, like \\(w(n)=\\frac{6}{n^2\\pi^2}\\)\nintuitively, \\(H_n\\) with larger \\(w(n)\\) will need less samples since it is required for less confidence, we actually focus on some hypothesis classes instead of treat all \\(H_n\\) evenly.\nsecond, if \\(h_1\\) and \\(h_2\\) has the same empirical risk, we will prefer the one with higher weight if using SRM\nseems familiar? sounds like the principle Occam‚Äôs razor\n\n\ndescription length\nwe now consider a countable \\(H\\). it can be expressed as a union of singleton class \\(H_i=\\{h_i\\}\\) and for each \\(H_i\\), \\(m_{H_i}^{UC}(\\epsilon,\\delta)= \\left\\lceil\\frac{log(2/\\delta)}{2\\epsilon^2}\\right\\rceil\\)\nthen \\(e_n(m,w(n(h))\\cdot \\delta)= \\sqrt{\\frac{-logw(n(h)) +log(2/\\delta)}{2m}}\\)\ndef description language \\(\\{0,1\\}^\\star\\)\nwe assign each \\(H_i\\) with a description \\(d(h)\\) and denote \\(|h|=|d(h)|\\)\n\n\n\n\n\n\nNote\n\n\n\nif S is a prefix-free set of strings, then\n\\[\n\\sum_{\\sigma\\in S}\\frac{1}{2^{|\\sigma|}}\\le 1\n\\]\n\n\nso \\(w(h)=\\frac{1}{2^{|h|}}\\) is a legal weight function\nand the hypothesis with smaller description length is preferable if they have the same risk\nthat‚Äôs the principle of Occam‚Äôs razor"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#consistency",
    "href": "posts/understanding-machine-learning/uml-06.html#consistency",
    "title": "Understanding Machine Learning 06",
    "section": "consistency",
    "text": "consistency\n\n\n\n\n\n\nNote\n\n\n\nif we let \\(m_H\\) further be dependent on the distribution, we have the def of consistency\n\n\na learner A is consistency with respect to H and P where P is the set of possible distribution D‚Äôs, if\n\\(\\forall \\epsilon,\\delta\\gt 0\\) there exists a learner A and \\(m_H^{NU}(\\epsilon,\\delta,h,D)\\), s.t. for all \\(h\\in H\\) and \\(D\\in P\\), if \\(m\\ge m_H^{NU}(\\epsilon,\\delta,h,D)\\)\nwith probability greater than \\(1-\\delta\\) over the choice of \\(S\\sim D^m\\)\n\\[\nL_D(A(S))\\le L_D(h)+\\epsilon\n\\]\n\n\n\n\n\n\nNote\n\n\n\nif P is the set of all distributions, then A is universally consistent with respect to H\n\n\nthis def of learnability is even weaker than NU\nthe algorithm Memorize is universally consistent which will be overfit in the context of PAC learnability!!! (for every countable domain and finite label set w.r.t. zero-one loss)\ndef Memorize(x)\n    return y if (x,y) in S else 0 # any default value \n\nwhy different ability?\nnote when we determine the \\(m\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/uml-06.html#exercise",
    "href": "posts/understanding-machine-learning/uml-06.html#exercise",
    "title": "Understanding Machine Learning 06",
    "section": "exercise",
    "text": "exercise\n\n7.5. H that contains all functions is not nonuniform learnable\n\nconclusion: \\(H=\\cup_{n\\in\\mathbb{N}}H_n\\), if H shatters an infinite set, the some \\(H_n\\) has infinite VC dim"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html",
    "href": "posts/understanding-machine-learning/UML-02.html",
    "title": "Understanding Machine Learning 02",
    "section": "",
    "text": "def PAC learnability\nhypothesis H is PAC learnable if realizability assumption holds, and exists \\(m_H(\\epsilon,\\delta)\\rightarrow\\mathbb{N}\\)\nwhere with #i.i.d. sample \\(\\ge m_H\\), we always have a probability approx correct h just using ERM."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#removing-realizability-assumption",
    "href": "posts/understanding-machine-learning/UML-02.html#removing-realizability-assumption",
    "title": "Understanding Machine Learning 02",
    "section": "removing realizability assumption",
    "text": "removing realizability assumption\nthe realizability assumption is too strong and unrealistic where we assume the existence of a perfect hypothesis from \\(\\mathcal{H}\\) that can reveal ground truth \\(f\\) with \\(Pr=1\\)\nso, change \\(f(x)\\) into a joint distrib \\(D(x,y)\\) as most researchers would do\ndef generalized risk as \\(L_D(h)\\coloneqq \\mathcal{P}_{(x,y)\\sim D}[h(x)\\neq y]\\coloneqq D(\\{(x,y):h(x)\\neq y\\})\\), just changed \\(D\\) into a joint distrib\nempirical risk is the same\nstill, when take \\(D\\) to be a uniform distrib on \\(S\\) they are eq\nideally, func ‚Äú\\(f_D(x)=1\\text{ if }Pr[y=1|x]\\ge 0.5\\text{ and 0 otherwise}\\)‚Äù is the optimal sol to the gen risk min problem when \\(\\mathcal{Y}=\\{0,1\\}\\), w.r.t 0-1 loss. Bayes optimal sol.\n\nnote: my stupid short proof about the above\nwe need to proof\n\\[\n[f(x)\\neq 0]Pr(y=0|x)+[f(x)\\neq 1]Pr(y=1|x)\\\\\n\\le [g(x)\\neq 0]Pr(y=0|x)\\dots\n\\]\njust consider cond on \\(Pr(y=0|x)\\gt 0.5\\), and it becomes\n\\[\nLHS=Pr(y=1|x)\\\\\n=\\{[g(x)\\neq 0]+[g(x)\\neq 1]\\}Pr(y=1|x)\\\\\n\\le RHS\n\\]\nvery ugly and not clever proof :/ \\(\\blacksquare\\)\n\nhere, we still have the opt function \\(f\\), which minimizes the gen risk but does not minimize it to zero"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#def-agnostic‰∏çÂèØÁü•ËÆ∫ÁöÑ-pac-learnability",
    "href": "posts/understanding-machine-learning/UML-02.html#def-agnostic‰∏çÂèØÁü•ËÆ∫ÁöÑ-pac-learnability",
    "title": "Understanding Machine Learning 02",
    "section": "def Agnostic(‰∏çÂèØÁü•ËÆ∫ÁöÑ) PAC learnability",
    "text": "def Agnostic(‰∏çÂèØÁü•ËÆ∫ÁöÑ) PAC learnability\nbefore: \\(L_{D,f}(h)\\le \\epsilon\\), now: \\(L_{D}(h)\\le min_{g\\in\\mathcal{H}}L_{D}(g)+\\epsilon\\)\nso \\(f\\) above will not be used, but the joint distrib \\(D\\) instead\nand we see that if the realizability assumption holds, it‚Äôs the same as the original PAC learnability\nagnostic just means we can‚Äôt obtain an h with arbitrary small gen risk\nrelative best instead of abs best"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#other-loss-functions",
    "href": "posts/understanding-machine-learning/UML-02.html#other-loss-functions",
    "title": "Understanding Machine Learning 02",
    "section": "other loss functions",
    "text": "other loss functions\nwe can actually use other measures in place of the 0-1 loss, especially in regression problems\nnaturally, we can extend the loss function into a more formal definition, \\(l:\\mathcal{H}\\times\\mathcal{Z}\\rightarrow \\mathbb{R}_+\\), where \\(\\mathcal{Z}\\) is the set of instances, in prediction tasks, it could be \\(\\mathcal{X}\\times\\mathcal{Y}\\)\n\nAgnostic PAC learnability for General Loss Functions\n\\(L_D(h)\\le\\min_{h'\\in\\mathcal{H}}L_D(h')+\\epsilon\\)\nwhere \\(L_D(h)=\\mathcal{E}_{z\\sim D}[l(h,z)]\\)\nnote: \\(l(h,\\dot)\\) is a rand var, it should be measurable.."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-02.html#some-exercises",
    "href": "posts/understanding-machine-learning/UML-02.html#some-exercises",
    "title": "Understanding Machine Learning 02",
    "section": "some exercises",
    "text": "some exercises\n\n3.1. about monotonicity of \\(m_\\mathcal{H}\\) on \\(\\epsilon\\) and \\(\\delta\\) respectively: seems trivial\n3.2. about \\(\\mathcal{H}_{singleton}\\): first, you need to come up with a simple learning alg. if no pos samples appear, just choose \\(h^-\\). this is enough to prove the PAC learnability, as m is large enough, the cost will be small enough\n3.3. about \\(\\mathcal{H}\\) consist of disks. similar to the rect situation from this chap, but simpler, ‚Äôcause it‚Äôs convenient to construct how the bad samples look like\n3.4. about \\(\\mathcal{H}\\) consist of boolean conjunctions. similar to 3.2. it‚Äôs easy to determine f if \\(S\\) contains a positive sample. Otherwise, we just return an all-negative hypothesis\n3.5. about samples from i. but not i.d. from the derivation of the finite hypo corollary, the key is to deal with \\(\\prod_i \\mathcal{P}_{x\\in D_i}(x_i|h(x_i)=f(x_i))\\), where the GA mean ineq could be used to make it as \\((1-\\epsilon)^m\\)\n3.6. ??, since you added the realizability assumption, it‚Äôs naive to have the PAC learnability, maybe?\n3.7. about ideally opt sol of binary classification (w.r.t 0-1 loss). see this\n3.8.1. same question as above, but use abs loss, and consider probabilistic hypothesis (outputs a distribution instead of the deterministic answer), method from here seems enough?\n3.8.2. prove the existence of a learning algo that is better than any others provided \\(D\\). ??? an algo from God that can directly output \\(f\\) from the previous question -_-\n3.8.3. for every learning algo A from \\(D\\), there always exists a B and \\(D'\\) that A is not better than B on \\(D\\). shit, this is also naive from 3.8.2\n3.9. about a variant of PAC learnability, which uses a two-oracle model that allows learner access to \\(D^+\\) and \\(D^-\\) on its preference. the learner can actually change the popularity of the observances, e.g.¬†a learner can take samples from both pos and neg with equal probability (I don‚Äôt understand here so much, does )\n\n\nproof PAC in the standard model \\(\\implies\\) PAC in the two-oracle model. we need to construct a learner from the one in the standard model. suppose the learner puts equal weights on pos and neg samples. It learns a population with equal pos and neg using a standard model. denote the new distribution as \\(D'\\), so\n\n\n\\[\\begin{aligned}\nL_{D'}(h)&=P_{D'}[h\\neq f,f=0]+P_{D'}[h\\neq f, f=1]\\\\\n&=P_{D'}[f=0]P_D[h\\neq f|f=0]+P_{D'}[f=1]P_D[h\\neq f|f=1]\\\\\n&=\\frac{1}{2} L_{D^+}(h)+\\frac{1}{2}L_{D^-}(h)\n\\end{aligned}\n\\]\n\n\nproof PAC in the two-oracle model \\(\\implies\\) PAC in the standard model if \\(h^+,h^-\\in\\mathcal{H}\\). if we have enough samples (\\(m_H\\)), it will contain \\(m^+\\) pos, and \\(m^-\\) neg with high prob, and even it fails to have enough pos (or neg), we can just return \\(h^-\\) ((\\(h^+\\)) and won‚Äôt cost much risk."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-04.html",
    "href": "posts/understanding-machine-learning/UML-04.html",
    "title": "Understanding Machine Learning 04",
    "section": "",
    "text": "What is a universal learner? It means it doesn‚Äôt need any prior knowledge. To be more specific, \\(\\mathcal{H}\\) that contains all possible functions is PAC-learnable. ? that is certainly not true since this is about the NFL theorem.\nTypically, prior knowledge could be assumptions of knowing \\(\\mathcal{D}\\) comes from some family of distribution or assuming \\(\\exists h\\in\\mathcal{H}\\) that \\(L_D(h)\\) is small enough.\n\n\nwe consider binary classification problem and 0-1 loss over \\(\\mathcal{X}\\)\n\\(m\\) be the tr set size smaller than \\(\\frac{|\\mathcal{X}|}{2}\\), A be any learner\nthere is a D that\n\nthere exists \\(f\\), with zero error on \\(\\mathcal{D}\\)\nwith a probability of at least 1/7 over the choice of \\(S\\sim \\mathcal{D}^m\\) we have the error of \\(A(S)\\ge 1/8\\)\n\nLet‚Äôs prove this famous theorem!!!\n\nlet C be a subset of \\(\\mathcal{X}\\) with size 2m as the finite ‚Äòdomain‚Äô we will consider\nthe intuition is that since the learner can only see at most half of the domain set, we can then make the other part of distribution anything we want to defeat A\nwe need to find the D and f\nthere could be like \\(2^{2m}\\) possible f‚Äôs; we can prove that\n\\[\n\\max_{D,f}\\mathbb{E}_{S\\sim D^m}[L_{D,f}(A(S))]\\ge 1/4\n\\]\nwhich means there exists some bad f that will make A fail\nnote D and f should be matched, so we construct \\(D_i\\) as follows for each \\(f_i\\), \\(i=1\\dots T\\) where \\(T=2^{2m}\\)\n\\[\nD_i(x,y) = \\begin{cases}\n            1/|C| &\\text{if } f_i(x)=y \\\\\n            0 &\\text{otherwise}\n           \\end{cases}\n\\]\nsince \\(C\\) is finite, we can enumerate all S\ndenote as \\(S_i\\), \\(i=1\\dots M\\) where \\(M=(2m)^m\\), note instances in S can be duplicated\n\\[\n\\begin{aligned}\n   &\\max_i\\mathbb{E}_{S\\sim D_i}[L_{D_i}(A(S))]\\\\\n   &=\\frac{1}{M}\\max_i\\sum_j^ML_{D_i}(A(S_j^i))\\\\\n   &\\ge\\frac{1}{MT}\\sum_i^T\\sum_j^ML_{D_i}(A(S_j^i))\\\\\n   &=\\frac{1}{TM}\\sum_j^M\\sum_i^TL_{D_i}(A(S_j^i))\\\\\n   &\\ge\\frac{1}{T}\\min_j\\sum_{i}^{T}L_{D_i}(A(S_j^i))\n\\end{aligned}\n\\]\nhere the \\(i\\) in \\(S_j^i\\) means the label is assigned by \\(D_i\\), hence \\(f_i\\)\nWe now convert the problem of finding the \\(f\\) into \\(S_j^i\\) that has a significant error averaged on all functions. Intuitively, since we consider all functions, they must have some disagreement on the samples\nfor \\(S_j^i\\), we let \\(P=\\{x_i\\in C|x_i\\notin S_j^i\\}\\) and \\(p=|P|\\), where \\(p\\ge m\\)\n\\[\n\\begin{aligned}\n    &L_{D_i}(A(S_j^i))\\\\\n    =&\\frac{1}{2m}\\sum_k^{2m}\\mathbf{1}[f_i(x_k)\\neq A(S_j^i)(x_k)]\\\\\n    \\ge&\\frac{1}{2p}\\sum_{x\\in P}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n\\end{aligned}\n\\]\nthen\n\\[\n\\begin{aligned}\n    &\\frac{1}{T}\\sum_{i}^{T}L_{D_i}(A(S_j^i))\\\\\n    \\ge&\\frac{1}{T}\\sum_i^T\\frac{1}{2p}\\sum_{x\\in P}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n    =&\\frac{1}{2}*\\frac{1}{p}\\sum_{x\\in P}\\frac{1}{T}\\sum_{i}^{T}\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\\\\n\\end{aligned}\n\\]\nthere are details here, note we have removed out the instances that are in \\(S_j\\) (no \\(i\\) here), that is because \\(\\mathbf{1}[f_i(x)\\neq A(S_j^i)(x)]\\) will always be zero if \\(x\\in S_j\\) and we can certainly not consider them\nsecond, since we take \\(f\\) from func space that contains all possible funcs, so they can be separated into pairs that can cancellate each other\nthere must always exist a pair of f‚Äôs (\\(f_a\\) and \\(f_b\\)) that only differ on one instance \\(x\\) which is not in \\(S_j\\) (\\(x\\in P\\))\ns.t.\n\\(\\mathbf{1}[f_a(x)\\neq A(S_j^a)(x)]+\\mathbf{1}[f_b(x)\\neq A(S_j^b)(x)]=1\\)\nthen the ave above actually is \\(\\frac{1}{4}\\)\nnow we have proved that\n\\[\n\\max_{D,f}\\mathbb{E}_{S\\sim D^m}[L_{D,f}(A(S))]\\ge 1/4\n\\]\nuse some prob inequality can give us the conclusion that \\(\\exists D\\) \\[\n\\mathbb{P}_{S\\sim D^m}[L_D(A(S))\\ge1/8]\\ge1/7\n\\]\n\nusing the def, \\(H\\) that contains every possible h is not PAC learnable"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-04.html#bias-complexity-trade-off",
    "href": "posts/understanding-machine-learning/UML-04.html#bias-complexity-trade-off",
    "title": "Understanding Machine Learning 04",
    "section": "bias-complexity trade-off",
    "text": "bias-complexity trade-off\nUsually, we see a bias-variance trade-off. I think there may be some connection, but I haven‚Äôt seen it\nthe idea is we divide the true risk as below\n\\[\n\\mathcal{L}_D(h)=\\epsilon_{app}+\\epsilon_{est}\n\\]\nwhere \\(\\epsilon_{app}=\\min_{h'\\in H}L_D(h')\\) and \\(\\epsilon_{est}=\\mathcal{L}_D(h)-\\epsilon_{app}\\)\n\n\\(\\epsilon_{app}\\), approximation error, it measures how well your hypothesis space is\n\\(\\epsilon_{est}\\), estimation error, it measures how well your learner can estimate the best h (\\(\\in H\\)) using ERM\n\napprox error has nothing to do with how you train with the dataset. if your hypothesis space is well enough (or large enough), it will be small\nest error is on the opposite. It really depends on the learner, sample size so on. It‚Äôs similar to the \\(\\epsilon\\) in the definition of agnostic PAC learnability. If your h space is too large (too complicated), then it will need more samples to decrease the est error.\nso, large \\(|H|\\) will reduce approx error, but it may be hard to have a small est error (overfitting), with a high probability a tr set will cause a bad generalization ability\non the contrary, small \\(|H|\\) indeed will give us a small est error but will cause a large approximate error (underfitting)"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html",
    "href": "posts/understanding-machine-learning/UML-01.html",
    "title": "Understanding Machine Learning 01",
    "section": "",
    "text": "domain set. a set of instances \\(\\mathcal{X}\\)\nlabel set. \\(\\mathcal{Y}\\)\ntraining data. finite sequence \\(\\mathcal{S}\\) in \\(\\mathcal{X}\\times \\mathcal{Y}\\)\nlearner. output a rule, \\(h:\\mathcal{X}\\rightarrow\\mathcal{Y}\\). learning algorithm \\(\\mathcal{A(S)}\\)\ndata-generation model. ideally, we assume (for sim) there exists a \"correct\" labeling funciton, \\(f\\). and \\(\\mathcal{S}\\) is generated under an unknown distrib \\(\\mathcal{D}\\), then labeling it using \\(f\\).\nmeasurement. actually generalization error (risk or true error of \\(f\\))\n\n\\[\nL_{\\mathcal{D},f}(h):=\\mathbb{P}_{x\\sim D}[h(x)\\neq f(x)]:=\\mathcal{D}(\\{x:h(x)\\neq f(x)\\})\n\\]\nwhere \\(\\mathcal{D}\\) desc the prob of the event of observation.\n\\(\\mathcal{D}(A):=\\mathbb{P}_{x\\sim D}[\\pi(x)]\\), where \\(\\pi\\) is the char func of whether it was observed. \\(A={x\\in \\mathcal{X}:\\pi(x)=1}\\)"
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#erm-empirical-risk-minimization",
    "href": "posts/understanding-machine-learning/UML-01.html#erm-empirical-risk-minimization",
    "title": "Understanding Machine Learning 01",
    "section": "ERM (empirical risk minimization)",
    "text": "ERM (empirical risk minimization)\nuse training loss to approx generalization loss.\n\noverfitting\nwe may obtain a bunch of funcs just by ERM. so we need an inductive bias to set a preference on a certain funcs.\nwe choose the hypothesis space H before seeing the data. restric our search space of the ERM, otherwise we got a trivial useless solution.\nbefore seeing the data \\(\\rightarrow\\) should be based on some prior knowledge."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#h",
    "href": "posts/understanding-machine-learning/UML-01.html#h",
    "title": "Understanding Machine Learning 01",
    "section": "H",
    "text": "H\nexamples of H\n\nfinite H space\n\\(\\mathcal{H}\\) will not overfit provided a sufficiently large training set.\nnote: the class of axix-aligned rectangles could be finite if we consider it on a computer. (discrete repr of real numbers)\n\\(h_S\\in argmin_{h\\in H}L_s(h)\\)\nsince S are randomly chosen, so \\(h_S,L_{D,f}\\) are actually random vars.\n\n\na few assumptions\na few assumptions on the PAC learnability\n\ndef the realizability assumption\nthere exists \\(h^\\star\\in\\mathcal{H}\\) s.t. \\(L_{D,f}(h^\\star)=0\\)\nfurther, we have\n\\(\\rightarrow L_S(h^\\star)=0 \\text{ with prob 1 over the S }\\rightarrow L_S(h_S)=0\\)\nwe are interested in \\(L_{D,f}(h_S)\\)\n\n\nconfidence param\nwe address a prob \\(\\delta\\) of getting a very nonrepresentative training set (e.g all lie in class A). and \\((1-\\delta)\\) is the confidence of our prediction.\n\n\naccuracy param\n\\(\\epsilon\\)\nwe call \\(L_{D,f}(h_S)\\ge \\epsilon\\) as a failure of the learner, otherwise approx correct predictor.\nSo we're interested in the upper bound of prob to sample S that leads to the learner‚Äôs failure.\nupper bound of\n\\[\nD^m\\{S|_x:L_{D,f}(h_S)\\gt \\epsilon\\}\n\\]\nlet \\(H_B\\) be the set of bad hypotheses\n\\[\n\\{h\\in H: L_{D,f}\\gt \\epsilon\\}\n\\]\nlet M be the set of the misleading training set\n\\[\n\\{S|_x:\\exists h\\in H_B, L_S(h)=0\\}\n\\]\nwhere \\(S|_x\\) is the instances of tr set\ndue to real.. assumption, only M will cause failure.\nso only a subset of S from M will cause \\(h_S\\) to fail.\n\\[\n\\begin{align}\n&D^m\\{S|_x:L_{D,f}(h_S)\\gt \\epsilon\\} \\\\\n\\le& \\sum_{h\\in H_B}D^m\\{S|_x:L_S(h)=0\\}\\\\\n=&\\sum_{h\\in H_B}\\prod D\\{x_i:h(x_i)=f(x_i)\\}\n\\end{align}\n\\]\nhere, the countability of \\(\\mathcal{H}\\) is used, and I think if we can control the order of \\(|\\mathcal{H}|\\) and with more careful scaling (with more assumption or knowledge about the h's, like \\(D^m\\{S|_x:L_S(h)=0\\}\\) can be approx related to h) then we could have the inf conclusion, though maybe not that interesting, and there are other ways on it.\nand\n\\[\nD\\{x_i:h(x_i)=f(x_i)\\}=1-L_{D,f}(h)\\le 1-\\epsilon\n\\]\nso using a series of loose relaxation, we have\n\\[\nD^m\\{S|_x:L_{D,f}(h_S)\\}\\le |H_B|e^{-\\epsilon m}\\le |H|e^{-\\epsilon m}\n\\]\nfinite is used here\nLHS is \\(\\delta\\)\nNote sometimes m should be really large to ensure with at least \\(1-\\delta\\) confidence over the choice of S, every ERM hypothesis, \\(h_S\\) is approx correct."
  },
  {
    "objectID": "posts/understanding-machine-learning/UML-01.html#small-corollary",
    "href": "posts/understanding-machine-learning/UML-01.html#small-corollary",
    "title": "Understanding Machine Learning 01",
    "section": "small corollary",
    "text": "small corollary\nwhen hypothesis space is finite, then we can immediately have an upper bound for \\(m_{\\mathcal{H}}\\)\n\\[\nm_{\\mathcal{H}}(\\epsilon,\\delta)\\le \\left \\lceil \\frac{log\\left(|\\mathcal{H}|/\\delta\\right)}{\\epsilon} \\right \\rceil\n\\]"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html",
    "href": "posts/notes-about-technical-writing.html",
    "title": "Notes about Technical Writing",
    "section": "",
    "text": "extra spaces between words and extra blank lines between passages will be ignored.\n\nspaces in the front will be ignored.\nnewline is regarded as an extra space, adding % at the end will remove it.\nnote: space before macros will not be ignored, e.g., \\TeX ing. But spaces at the rear will be ignored. Adding brankets {} will resolve it, e.g., {\\TeX} ing or \\Tex{} ing.\n\n\\qquad: as wide as about two M s\ndouble hyphen: -- denotes number range, en dash; triple hyphen: --- denotes punctuation dash, em dash. en and em denotes the width\nfloat environment: e.g., figure, table. accept an optional arg (h: here; t: top; H: here and not float, extended from package float)\n\\eqref is specified for math equations from package amsmath\navoid setting font or controlling indents, etc., within the document environment. try to replace them with meaningful commands or environments.\n\n\\newenvironment{myenv}\n\\newcommand\n\nellipsis: \\ldots or \\dots. instead of \\cdots or ...(three dots).\n\nH\\dots. (OK)\nH \\ldots H (not good), H $\\ldots$ H (recommended, math env)\n\nescape chars in the main body.\n\n\\_ for _; \\textbackslash for \\\n\nnon-breakable space.\n\nQuestion~1\nDonald~E. Knuth within names\nMr.~Knuth\nfunction~$f(x)$\n1,~2, and~3\n\nperiod after capital is regarded as a abbreviation. use \\ or \\@ to resolve it. E.g., Roman number XII\\@. Yes.\nBibTeX\n\n\\citep (index) and \\citet (author) are recommended. remember to use package natbib and use plainnat bibliography style\n\nforce line break\n\n\\\\ accepts an optional argument for vertical space. e.g., \\\\[2cm]. Often used for in equation environment\n\nspecial chars used in main body\n\n¬ß: \\S\n: \\dag\n: \\ddag\n¬∂: \\P\n¬©: \\copyright\n¬£: \\pounds\n\\(\\bullet\\): \\textbullet (\\bullet in math env)\ngo check the book The Comprehensive LATEX Symbol List, 2009 by Scott Pakin for more"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-family",
    "href": "posts/notes-about-technical-writing.html#font-family",
    "title": "Notes about Technical Writing",
    "section": "font family",
    "text": "font family\n\nroman family: \\textrm{font family} font family\nsans-serif family: \\textsf{font family} font family\ntypewritter family: \\texttt{font family} font family\n\ndeclaration, {\\rmfamily font family} font family"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-shape",
    "href": "posts/notes-about-technical-writing.html#font-shape",
    "title": "Notes about Technical Writing",
    "section": "font shape",
    "text": "font shape\n\nupright shape: \\textup{Font Shape} Font Shape\nitalic shape: \\textit{Font Shape} Font Shape\nslanted shape: \\textsl{Font Shape} Font Shape\nsmall captical shape: \\textsc{Font Shape} Font Shape\n\ndeclaration: {\\itshape Font Shape} Font Shape"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-series",
    "href": "posts/notes-about-technical-writing.html#font-series",
    "title": "Notes about Technical Writing",
    "section": "font series",
    "text": "font series\n\nmedium series: \\textmd{font series} font series (default in main body)\nbold series: \\textbf{font series} font series\n\ndeclaration: {\\mdseries font series} font series"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-typeface",
    "href": "posts/notes-about-technical-writing.html#font-typeface",
    "title": "Notes about Technical Writing",
    "section": "font typeface",
    "text": "font typeface\n\nThe default font family of \\(\\LaTeX\\) is Computer Modern\nSerif Times Roman (i.e., Times New Roman) is recommended for papers, magazines and books. Use package txfonts\nConcrete is recommended for presentation. package combination ccfonts, eulervm is great. (also arec ,cmbright)\n\nwe can specify three font family individually\n\\usepackage{fontspec}\n\\setmainfont{Times New Roman}\n\\setsansfont{Verdana}\n\\setmonofont{Courier New}"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#emphasis",
    "href": "posts/notes-about-technical-writing.html#emphasis",
    "title": "Notes about Technical Writing",
    "section": "emphasis",
    "text": "emphasis\nmake upright or make italic upright\n\nthis is \\emph{emphasis}: this is emphasis\n\\textit{this is \\emph{emphasis}} this is emphasis**\n\nunderline\n\\underline{Emphasized} text and \\underline{another}: Emphasized text and another\nuse package \\usepackage[normalem]{ulem}\n\\uline{Emphasized} text and \\uline{another}: Emphasized text and another"
  },
  {
    "objectID": "posts/notes-about-technical-writing.html#font-size",
    "href": "posts/notes-about-technical-writing.html#font-size",
    "title": "Notes about Technical Writing",
    "section": "font size",
    "text": "font size"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html",
    "href": "posts/paper-reading/multi-view-clustering.html",
    "title": "some papers about multi-view-clustering",
    "section": "",
    "text": "very brief summaries of some papers I read"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#binary-multi-view-clustering",
    "href": "posts/paper-reading/multi-view-clustering.html#binary-multi-view-clustering",
    "title": "some papers about multi-view-clustering",
    "section": "Binary Multi-View Clustering",
    "text": "Binary Multi-View Clustering\nlarge scale multi-view image clustering\njointly learn collaborative discrete representation and binary cluster structures\nhas an algorithm with proved convergence analysis"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#contrastive-clustering",
    "href": "posts/paper-reading/multi-view-clustering.html#contrastive-clustering",
    "title": "some papers about multi-view-clustering",
    "section": "Contrastive Clustering",
    "text": "Contrastive Clustering\nunified instance- and cluster-level contrastive learning\nrow vectors and column vectors as instance representation and cluster representation\nQ: how to construct negative instance, how to design the training target"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#deep-clustering-on-the-link-between-discriminative-models-and-k-means",
    "href": "posts/paper-reading/multi-view-clustering.html#deep-clustering-on-the-link-between-discriminative-models-and-k-means",
    "title": "some papers about multi-view-clustering",
    "section": "Deep Clustering: On the Link between Discriminative Models and K-Means",
    "text": "Deep Clustering: On the Link between Discriminative Models and K-Means\ndiscover the equivilance between discriminative models using L2 regularized MI loss and soft regularized K-means loss, under some conditions"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#multiview-clustering-a-scalable-and-parameter-free-bipartite-graph-fusion-method",
    "href": "posts/paper-reading/multi-view-clustering.html#multiview-clustering-a-scalable-and-parameter-free-bipartite-graph-fusion-method",
    "title": "some papers about multi-view-clustering",
    "section": "Multiview Clustering: A Scalable and Parameter-Free Bipartite Graph Fusion Method ",
    "text": "Multiview Clustering: A Scalable and Parameter-Free Bipartite Graph Fusion Method \nparameter-free, graph based multi-view clustering (graph fusion frameswork ?)"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#dual-contrastive-prediction-for-incomplete-multi-view-representation-learning",
    "href": "posts/paper-reading/multi-view-clustering.html#dual-contrastive-prediction-for-incomplete-multi-view-representation-learning",
    "title": "some papers about multi-view-clustering",
    "section": "Dual Contrastive Prediction for Incomplete Multi-view Representation Learning ",
    "text": "Dual Contrastive Prediction for Incomplete Multi-view Representation Learning \nin-complete MvRL\nunify consistency learning and missing data recovery: proved using information theory\nframework based on contrastive learning loss"
  },
  {
    "objectID": "posts/paper-reading/multi-view-clustering.html#robust-multi-view-clustering-with-incomplete-information",
    "href": "posts/paper-reading/multi-view-clustering.html#robust-multi-view-clustering-with-incomplete-information",
    "title": "some papers about multi-view-clustering",
    "section": "Robust Multi-view Clustering with Incomplete Information",
    "text": "Robust Multi-view Clustering with Incomplete Information\nunified framework to solve PVP and PSP\ncontrastive loss to eliminate the false negative samples"
  },
  {
    "objectID": "posts/paper-reading/moco.html",
    "href": "posts/paper-reading/moco.html",
    "title": "MoCo",
    "section": "",
    "text": "Âä®ÈáèÂØπÊØîÂ≠¶‰π†(Momentum Contrast, MoCo)Áî®‰∫éÂõæË±°Ë°®ÂæÅÁöÑÊó†ÁõëÁù£Â≠¶‰π†„ÄÇ‰∏ªË¶ÅÁöÑËßÇÁÇπÊòØÂ∞ÜÂØπÊØîÂ≠¶‰π†Áúã‰ΩúÊòØ‰∏ÄÁßçÂ≠óÂÖ∏Êü•ËØ¢ÔºåÂç≥ÁªôÂÆö‰∏Ä‰∏™ÂõæÁâáÁöÑÁºñÁ†Å‰Ωú‰∏∫ËØ∑Ê±ÇÔºåÂú®Â≠óÂÖ∏‰∏≠ÊâæÂà∞‰∏é‰πãÂØπÂ∫îÁöÑÂõæÁâáÁºñÁ†Å„ÄÇÂπ∂‰æùÊ≠§ÊèêÂá∫‰∫ÜÂú®ËßÜËßâÈ¢ÜÂüü‰∏≠Ôºå‰ΩøÁî®ËøôÁßçÊñπÊ≥ïÈúÄË¶ÅÈù¢‰∏¥ÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÁÇπÔºö‰∏ÄÊòØÂ≠óÂÖ∏Â∫îÂ∞ΩÂèØËÉΩÁöÑÂ§ßÔºõ‰∫åÊòØÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Áî®‰∫éÁºñÁ†ÅÂ≠óÂÖ∏ÁöÑÁºñÁ†ÅÂô®Â∫îÂ∞ΩÂèØËÉΩÂú∞‰øùÊåÅ‰∏ÄËá¥Ôºå‰πüÂ∞±ÊòØË¶Å‰øùËØÅÊâÄÊúâÁöÑÂõæÁâáÈÉΩÂ§ßËá¥Êò†Â∞ÑÂà∞‰∫ÜÂêå‰∏Ä‰∏™Ë°®ÂæÅÁ©∫Èó¥‰∏≠„ÄÇ‰∏∫Ëß£ÂÜ≥Á¨¨‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰ΩøÁî®ÈòüÂàóÂä®ÊÄÅÁª¥Êä§Â≠óÂÖ∏ÔºõÈíàÂØπÁ¨¨‰∫å‰∏™ÈóÆÈ¢òÔºå‰ΩøÁî®Âä®ÈáèÊõ¥Êñ∞ÁöÑÊñπÊ≥ïÂéª‰øÆÊîπÁºñÁ†ÅÂô®ÁöÑÂèÇÊï∞„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#background",
    "href": "posts/paper-reading/moco.html#background",
    "title": "MoCo",
    "section": "Background",
    "text": "Background\nÊó†ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ(NLP)È¢ÜÂüüÂèñÂæó‰∫ÜÂæàÂ§ßÁöÑÊàêÂäüÔºåËøôÂæóÁõä‰∫éÂú® NLP ‰∏≠ÔºåÊ®°ÂûãÈù¢ÂØπÁöÑËæìÂÖ•‰ø°Âè∑ÊòØÁ¶ªÊï£ÁöÑÔºåÂêåÊó∂Êòì‰∫éÂàÜÂâ≤ÊàêÂ∞èÁöÑÂçïÂÖÉÊù•Âª∫ÈÄ†Â≠óÂÖ∏ÊàñËÄÖÂØπÂ∫îÁöÑË°®ÂæÅ„ÄÇËÄåÂú®ËßÜËßâÈ¢ÜÂüüÔºåËæìÂÖ•ÁöÑ‰ø°Âè∑ÊòØËøûÁª≠ÁöÑ„ÄÅÈ´òÁª¥Â∫¶ÁöÑ„ÄÅÈùûÁªìÊûÑÂåñÁöÑÔºåÊÉ≥Ë¶ÅÂæóÂà∞ÂêåÊ†∑ÊïàÊûúÁöÑÁ®ÄÁñèÁöÑË°®ÂæÅÊòØÂæàÂõ∞ÈöæÁöÑ„ÄÇ\nÊó†ÁõëÁù£Â≠¶‰π†Âú®Â§ßÂûãÊó†Ê†áËÆ∞Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰ΩøÁî®ÊòØÈùûÂ∏∏ÂÄºÂæóÂÖ≥Ê≥®ÁöÑÔºå‰ΩøÁî®Êó†ÁõëÁù£Â≠¶‰π†Êù•‰∏éËÆ≠ÁªÉÂæóÂà∞ÁöÑË°®ÂæÅ‰πüËÉΩÁî®‰∫é‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂ≠¶‰π†„ÄÇ‰ΩÜ‰ªñÁöÑÊïàÊûúÂç¥Âπ∂‰∏çÂ¶ÇÁõÆÂâçÁöÑÁõëÁù£ÂºèÂ≠¶‰π†ÊïàÊûúÂ•Ω„ÄÇËã•ËÉΩÂæàÂ•ΩÁöÑËß£ÂÜ≥Ëøô‰∏ÄË°®ÂæÅÂ≠¶‰π†ÁöÑÈóÆÈ¢òÔºåÂàôËÉΩÂ§ßÂ§ßÁº©Â∞èÁõëÁù£‰∏éÊó†ÁõëÁù£Â≠¶‰π†‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#retated-work",
    "href": "posts/paper-reading/moco.html#retated-work",
    "title": "MoCo",
    "section": "Retated Work",
    "text": "Retated Work\n‰∏∫‰∫ÜËß£ÂÜ≥‰ª•‰∏äÊèêÂá∫ÁöÑÈóÆÈ¢òÔºåÂæàÂ§öÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏çÂêåÁöÑÂü∫‰∫éÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÊ≥ï„ÄÇ\n\nloss function\nÊúÄÊó©ÊúüÁöÑÊñπÊ≥ï‰∏≠ÔºåÊàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂØªÊâæ‰∏ÄÁßçÁºñÁ†ÅÂô®Ë¥üË¥£ÈôçÁª¥‰ª•ÂèäÁâπÂæÅÁöÑÂ≠¶‰π†Ôºå‰ª•ÊàêÂØπÁöÑÊï∞ÊçÆ‰Ωú‰∏∫ËæìÂÖ•Ôºå‰ª•‰ªñ‰ª¨ÁöÑË∑ùÁ¶ªÊù•Ë°®ËææÁõ∏‰ººÂ∫¶„ÄÇ‰∏∫Ê≠§ÔºåÊçüÂ§±ÂáΩÊï∞ÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜÔºåÂâçËÄÖÁî®Êù•Áº©Â∞èÁõ∏‰ººÊï∞ÊçÆÁöÑË∑ùÁ¶ªÔºåÂêéËÄÖÁî®Êù•Êâ©Â§ß‰∏çÁõ∏‰ººÊï∞ÊçÆÁöÑË∑ùÁ¶ª„ÄÇ\n‰πãÂêéËΩ¨ÂåñÊàê‰ΩøÁî®‰∫í‰ø°ÊÅØÂª∫Ê®°ÔºåÊèêÂá∫ InfoNCE ÊçüÂ§±ÂáΩÊï∞„ÄÇÈááÊ†∑Ê≠£Ë¥üÊ†∑Êú¨ËøõË°åËÆ≠ÁªÉ„ÄÇÊúÄÁªàÊôÆÈÅç‰ΩøÁî®ÁöÑÊòØÈùûÂèÇÊï∞ÂåñÁöÑsoftmaxÂáΩÊï∞„ÄÇ\n\\[P(i|v)=\\frac{\\exp(v_i^Tv/\\tau)}{\\sum_{j=1}^{n}{\\exp(v_j^Tv/\\tau)}}\\]\n\n\ncontrastive loss mechanisms\n\nend-to-end\nÁ´ØÂà∞Á´ØÁöÑÊñπÊ≥ï‰∏≠ÔºåÂΩìÂâçÁöÑbatchË¢´‰Ωú‰∏∫Â≠óÂÖ∏ËøõË°åËÆ≠ÁªÉÔºåqueryÂíåkeyÂàÜÂà´ÈááÁî®‰∏çÂêåÁöÑÁºñÁ†ÅÂô®ÔºåÂπ∂ÂàÜÂà´ËøõË°åÊ¢ØÂ∫¶‰∏ãÈôç„ÄÇÁî±‰∫éÁÆóÂäõÁöÑÈôêÂà∂ÔºåÊõ¥Â§ßÁöÑÂ≠óÂÖ∏Èöæ‰ª•ËøõË°åÊ¢ØÂ∫¶ÁöÑËÆ°ÁÆóÔºåÂêåÊó∂‰ªñ‰πüÂèóÈôê‰∫ébatchÁöÑÂ§ßÂ∞è„ÄÇ‰ªñÁöÑ‰∏ÄËá¥ÊÄß‰øùÊåÅÂæóÂæàÂ•ΩÔºåÂõ†‰∏∫‰∏Ä‰∏™batchÂ≠óÂÖ∏ÁöÑÁºñÁ†ÅÂô®ÊÄªÊòØÂêå‰∏Ä‰∏™Ôºå‰ΩÜÁº∫ÁÇπÊòØÂ≠óÂÖ∏Êï∞ÈáèÈöæ‰ª•‰øùËØÅ„ÄÇ\n\n\nmemory bank\nÂè¶‰∏ÄÁßçÊú∫Âà∂ÊòØÊèêÂâçÂ∞ÜÊâÄÊúâÊ†∑Êú¨ÁöÑÁºñÁ†ÅËÆ°ÁÆóÂá∫Êù•ÔºåÊØèÊ¨°queryÊó∂Ôºå‰ºöÂú®ÊâÄÊúâÊ†∑Êú¨‰∏≠ÂÜçËøõË°åÈááÊ†∑‰Ωú‰∏∫Â≠óÂÖ∏„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨‰∏çÂÜçÂØπkeyÁöÑÁºñÁ†ÅÂô®ËøõË°åÊ¢ØÂ∫¶‰∏ãÈôçÔºåËÄåÊòØÁõ¥Êé•‰ΩøÁî®queryÁöÑÂèÇÊï∞„ÄÇÊ≥®ÊÑèÔºåËøôÈáåÁöÑÊõ¥Êñ∞ÊòØ‰∏çÂèäÊó∂ÁöÑÔºåÊØèÊ¨°Âè™ÊúâÂΩìÂâçË¢´ÈááÊ†∑ÁöÑÊ†∑Êú¨ÁºñÁ†ÅË¢´Êõ¥Êñ∞‰∫Ü„ÄÇËøôÊ†∑Â≠óÂÖ∏Â§ßÂ∞èÁöÑÈóÆÈ¢òËß£ÂÜ≥‰∫ÜÔºå‰ΩÜ‰∏ÄËá¥ÊÄßÁöÑÈóÆÈ¢òÂç¥ÂèòÂæóÊ£òÊâãÔºåÂõ†‰∏∫ÊØèÊ¨°ÈááÊ†∑ÁöÑÁºñÁ†ÅÂèØËÉΩÊòØÁî±‰∏çÂêåÁöÑÁºñÁ†ÅÂô®(ÂèÇÊï∞‰∏çÂêå)ÂæóÂà∞„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#novelty",
    "href": "posts/paper-reading/moco.html#novelty",
    "title": "MoCo",
    "section": "novelty",
    "text": "novelty\nMoCo ÊèêÂá∫‰ΩøÁî®ÈòüÂàóÊù•Âä®ÊÄÅÁª¥Êä§Â≠óÂÖ∏ÔºåÊàêÂäüÂÅöÂà∞‰∫ÜÂ≠óÂÖ∏Â§ßÂ∞èÂÆåÂÖ®‰∏ç‰æùËµñbatchÁöÑÂ§ßÂ∞è„ÄÇÊñ∞ÁöÑbatchË¢´Âä†ÂÖ•Âà∞ÈòüÂàó‰∏≠ÔºåÊúÄÊóßÁöÑbatch‰ºöË¢´ÁßªÈô§„ÄÇËøôÊ†∑ÔºåÈòüÂàó‰∏≠ÁöÑkeys‰ΩøÁî®ÁöÑÁºñÁ†ÅÂô®ÊòØËøûÁª≠„ÄÅÂπ≥ÊªëÁöÑÊõ¥Êñ∞ÁöÑ„ÄÇ\nÁ¨¨‰∫åÁÇπÊòØkeyÁºñÁ†ÅÂô®ÂèÇÊï∞ÁöÑÊõ¥Êñ∞‰∏äÔºåÂπ∂‰∏çÊòØÁõ¥Êé•Â§çÂà∂queryÁöÑÂèÇÊï∞ÔºåËÄåÊòØÂä®ÈáèÊõ¥Êñ∞ÔºåÊõ¥Âä†ÁöÑÂπ≥ÊªëÔºå‰øùËØÅ‰∫Ü‰∏ÄËá¥ÊÄß„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#dict-look-up",
    "href": "posts/paper-reading/moco.html#dict-look-up",
    "title": "MoCo",
    "section": "dict look-up",
    "text": "dict look-up\n‰∏é‰∏äÊñáÊèêÂà∞ÁöÑ memory-bank ÊñπÊ≥ïÁ±ª‰ººÔºåÊ†πÊçÆ‰∏Ä‰∏™Ê†∑Êú¨ÔºåÈÄöËøáÈöèÊú∫ÂèòÊç¢ÂæóÂà∞Êñ∞Ê†∑Êú¨‰Ωú‰∏∫Ê≠£‰æãÔºå‰πãÂêéÂèñ K ‰∏™Ë¥ü‰æãÊ±ÇÂá∫ InfoNCE loss ËøõË°å‰ºòÂåñ„ÄÇ‰∏çÂêåÁöÑÊòØË¥ü‰æãÁöÑÈÄâÊã©ÊñπÂºèÂíåÁºñÁ†ÅÂô®ÁöÑÊõ¥Êñ∞ÊñπÂºè„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#momentum-contrast",
    "href": "posts/paper-reading/moco.html#momentum-contrast",
    "title": "MoCo",
    "section": "Momentum Contrast",
    "text": "Momentum Contrast\n\nqueue\nÊØèÊ¨°ÂèñÂæó‰∏Ä‰∏™Êñ∞ÁöÑbatchÊï∞ÊçÆËøõË°åÈöèÊú∫Â¢ûÂº∫‰Ωú‰∏∫ËØ∑Ê±Ç qÔºåÂÜçÂÅöÂè¶‰∏ÄÁßçÈöèÊú∫Â¢ûÂº∫‰Ωú‰∏∫ÊØè‰∏™ÂçïÁã¨ËØ∑Ê±ÇÁöÑÊ≠£‰æã k„ÄÇÂ∞ÜÈòüÂàó‰∏≠ÁöÑÂÖ®ÈÉ®ÂÖÉÁ¥†‰Ωú‰∏∫ÊØè‰∏™ËØ∑Ê±ÇÁöÑË¥ü‰æãÔºåÊ±ÇÂæóÊçüÂ§±ÔºåÂà©Áî®Ê¢ØÂ∫¶‰∏ãÈôçÊõ¥Êñ∞ÂèÇÊï∞„ÄÇÈöèÂêéÂ∞Ü k Âä†ÂÖ•Âà∞ÈòüÂàó‰∏≠ÔºåÂêåÊó∂ÂéªÊéâÊóßÁöÑbatch„ÄÇ Áî±‰∫éÈòüÂàóÁöÑËßÑÊ®°‰ºöÂæàÂ§ßÔºåÈòüÂàó‰∏≠ÁöÑÁºñÁ†ÅÂä†ÂÖ•ÈòüÂàóÂêéÂπ∂‰∏ç‰ºöÂÜçÊõ¥Êñ∞Ôºå‰ΩÜËøôÂπ∂‰∏ç‰ºöÂ¶®Á¢ç‰∏ÄËá¥ÊÄßÁöÑ‰øùËØÅÔºåÂõ†‰∏∫ÂÆÉÊÄª‰ºöÊääÊúÄ‰∏ç‰∏ÄËá¥ÁöÑÁºñÁ†ÅÂéªÈô§Êéâ„ÄÇ\n\n\nMomentum update\nÂú®Êõ¥Êñ∞ÂèÇÊï∞Êó∂ÔºåÂè™ÂØπqueryÁöÑÁºñÁ†ÅÂô®ÂèÇÊï∞ËøõË°åÊ¢ØÂ∫¶‰∏ãÈôçÔºåËÄåÂØπkeyÁºñÁ†ÅÂô®ËøõË°åÂ¶Ç‰∏ãÊõ¥Êñ∞: \\[\\theta_k \\leftarrow m\\theta_k+(1-m)\\theta_q\\]\nËÄåÂú®ÂÆûÈ™å‰∏≠ÔºåÁ®çÂ§ßÁöÑ m ÂæóÂà∞ÁöÑÁªìÊûú‰ºöÊõ¥Â•ΩÔºåËøô‰πüÂç∞ËØÅ‰∫Ü‰∏ÄËá¥ÊÄßÁöÑÈáçË¶Å„ÄÇ\n\n\nPretext Task\nÂâçÁΩÆ‰ªªÂä°‰∏é memory-bank Á±ª‰ºº„ÄÇËøôÈáå‰ΩúËÄÖÈááÁî®‰∫Ü ResNet ‰Ωú‰∏∫ÁºñÁ†ÅÂô®ÔºåËÄå‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÂèëÁé∞ BN ÂØπÁºñÁ†ÅÁöÑÂ≠¶‰π†Ëµ∑Âà∞‰∫ÜÊ∂àÊûÅÁöÑ‰ΩúÁî®ÔºåËÄåËøôÂèØËÉΩÊòØÁî±‰∫é BN ËûçÂêà‰∫ÜÂΩìÂâçbatchÁöÑ‰ø°ÊÅØÔºåÂú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏ä‰ºö‰ΩøÊ®°Âûã‚ÄùËµ∞Êç∑ÂæÑ‚Äù„ÄÇ\nÂØπ BN ÁöÑÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫Ü shuffling BN ÁöÑÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú® BN ÂâçÂÖàÂ∞Ü sample ËøõË°åÊâì‰π±ÔºåÊ±ÇÂæóÁºñÁ†ÅÂêéÂÜçÂ§çÂéü„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#training",
    "href": "posts/paper-reading/moco.html#training",
    "title": "MoCo",
    "section": "training",
    "text": "training\nÊï∞ÊçÆÈõÜÈááÁî®ÁöÑÊòØ\n\nImageNet: ÂåÖÂê´Á∫¶ 1 Áôæ‰∏áÂº†ÂõæÁâáÔºå1000 ‰∏™Á±ªÂà´„ÄÇÂõæÁâáÁöÑÁ±ªÂà´ÂàÜÂ∏ÉÂùáÂåÄ\nInstagram: Áî± Instagram ‰∏≠ÂæóÂà∞ÁöÑÁ∫¶ 10 ‰∫øÂº†ÂõæÁâá„ÄÇÂÖ∑ÊúâÈïøÂ∞æÂàÜÂ∏É„ÄÇ\n\nËøôÈáå‰ΩúËÄÖÊòØÂ∞ÜÁâπÂæÅÂ±ÇÂÜªÁªìÔºåÂæÆË∞ÉÁ∫øÊÄßÂàÜÁ±ªÂ±ÇÊù•È™åËØÅÊ®°ÂûãÁöÑÊïàÊûú„ÄÇ‰∏é‰ºóÂ§öÊó†ÁõëÁù£Ê®°ÂûãËøõË°åÊØîÂØπ„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#findings",
    "href": "posts/paper-reading/moco.html#findings",
    "title": "MoCo",
    "section": "findings",
    "text": "findings\n\nÂú®ÂæÆË∞ÉMoCoÊó∂Ôºå‰∏Ä‰∫õË∂ÖÂèÇÊï∞‰∏éÁõëÁù£ÂºèÂ≠¶‰π†Ê®°ÂûãÁõ∏Â∑ÆÂæàÂ§ßÔºåËøôË°®Êòé‰ªñ‰ª¨ÁöÑÁâπÂæÅÂàÜÂ∏ÉÊúâËæÉÂ§ßÁöÑÂ∑ÆÂºÇ\nÈÄöËøá‰∏é end-to-end Âíå memory-bank ‰∏§ÁßçÊñπÊ≥ïÁöÑÂØπÊØîÂèØ‰ª•ÂèëÁé∞ÔºåÂΩìÂ≠óÂÖ∏ËßÑÊ®°ËæÉÂ∞èÊó∂Ôºåend-to-end Ê®°ÂûãÂíå MoCo ‰∏çÁõ∏‰∏ä‰∏ãÔºåmemory-bank ÂàôÁ®çÈÄäÈ£éÈ™öÔºõ‰ΩÜÂ¢ûÂ§ßÂ≠óÂÖ∏ËßÑÊ®°ÂêéÔºåend-to-end Ê®°ÂûãÂàôÂèóÈôê‰∫éÁÆóÂäõÔºåÊó†Ê≥ïËÆ°ÁÆóÔºåÂÖ∂‰ªñ‰∏§‰∏™Ê®°ÂûãÈÉΩÂèØ‰ª•ÊåÅÁª≠Â¢ûÂä†Ôºå‰ΩÜ MoCo ÂàôÊòéÊòæËÉúÂá∫ËÆ∏Â§ö„ÄÇËØ¥Êòé MoCo ÂæàÂ§ßÁöÑÊèêÂçá‰∫ÜÊó†ÁõëÁù£Â≠¶‰π†ÁöÑÊïàÊûú„ÄÇ\nËã•ÂÆåÂÖ®Â§çÂà∂queryÁºñÁ†ÅÂô®ÂèÇÊï∞ÔºåÂàôÊó†Ê≥ïÊî∂Êïõ\nMoCo ÂèØ‰ª•ÂæàÂ•ΩÁöÑ‰Ωú‰∏∫pre-trainedÊ®°ÂûãËøõË°å‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂ≠¶‰π†„ÄÇ"
  },
  {
    "objectID": "posts/paper-reading/moco.html#conclusion",
    "href": "posts/paper-reading/moco.html#conclusion",
    "title": "MoCo",
    "section": "conclusion",
    "text": "conclusion\nMoCo Êèê‰æõ‰∫Ü‰∏ÄÁßçÂØπÊØîÂ≠¶‰π†ÁöÑÊ°ÜÊû∂ÔºåÂú®ËÉΩÂ§ü‰ΩøÂ≠óÂÖ∏Â∞ΩÂèØËÉΩÂ§ßÁöÑÂêåÊó∂ÔºåËøòÂèØ‰ª•‰øùËØÅË°®ÂæÅÁ©∫Èó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËøôÊòØ‰∏ÄÁßçÈùûÂ∏∏ÁÆÄÊ¥ÅËÄåÊúâÊïàÁöÑÊñπÊ≥ï„ÄÇ"
  },
  {
    "objectID": "posts/programming/vim_tech.html",
    "href": "posts/programming/vim_tech.html",
    "title": "vim techs",
    "section": "",
    "text": "add key mapping yourself\nsave below as xxx.vim in /path/to/nerdtree/nerdtree_plugin/xxx.vim\nyy for abs path of current node, yr for rel path\n```{vimscript}\ncall NERDTreeAddKeyMap({\n        \\ 'key': 'yy',\n        \\ 'callback': 'NERDTreeYankFullPath',\n        \\ 'quickhelpText': 'put full path of current node into the default register' })\n\nfunction! NERDTreeYankFullPath()\n    let n = g:NERDTreeFileNode.GetSelected()\n    if n != {}\n        call setreg('\"', n.path.str())\n    endif\n    call nerdtree#echo(\"Node full path yanked!\")\nendfunction\n\ncall NERDTreeAddKeyMap({\n        \\ 'key': 'yr',\n        \\ 'callback': 'NERDTreeYankRelativePath',\n        \\ 'quickhelpText': 'put relative path of current node into the default register' })\n\n\nfunction! NERDTreeYankRelativePath()\n    let n = g:NERDTreeFileNode.GetSelected()\n    if n != {}\n        call setreg('\"', fnamemodify(n.path.str(), ':.'))\n    endif\n    call nerdtree#echo(\"Node relative path yanked!\")\nendfunction\n```"
  },
  {
    "objectID": "posts/programming/vim_tech.html#add-sth.-surround",
    "href": "posts/programming/vim_tech.html#add-sth.-surround",
    "title": "vim techs",
    "section": "add sth. surround",
    "text": "add sth. surround\nys takes a motion or text object, and then the char you want to put surround with\nlike ysiw\" surr current word with \""
  },
  {
    "objectID": "posts/programming/pytorch-notes.html",
    "href": "posts/programming/pytorch-notes.html",
    "title": "PyTorch Notes",
    "section": "",
    "text": "x.numel() number of elements in x\nbroadcast mechanism\nY = Y + X would allocate a new space for the result of Y + X. Y then is the reference to this new memory. Y += X or Y[:] = Y + X is better. (id(Y) will be unchanged.)\nfloat() or int() converts a scalar tensor to a standard number, similar to x.item()\nA * B element-wise product.\nA_sum = A.sum(axis=1, keepdims=True) will keep the number of axis. A / A_sum will enable broadcast mechanism.\nA.cumsum(axis=0) accumulated sum.\nlinear algebra (matrix, vector) multiplication\n\ndot product: torch.dot(x, y) only accept 1-D tensor.\nmatrix-vector multiplication: torch.mv(A, x) matrix and vector. x is 1-D vector. All in column vectors.\nmatrix-matrix multiplication: torch.mm(A, B).\n\ntorch.distributions\n\nmultinomial.Multinomial(n, probs).sample(N, )"
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#gadget-functions-pandas",
    "href": "posts/programming/pytorch-notes.html#gadget-functions-pandas",
    "title": "PyTorch Notes",
    "section": "gadget functions (pandas)",
    "text": "gadget functions (pandas)\n\ndata.fillna(data.mean())\npd.get_dummies(data, dummy_na=True) Convert categorical variable into dummy/indicator variables. Works for str."
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#gadget-functions-d2l",
    "href": "posts/programming/pytorch-notes.html#gadget-functions-d2l",
    "title": "PyTorch Notes",
    "section": "gadget functions (d2l)",
    "text": "gadget functions (d2l)\n\nd2l.plot example\n\nx = np.arange(0, 3, 0.1) d2l.plot(x, [x ** 2, 2 * x - 1], 'x', 'f(x)', legend=['f(x)', 'Tangent'])"
  },
  {
    "objectID": "posts/programming/pytorch-notes.html#plot",
    "href": "posts/programming/pytorch-notes.html#plot",
    "title": "PyTorch Notes",
    "section": "plot",
    "text": "plot\n\nexample\n\nplt.plot(y, label=(\"label\"))\nplt.axhline(y=0, color='black', linestyle='dashed')\nplt.gca().set_xlabel('x label')\nplt.gca().set_ylabel('y label')\nplt.legend()"
  },
  {
    "objectID": "posts/esl/ESL-02.html",
    "href": "posts/esl/ESL-02.html",
    "title": "unsupervised model related",
    "section": "",
    "text": "quick notes while reading unsupervised model related part in ESL"
  },
  {
    "objectID": "posts/esl/ESL-02.html#knn",
    "href": "posts/esl/ESL-02.html#knn",
    "title": "unsupervised model related",
    "section": "KNN:",
    "text": "KNN:\n\ntangent distance\nInvariant Metrics and Tangent Distance (Simard et al., 1993)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#k-means",
    "href": "posts/esl/ESL-02.html#k-means",
    "title": "unsupervised model related",
    "section": "K-means:",
    "text": "K-means:\n\nfind clusters,\nrecompute the cluster centers\n\n\nK-means for classification\n\napply K-means for each class, using R prototypes per class (K in total)\nassign a label for each prototypes (K x R in total)\nclassify new x to the nearest prototype"
  },
  {
    "objectID": "posts/esl/ESL-02.html#learning-vector-quantization",
    "href": "posts/esl/ESL-02.html#learning-vector-quantization",
    "title": "unsupervised model related",
    "section": "Learning Vector Quantization",
    "text": "Learning Vector Quantization\n\ncons of vanilla K-means classification:\n\nother samples of different class won‚Äôt involve in the position of other class\n\nfor a random training sample, move the closest prototype a bit (towards or away)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#gaussian-mixture",
    "href": "posts/esl/ESL-02.html#gaussian-mixture",
    "title": "unsupervised model related",
    "section": "Gaussian mixture",
    "text": "Gaussian mixture\n\nassign weight for each cluster (gaussian density)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#discriminant-adaptive-nearest-neighbor-dann",
    "href": "posts/esl/ESL-02.html#discriminant-adaptive-nearest-neighbor-dann",
    "title": "unsupervised model related",
    "section": "discriminant adaptive nearest-neighbor (DANN)",
    "text": "discriminant adaptive nearest-neighbor (DANN)\n\n\\(\\varSigma=W^{-1/2}[B^*+\\epsilon I]W^{-1/2}\\)\n\\(\\varSigma^{1/2} (x-x_0)=\\textrm{circle}\\)\n\\(W\\) normalization, spheres the data\n\\(B\\) stretch, assign larger weights to the directions with larger covariance."
  },
  {
    "objectID": "posts/esl/ESL-02.html#market-basket-analysis",
    "href": "posts/esl/ESL-02.html#market-basket-analysis",
    "title": "unsupervised model related",
    "section": "Market Basket Analysis",
    "text": "Market Basket Analysis\n\nThe Apriori Algorithm\ndiscover association rules with high support values and confidence"
  },
  {
    "objectID": "posts/esl/ESL-02.html#unsupervised-as-supervised-learning",
    "href": "posts/esl/ESL-02.html#unsupervised-as-supervised-learning",
    "title": "unsupervised model related",
    "section": "Unsupervised as Supervised Learning",
    "text": "Unsupervised as Supervised Learning\n\nreference density function"
  },
  {
    "objectID": "posts/esl/ESL-02.html#generalized-association-rules-not-understand",
    "href": "posts/esl/ESL-02.html#generalized-association-rules-not-understand",
    "title": "unsupervised model related",
    "section": "Generalized Association Rules (NOT UNDERSTAND)",
    "text": "Generalized Association Rules (NOT UNDERSTAND)"
  },
  {
    "objectID": "posts/esl/ESL-02.html#clustering-analysis",
    "href": "posts/esl/ESL-02.html#clustering-analysis",
    "title": "unsupervised model related",
    "section": "Clustering Analysis",
    "text": "Clustering Analysis\n\nDissimilarity Based on Attributes (dissimilarity defined individually on each attributes)\n\nQuantitative variables:\n\n\\(\\ell(\\textrm{abs}(x_i-x_j))\\) \\(\\rightarrow\\) squared-error loss, identity (just \\(\\textrm{abs}\\))\n\n\n\n\ncorreltation\n\nOrdinal variables\nCategorical variables\n\n\n\nObject Dissimilarity (combining the p-individual attribute dissimilarities)\n\nweighted average"
  },
  {
    "objectID": "posts/esl/ESL-01.html",
    "href": "posts/esl/ESL-01.html",
    "title": "The elements of statistical learning 01",
    "section": "",
    "text": "committee from a bunch of weak learners\\(G_m\\)(slightly better than rand)\n\\[\nG(x)=sign\\left(\\sum_{m=1}^Ma_mG_m(x)\\right)\n\\]\none generic method is forward-stagewise method where you compute one model \\(G_m\\) and its correspd weight \\(a_m\\) at a time (min \\(L(y_i, f_m(x_i)+\\beta G_m(x_i))\\)).\nif using MSE as the \\(L\\) loss, each time we are seeking for a model \\(\\beta G\\) that fit the residual.\n\n\niteratively fit \\(G_m\\) on a weighted dataset.\nmethod derived by using exp loss instead of the common mse ..\n\\[e^{-y_if(x_i)}\\]\nsuppose we consider scaled model (\\(range f=\\{-1,1\\}\\))\nat stage m\nwe want to opt the following\n\\[\n\\min_{(a,G)}\\sum_{i=1}^Nexp\\left(-y_i(f_{m-1}(x_i)+aG(x_i))\\right)\\\\\n=\\sum w_iexp(-y_iaG(x_i))\n\\]\nbasically using forward-stagewise method, if we fix \\(a\\)\n\\[\\begin{align}\n&exp(a)\\sum_{y_i\\neq G(x_i)}w_i+exp(-a)\\sum_{y_i=G(x_i)}w_i\\\\\n=&exp(a)\\sum_i^N w_i - exp(a)\\sum_{y_i=G(x_i)}w_i+\\dots\\\\\n=&(exp(-a)-exp(a))\\sum_{y_i=G(x_i)} w_i + exp(a)\\sum_i^Nw_i\\\\\n=&A\\sum_i^Nw_i[y_i\\neq G(x_i)]+\\dots\n\\end{align}\\]\nso actually, we are minimizing a weighted dataset using 0-1 loss\nwith this new solved \\(G\\), we can then solve for \\(a\\)\n\\[\\begin{align}\n&\\frac{d \\sum w_iexp(-aG(x_i))}{da}\\\\\n=&-\\sum w_iy_iG(x_i)exp(-ay_iG(x_i))\\\\\n=&-(exp(-a)+exp(a))\\sum_{y_i=G(x_i)} w_i + exp(a)\\sum_i^Nw_i=0\n\\end{align}\\]\nin short, AdaBoost.M1 is directly abtained from forward-stagewise method. With exp loss, we can get this neat sol.\nfurther, since \\(f_m=f_{m-1}+G_m\\), the weights \\(w_i\\) can be calculated iteratively.\n\n\n\n\\(f^\\star=arg \\min_fE_{Y|x}(e^{-Yf(x)})=\\frac{1}{2}log\\frac{Pr(Y=1|x)}{Pr(Y=-1|x)}\\), it est one half the log odds of \\(Pr(Y=1|x)\\), and \\(Pr(Y=1|x)=\\frac{1}{1+e^{-2f}}\\)\nlet \\(p(x)=Pr(Y=1|x)=\\frac{1}{1+e^{-2f}}\\) and \\(Y^\\prime =(Y+1)/2\\), we can see that cross-entropy and exp loss are actually est the same population.\n\nce: \\(-l(Y,f(x))=log(1+e^{-2Yf(x)})=Y^\\prime logp(x)+(1-Y^\\prime)log(1-p(x))\\), (f is softmaxed before output)\nexp: \\(e^{-Yf}\\)\nsame Pr, and f\n\n\n\n\n\n\nhere robustness means being disturbed less by outlier samples.\n\n\n\nsquared-error -> \\(f(x)=\\mathbf{E}(Y|x)\\), more focus on obs with large absolute residuals during fitting process, far less robust, bad on long-tailed error distrb, gross outliers\nabsolute loss -> median\nHuber loss. \\([y-f(x)]^2 \\text{ for abs residual } \\leq \\delta \\text{ and } 2\\delta|y-f(x)|-\\delta^2 \\text{ otherwise }\\)\n\n\n\n\nloss and robustness on regression prob\n\n\n\n\n\nwe consider two-class classification problem\nin regression problem, \\(y-f(x)\\) is considered as the margin\nin classif.., \\(yf(x)\\) plays the same role. where \\(y\\in\\{-1,1\\}\\)\n\n\n\nLoss functions for two-class classification.\n\n\n\n\n\nsquared-error, and exp loss are not robust, but give rise to simple boosting algorithms."
  },
  {
    "objectID": "posts/esl/ESL-01.html#specific-boosting-example",
    "href": "posts/esl/ESL-01.html#specific-boosting-example",
    "title": "The elements of statistical learning 01",
    "section": "specific boosting example",
    "text": "specific boosting example\n\nboosting tree\nregion \\(R_j,\\, j=1,\\dots,J\\)\n\\(x\\in R_j\\mathbb{R}ightarrow f(x)=y_j\\)"
  },
  {
    "objectID": "posts/calculus/calculus-02.html",
    "href": "posts/calculus/calculus-02.html",
    "title": "calculus review 02",
    "section": "",
    "text": "can be used for computing ranks, solving systems of linear equations, etc.\nGiven \\(\\mathbf{A}_{m\\times n}\\mathbf{x}=\\mathbf{b}\\)\n\\([\\mathbf{A}\\mid\\mathbf{b}]\\) row reduces to \\([\\tilde{\\mathbf{A}}\\mid\\tilde{\\mathbf{b}}]\\), determine the case of the solutions.\n\n\n\n\n\n\nextend\n\n\n\nsquare matrix can be approximated by invertible matrices (replace zeros in the diagonal entries of the row reduced matrix with \\(1/n\\)).\n\n\n\n\nthe dim of the image\nrow rank = column rank"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#vector-space",
    "href": "posts/calculus/calculus-02.html#vector-space",
    "title": "calculus review 02",
    "section": "Vector Space",
    "text": "Vector Space\n\n‚Äúcontrete to abstract‚Äù function \\(\\Phi_{v}\\)\nA linear transformation from contrete \\(\\mathbb{R}^n\\) coordinate representation to abstract vector space \\(V\\)\n\\[\n\\Phi_{\\{\\mathbf{v}\\}}(\\mathbf{a})=\\Phi_{\\{\\mathbf{v}\\}}\\left(\\begin{matrix}a_1\\\\\\vdots\\\\a_n\\end{matrix}\\right)=a_1\\mathbf{v_1}+\\cdots+a_n\\mathbf{v_n},\n\\]\nwhere \\(\\{\\mathbf{v}\\}\\) is the set of base vectors of \\(V\\).\n\n\nChange of bases \\(P_{\\{\\mathbf{v}'\\}\\to \\{\\mathbf{v}\\}}\\)\n\\[[P_{\\{\\mathbf{v}'\\}\\to\\{\\mathbf{v}\\}}]\\mathbf{a}=\\mathbf{b},\\]\nwhere \\(\\sum_ia_i\\mathbf{v}_i'=\\sum_ib_i\\mathbf{v}_i\\)\nIn other words,\n\\[P_{\\{\\mathbf{v}'\\}\\to \\{\\mathbf{v}\\}}=\\Phi_{\\{\\mathbf{v}'\\}}^{-1}\\Phi_{\\{\\mathbf{v}\\}}\\]"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#eigenvectors-and-eigenvalues",
    "href": "posts/calculus/calculus-02.html#eigenvectors-and-eigenvalues",
    "title": "calculus review 02",
    "section": "Eigenvectors and Eigenvalues",
    "text": "Eigenvectors and Eigenvalues\neigenvectors with different eigenvalues are linearly independent.\ndiagnolization"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#newtons-method",
    "href": "posts/calculus/calculus-02.html#newtons-method",
    "title": "calculus review 02",
    "section": "Newton‚Äôs Method",
    "text": "Newton‚Äôs Method\nwhy talking about Newton‚Äôs Method?\na convenient and practical way to introduce implicit function and inverse function\n\ndef\ntarget: solve for a solution \\(f(x)=0\\)\nstart at \\(a_0\\)\niterate with \\(a_i=a_{i-1}-[Df(a_{i-1})]^{-1}f(a_{i-1})\\)\n\n\n\n\n\n\nNote\n\n\n\nnotes: since \\(f(x)\\approx f(a)+[Df(a)](x-a)\\), letting \\(f(x)=0\\) will give us the next guess.\n\n\nThere is actually a sufficient condition to ensure the convergence.\n\n\nKantorovich‚Äôs theorem\nlet \\(a_0\\in\\mathbb{R}^n\\), \\(U\\): open neighborhood of \\(a_0\\) in \\(\\mathbb{R}^n\\), \\(f:U\\to\\mathbb{R}^n\\)\ndef \\(h_0=-[Df(a_0)]^{-1}f(a_0)\\), \\(a_1=a_0+h_0\\), \\(U_1=B_{|h_0|}(a_1)\\), \\(M\\) as the Lipschitz ratio of \\(Df(x)\\) in \\(U_1\\)\ntheorem: if \\(|f(a_0)||[Df(a_0)]^{-1}|^2M\\le\\frac{1}{2}\\), then \\(f(x)=0\\) has a unique solution in the closed ball \\(\\overline{U_1}\\) and Newton‚Äôs Method converges to it with initial guess \\(a_0\\).\n\n\n\n\n\n\nNote\n\n\n\nnotes 1: in short, \\(a_1\\) is the next guess\nnotes 2: a valid value of Lipschitz ratio\nIf \\(|D_kD_jf_i(x)|\\le c_{i,j,k}\\)\nthen\n\\[\\left(\\sum_{1\\le i,j,k\\le n}c_{i,j,k}^2\\right)^{1/2}\\]\nis valid\n\n\n\n\nproposition\ndef \\(U_0=\\{x\\mid |x-a_0|\\lt 2|h_0|\\}\\)\nIf \\(Df\\) satisfy Lipschitz condition in \\(U_0\\), then \\(f(x)=0\\) has a unique solution in \\(\\overline{U_0}\\) and Newton‚Äôs Method converges to it.\n\n\n\n\n\n\nNote\n\n\n\nIf the product is strictly less than \\(1/2\\), then Newton‚Äôs Method superconverges.\nIf \\(|h_n|\\le\\frac{1}{c}\\), for \\(c\\) some constant depend on \\(f\\), then\n\\[|h_{n+m}|\\le \\frac{1}{c}\\cdot\\left(\\frac{1}{2}\\right)^{2m},\\]\nwhere \\(h_i=a_{i+1}-a_{i}\\).\n\n\n\n\nstronger version\nreplace all lengths of matrices with norms.\n\n\n\n\n\n\nNote\n\n\n\nnorm of a matrix \\(A\\)\n\\[\\lVert A\\rVert=\\sup_{|x|=1} |A\\mathbf{x}|\\]\neasy to check: \\(\\lVert A\\rVert\\le |A|\\)"
  },
  {
    "objectID": "posts/calculus/calculus-02.html#inverse-function-theorem",
    "href": "posts/calculus/calculus-02.html#inverse-function-theorem",
    "title": "calculus review 02",
    "section": "inverse function theorem",
    "text": "inverse function theorem\n\\(f\\) is continously differentiable\n\\(Df\\) is invertible at \\(x_0\\)\nthen \\(f\\) is locally invertible, with differentiable inverse in some neighborhood of \\(f(x_0)\\)\n\n\n\n\n\n\nNote\n\n\n\nthe vigorous version is too lengthy, I am not gonna put it here.\nAs you might expect, the locality can actually be quantified by Kantorovich‚Äôs theorem."
  },
  {
    "objectID": "posts/calculus/calculus-02.html#implicit-function-theorem",
    "href": "posts/calculus/calculus-02.html#implicit-function-theorem",
    "title": "calculus review 02",
    "section": "implicit function theorem",
    "text": "implicit function theorem\n\\(U\\): a open subset of \\(\\mathbb{R}^{n+m}\\)\n\\(\\mathbf{F}:U\\to\\mathbb{R}^n\\) a \\(C^1\\) mapping s.t. \\(\\mathbf{F}(\\mathbf{c})=0\\) and \\([D\\mathbf{F}(c)]\\) is onto\nthen the system of linear equations \\(D\\mathbf{F}(\\mathbf{x})=0\\) has n pivotal vars and m nonpivotal vars. there exists a neighborhood of \\(\\mathbf{c}\\) where \\(\\mathbf{F}(\\mathbf{c})=0\\) implicitly defines the n passive vars as a function \\(\\mathbf{g}\\) of the m active vars.\n\n\n\n\n\n\nNote\n\n\n\nnotes 1: \\(\\mathbf{F}\\) behaves similar to \\(D\\mathbf{F}\\) locally.\nin a normal system of linear equations \\(\\mathbf{Ax}=0\\), where \\(\\mathbf{x}\\in\\mathbb{R}^{m+n}\\), \\(\\textrm{rank }\\mathbf{A}=n\\), the dimension of the kernel space of \\(\\mathbf{A}\\) is m which corresponds to the m active vars (the base of the kernel).\nnotes 2: again, the rigorous version is a little lengthy.\n\n\nnot like the existence theorem, which only claims the existence of the inverse function or the implicit function, using Newton‚Äôs Method gives us a more practical way to find the function and a more quantified result."
  },
  {
    "objectID": "posts/calculus/calculus-03.html",
    "href": "posts/calculus/calculus-03.html",
    "title": "calculus review 03",
    "section": "",
    "text": "New stuff (to me)\n\n\n\n\n\n\n\n\nNote\n\n\n\nmanifold: a subset \\(M\\subset \\mathbb{R}^n\\) is a smooth \\(k\\)-dimensional manifold\nif locally it is the graph of a \\(C^1\\) mapping \\(f\\) expressing \\(n-k\\) variables as functions of the other \\(k\\) variables\n\n\nThe definition seems highly related to the implicit function theorem\nTherefore, we can quickly catch the spirit below\nlet \\(\\mathbf{F}:U\\rightarrow \\mathbb{R}^{n-k}\\) be a \\(C^1\\) mapping, where \\(U\\subset\\mathbb{R}^n\\).\n\n\n\n\n\n\nNote\n\n\n\n\nSome subset \\(M\\) of the domain \\(U\\) is a \\(k\\)-dimensional manifold if \\(\\mathbf{F}(z)=0\\) and \\([D\\mathbf{F}(z)]\\) is onto for every \\(z\\in M\\).\n(converse) if \\(M\\) is a smooth \\(k\\)-dimensional manifold, then for every \\(z\\in M\\), there exists \\(\\mathbf{F}\\) s.t., \\([D\\mathbf{F}(z)]\\) is onto and \\(\\mathbf{F}(y)=0\\) with a neighborhood of \\(z\\) as the domain\n\n\n\n\\(\\star\\) it says we can virtually claim a manifold by \\(\\mathbf{F}(z)=0\\)\nintuitively, the definition of manifold should not depend on the coordinates\n\n\n\n\n\n\nNote\n\n\n\n\\(k\\)-dimensional manifold \\(M\\subset\\mathbb{R}^m\\), \\(f\\) is a mapping with some properties 11¬† \\(f:U\\rightarrow\\mathbb{R}^m\\) where \\(U\\) is an open subset of \\(\\mathbb{R}^n\\) and \\([Df(x)]\\) is surjective at \\(\\forall x\\in f^{-1}(M)\\)\nthen \\(f^{-1}(M)\\) is a submanifold of \\(\\mathbb{R}^n\\) of dimension \\(k+n-m\\)\n\n\nas a corollary, manifolds are independent of coordinates if \\(f\\) is an affine transformation\n\n\n\nparametrization is useful for analysing manifolds since taking a manifolds as domain directly is rather cumbersome\n\n\n\n\n\n\nNote\n\n\n\na parametrization of a \\(k\\)-dimensional manifold \\(M\\subset\\mathbb{R}^n\\) is a mapping \\(\\gamma:U\\subset\\mathbb{R}^k\\rightarrow M\\), s.t.,\n\n\\(U\\) is open\n\\(\\gamma\\) is \\(C^1\\), one to one, and onto \\(M\\)\n\\([D\\gamma(u)]\\) is one to one for every \\(u\\in U\\)\n\n\n\n\nas a comparison, linear algebra and differential calculus have mucn in common\n\nrow reduction \\(\\leftrightarrow\\) newton‚Äôs method\nkernels \\(\\leftrightarrow\\) defining manifolds by equations ( e.g., \\(f(x)=0\\) )\nimages \\(\\leftrightarrow\\) defining manifolds by a parametrization\n\n\n\n\n\ntangent space is similar to the linear approximation of a function \\(f\\), but it is used on a manifold.\n\n(derivative) replace a function \\(f\\) locally by a linear map\n(tangent space) replace a manifold locally by a linear space\n\nfor a manifold \\(M=\\left\\{\\left(\\begin{matrix} x\\\\y \\end{matrix}\\right)\\in\\mathbb{R}^n\\mid f(x)=y\\right\\}\\)\nfix \\(z_0\\in M\\).\nthe change of \\(z\\) should be approximated by a linear mapping\n\\[\ny-y_0=[Df(x_0)](x-x_0)\n\\]\nin short \\(\\Delta y=[Df(x_0)]\\Delta x\\)\n\n\n\n\n\n\nNote\n\n\n\nthe graph of \\([Df(x_0)]\\) is the tangent space, which is denoted as \\(T_{z_0}M\\)\nthe linear approximation to the graph is the graph of the linear approximation\n\n\n\n\n\nif you have an equation form \\(F(x)=0\\) of the manifold, then you are lucky.\n\n\n\n\n\n\nNote\n\n\n\nthe kernel space of the derivative of \\(F\\) is the tangent space.\n\\[\nT_{z_0}M=\\ker [DF(z_0)]\n\\]\n\n\nagain, this will certainly remind you the implicit function theorem, where we also claim the derivative of the implicit function\nhere is another approach to get the tangent space\n\n\n\n\n\n\nNote\n\n\n\nLet \\(U\\subset\\mathbb{R}^k\\) be open, and let \\(\\gamma:U\\rightarrow\\mathbb{R}^n\\) be a parametrization of manifold \\(M\\), then\n\\[\nT_{\\gamma(u)}M=\\textrm{img}[D\\gamma(u)].\n\\]\n\n\nit might look strange at first ‚Äî why taking the image instead of the kernel like above.\nquick calculation:\n\\[\n[D(F\\circ\\gamma)(u)]=[DF(\\gamma(u))]\\circ[D\\gamma(u)]=[0]\n\\]\nderivative is not quite like what we think as the origin function\nhere \\(\\textrm{img}[D\\gamma(u)]\\) and \\(\\textrm{img}[DF(\\gamma(u))]\\) are orthogonal complementary subspaces\n\n\n\nTBD :)"
  },
  {
    "objectID": "posts/calculus/calculus-01.html",
    "href": "posts/calculus/calculus-01.html",
    "title": "calculus review 01",
    "section": "",
    "text": "linear transformations\nmeasure of matrices: \\(\\left \\rVert A\\right \\lVert_F^2\\)\ntriangle inq in matrices: \\(\\rVert AB \\lVert\\le \\rVert A\\lVert\\rVert B\\lVert\\)\nthe neighborhood of \\(x\\), exist an open ball init\nclosure \\(\\bar{A}\\), the smallest close-set that contains A\ninterior \\(\\mathring{A}\\), the largest open set that A contains\nthe boundary of subset, \\(\\partial A\\)\n\nconvergence of a sequence, in terms of coordinates\nlimits of multivariable functions: continuity is preserved under dot product operation\ncontinuity: the preimage of a neighborhood of \\(f(x)\\) is also a neighborhood of x\nuniform continuity: linear transformations are uniform continuity\nconvergence of the sum of series (vectors): absolute(norm in vector cases) convergence implies convergence\ncomplex exponentials\n\ncomplex exponential series converges: \\(e^z=1+z+\\frac{z^2}{2!}+\\dots=\\sum_{k=0}^\\infty \\frac{z^k}{k!}\\), since the absolute series converges\n\neuler formular: \\(e^{it}=cost+isint\\)\ngeometric series of matrices:\n\n\\(S=I+A+A^2+\\dots\\) converges to \\((1-A)^{-1}\\) if \\(\\lVert A \\rVert \\lt 1\\)\nthe set of invertible n by n matrices is open\n\nbounded: subset \\(X\\subset \\mathbb{R}^n\\) is bounded if it is contained in some ball centered at the origin\ncompact: nonempty subset \\(C\\subset \\mathbb{R}^n\\) is compact if it is closed and bounded"
  },
  {
    "objectID": "posts/calculus/calculus-01.html#important-theorem",
    "href": "posts/calculus/calculus-01.html#important-theorem",
    "title": "calculus review 01",
    "section": "important theorem",
    "text": "important theorem\n\nBolzano-Weierstrass theorem: a compact set C contains a seq, then that seq has a convergent sub seq whose limit is in C\na continuous function on a compact set achieves its minimum and maximum\nthe mean value theorem\ncontinuity on a compact set is uniform continuity\nthe fundamental theorem of algebra"
  },
  {
    "objectID": "posts/calculus/calculus-01.html#differentiable",
    "href": "posts/calculus/calculus-01.html#differentiable",
    "title": "calculus review 01",
    "section": "differentiable",
    "text": "differentiable\n\ndef\ndifferentiable means \\(f\\) can be well approximated at \\(x=a\\) by a linear transformation\nlet \\(f:\\mathcal{U}\\to\\mathbb{R}^m\\) where \\(\\mathcal{U}\\) is a open subset of \\(\\mathbb{R}^n\\)\n\\(D_if(a)\\) means the partial derivative on the i-th value. It is an m by 1 vertical vector\nwe can treat it like the mapped element from \\(e_i\\)\n\\(Df(a)\\) is an m by n matrix where we stack \\(D_i\\)‚Äôs in a row\nthe precise def of the derivative is as below:\n\\(Df(a)\\) is a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\), s.t.\n\\[\n\\lim_{h\\to 0}\\frac{1}{|h|}\\left( (f(a+h)-f(a)) - [Df(a)]h\\right )=0\n\\]\n\n\nJacobian matrix\nJacobian matrix now fits this definition and don‚Äôt need for any transpose.\nnote the existence of Jacobian matrix does not mean differentiable, except that we let \\(f\\) to be continuously differentiable. I won‚Äôt present any examples of why and how. We just need to focus on the \\(C^1\\) functions from now on :\n\n\nfor some cases where the domain is not the subspace of Euclidiean space, we may need to use the definition of derivative instead of the Jacobian matrix.\ne.g.¬†1\n\\(\\mathcal{S}:Mat(n,n)\\to Mat(n,n)\\) given by \\(S(A)=A^2\\)\n\\[[\\mathbf{D}S(A)]H=AH+HA\\]\ne.g.¬†2\n\\(f(A)=A^{-1}\\) defined on invertible n by n matrices\n\\[[\\mathbf{D}f(A)]H=-A^{-1}HA^{-1}\\]\n\n\n\nabout gradient\n\\[\ngrad\\ f(a)=\\vec{\\nabla} f(a)=[D_1f(a),\\dots, D_nf(a)]^\\intercal\n\\]\nwhere \\(f:\\mathbb{R}^n\\to \\mathbb{R}\\)\nFrom my POV, gradient is the solution of the fastest increasing direction at some point \\(a\\) and it happens to be the (transpose of) derivative. note the transpose has its meaning, it makes the grad lying in the domain.\nfurthermore, from how we get the solution, we should have an inner-product defined on the domain space, which is not the case in the def of derivative.\n\n\ndirectional derivative\ndirectional derivative of \\(f\\) at \\(a\\) in the derection \\(\\vec{v}\\)\n\\[\n[Df(a)]\\vec{v}\n\\]\nthis is how linear transformation works.\n\n\nrules\n\nconstant function: 0\nlinear function: itself\n\\(\\mathbf{f}=(f_1,\\dots,f_m)^\\intercal\\) then\n\\[[Df(a)]\\vec{v}=\\left[[Df_1(a)]\\vec{v},\\cdots,[Df_m(a)]\\vec{v}\\right]^\\intercal\\]\n\\([D(f+g)(a)]=[Df(a)]+[Dg(a)]\\)\n\\(f:U\\to \\mathbb{R}\\) and \\(\\mathbf{g}:U\\to \\mathbb{R}^m\\)\n\\[\n[D(f\\mathbf{g})]\\vec{v}=f(a) [D\\mathbf{g}(a)]\\vec{v}+([Df(a)]\\vec{v})\\mathbf{g}(a)\n\\]\n\\[\\left[D\\left(\\frac{\\mathbf{g}}{f}\\right)(a)\\right]\\vec{v}=\\frac{[D\\mathbf{g}(a)]\\vec{v}}{f(a)}-\\frac{([Df(a)]\\vec{v})g(a)}{f^2(a)}\\]\ndot product \\(\\mathbf{f}:U\\to \\mathbb{R}^m\\)\n\\[\n[D(\\mathbf{f}\\cdot\\mathbf{g})]\\vec{v}=\\mathbf{f}(a)\\cdot[D\\mathbf{g}(a)]\\vec{v}+([D\\mathbf{f}(a)]\\vec{v})\\cdot\\mathbf{g}(a)\n\\]\nchain rule\n\\[[D(f\\circ g)(a)]=[Df(g(a))][Dg(a)]\\]\n\n\nref\n\nVector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html",
    "href": "posts/parallel-algorithm/pa-04.html",
    "title": "parallel algorithm course 04",
    "section": "",
    "text": "synchronization has costs\nsimple list traversal\ncompute all p in advance"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#task",
    "href": "posts/parallel-algorithm/pa-04.html#task",
    "title": "parallel algorithm course 04",
    "section": "task",
    "text": "task\n#pragma omp parallel {\n    // not often used\n    #pragma omp task\n    foo();\n\n    #pragma omp barrier\n\n    // more common case\n    #pragma omp single {\n        #pragma omp task\n        bar();\n    }\n}\nexample (with bug)\nint fib(int n) {\n    int x, y; // private\n    if (n < 2) return n;\n\n    #pragma omp task \n    // need `shared(x)`. by default, task copy the whole stack, \n    // x is in different address\n    x = fib(n-1);\n\n    #pragma omp task // need `shared(y)`, default is private in task\n    y = fib(n-2);\n\n    #pragma omp taskwait\n\n    return x+y; // wrong here!!!\n}\n\n#pragma omp parallel single\nfib(n);\ntwo important points\n\nbarrier\ndata env\n\nexample 2 (with bug)\nlist ml;\nElement *e;\n#pragma omp parallel\n#pragma omp single\n{\n    for (e=ml->first; e; e=e->next)\n    #pragma omp task // need firstprivate(e)\n        process(e); // has data racing\n}\nreturn to first example\n#pragma omp parallel\n{\n    #pragma omp single\n    {\n        node* p = head;\n        while(p) {\n            #pragma omp task firstprivate(p)\n                process(p);\n            p = p->next;\n        }\n    }\n}\n\n\n\nrunning schema"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#merge-sort",
    "href": "posts/parallel-algorithm/pa-04.html#merge-sort",
    "title": "parallel algorithm course 04",
    "section": "merge sort",
    "text": "merge sort\ndirectly using parallel\n\\(T_1=2T_1(\\frac{n}{2})+O(n)=O(n\\cdot log(n)\\)\n\\(T_\\infty=T_\\infty(\\frac{n}{2})+O(n)=O(n)\\)\n\nuse a differernt merge method\nparallel merge\nsay here we have two sorted seqs (\\(\\le\\))\n\\(A=a_1,\\dots,a_n\\) and \\(B=b_1,\\dots,b_n\\)\nlet \\(t=a_{\\lceil \\frac{n}{2}\\rceil}\\), split A and B into sub arr \\(A_1,A_2,B_1,B_2\\) s.t. \\(A_1\\le t, B_1\\le t\\) and \\(A_2\\gt t, B_2\\gt t\\)\nso seq \\(A_1,B_1,A_2,B_2\\) is semi-merged by t, then go on with \\(merge(A_1,B_1)\\) and \\(merge(A_2,B_2)\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-04.html#random-number-generating-problem",
    "href": "posts/parallel-algorithm/pa-04.html#random-number-generating-problem",
    "title": "parallel algorithm course 04",
    "section": "random number generating problem",
    "text": "random number generating problem\n\nproblem\nexample: approximate \\(\\pi\\) using Monto Carlo\nseed(SEED);\n#pragma omp parallel for private(x,y) reduction (+:n_in_circle)\nfor(int i = 0; i < num_trials; i++) {\n    x = random();\n    y = random();\n\n    if (x*x + y*y <= r*r) n_in_circle ++;\n}\n\npi = 4.0 * (1.0 * n_in_circle / num_trials);\n\nLinear Congruential Generator(LCG) (has data racing problem)\nint random_last = 0;\n// need \n// #pragma omp threadprivate(random_last)\n\nint random() {\n    random_next = (MULTIPLIER * random_last + ADDEND) % PMOD;\n    random_last = random_next;\n\n    return random_next;\n}\nparameter: (one possible parameter suite) MULTIPLIER=1366, ADDEND=150889, PMOD=714025\nhas a period (less than PMOD)\n\nsingle thread and multi-thread has different accuracy (multi-thread has poor performance)\ndata racing on var random_last\n\n\nsolution 1\nadd #pragma omp threadprivate(random_last) under int random_last=0\nhas a local copy of random_last when creating a thread\nsimilar to firstprivate\nproblem: still poor than single thread version\ndifferent random range in threads maybe overlap\n\n\nsolution 2\nLeap Frog method\ngenerate different seeds for each thread\nsituation for ADDEND=0\n#pragma omp single\n{\n    nthreads = ...\n    iseed = PMOD / MULTIPLIER;\n    pseed[0] = iseed;\n    mult_n = MULTIPLIER;\n    for (int i = 0; i < nthreads; i++) {\n        iseed = (unsigned long long) ((MULTIPLIER * iseed) % PMOD);\n        pseed[i] = iseed;\n        mult_n = (mult_n * MULTIPLIER) % PMOD;\n    }\n    id = ...\n    random_last = (unsigned long long) pseed[id];\n}"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-02.html",
    "href": "posts/parallel-algorithm/PA-02.html",
    "title": "parallel algorithm course 02",
    "section": "",
    "text": "problem \\(T_1\\)\n-> reduce dependency ->\ntasks \\(T_\\infin\\)\nwork sharing\nblocks\nsynchronization (better to avoid)\nfalse sharing"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-02.html#sync",
    "href": "posts/parallel-algorithm/PA-02.html#sync",
    "title": "parallel algorithm course 02",
    "section": "sync",
    "text": "sync\nSingle program multiple data\nseq\nfor(i=0;i<N;i++) a[i] = a[i] + b[i];\nomp par\n#pragma omp parallel\n{\n    id = omp_get_thread_num();\n    Nthrds\n    i_start = id * N / Nthrds\n    i_end = (id+1) * N / Nthrds\n    if (id==Nthrds - 1) iend=N\n    for (i=istart;i<iend;i++) ...    \n}\nshorter (but slower)\n#pragma omp parallel {\n    #pragma omp for\n        for () ...\n}\n\n// or equiv\n\n#pragma omp parallel for\n...\n\nloop worksharing constructs\nschedule clause\n\nstatic: least work for scheduling (done at compiling)\ndynamic (complex scheduling at run-time)\n\n\nreduction\nop should be associative\n+,*,-,min,max\n#pragma omp parallel for reduction (+: sum)\n    for (i=0; i < num_steps; i++)\n        x = ...\n        sum = sum + x\nslower than SPMD critical\n\nomp for loop has barrier by default\nhas implicit barrier\nuse nowait to cancel it\n\nmaster construct\n#pragma omp master\nonly master thread will execute\nno barrier for other threads\n\n#pragma omp single\nonly one thread will execute, has barrier\nuse nowait to cancel barrier\n\nsections\n#pragma omp sections\n{\n    #pragma omp section\n        thread A\n    #pragma omp section\n        thread B\n}\nA, B will execute in parallel\nhas barrier at the end of sections\n\nlock routines\nomp_init_lock();\nomp_set_lock(), omp_unset_lock()\nomp_destroy_lock()\ne.g.¬†array\nif use critical on a[i], then the whole array a will be lock\n#bins \\(\\gg\\) #threads\nlow prob of false sharing, conficts are rare, use locks\n\nOMP refernce card\n\nomp_set_dynamic()\n\nenv vars\nOMP_NUM_THREADS\n\nheap: shared\nstack: private\nint tmp=0; // in heap\n#pragma omp parallel for private(tmp)\n    for (int i; i < N; i++) {\n        tmp += j; // not initialized, local copy ( in stack )\n    }\nprint(tmp); // 0 here\nint tmp=999; // in heap\n#pragma omp parallel for firstprivate(tmp)\n    for ()\n        tmp += j; // has initialized with 1, local copy ( in stack ), still private\n\nprintf(tmp) // 999\nnot use often: lastprivate\nint tmp=0; // in heap\n#pragma omp parallel for lastprivate(j)\n    for ()\n        tmp = j; // copy the last j to tmp\n    \nprint(tmp); // print the last j"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html",
    "href": "posts/parallel-algorithm/pa-07.html",
    "title": "parallel algorithm course 07",
    "section": "",
    "text": "tensor core\nmath limited, memory limited"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#gpu",
    "href": "posts/parallel-algorithm/pa-07.html#gpu",
    "title": "parallel algorithm course 07",
    "section": "GPU",
    "text": "GPU\nL1 caches are independent, L2 caches are shared\nexpected: \\(T_\\text{math}\\ge T_\\text{mem}\\)\n\\[\n\\frac{\\#\\text{opt}}{\\text{math bandwidth}}\\ge \\frac{\\#\\text{bytes}}{\\text{mem bandwidth}}\n\\]\nArithmetic intensity \\(\\frac{\\#\\text{opt}}{\\#\\text{bytes}}\\)\nmath limited when arithmetic intensity \\(\\ge\\frac{\\text{math band}}{\\text{mem band}}\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#key-ideas",
    "href": "posts/parallel-algorithm/pa-07.html#key-ideas",
    "title": "parallel algorithm course 07",
    "section": "key ideas",
    "text": "key ideas\nsplit, independent ++\nuser cache\nTensor Cores: accelerate dot-product, matrix multiplies, size should better be a mulitple of 128 bits"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#how-to-do-split",
    "href": "posts/parallel-algorithm/pa-07.html#how-to-do-split",
    "title": "parallel algorithm course 07",
    "section": "how to do split",
    "text": "how to do split\n\nkey: tile\nmultiplication, read a tile at a time to L1 cache\nquantization: size should matches the hardware\nrunning time is stagewise increasing as size increases\nsmall batch \\(\\to\\) low usage of GPU"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-07.html#post-order-reversal",
    "href": "posts/parallel-algorithm/pa-07.html#post-order-reversal",
    "title": "parallel algorithm course 07",
    "section": "post-order reversal",
    "text": "post-order reversal\nDFS, post-order\nhow to parallelize\nhint: list-ranking\n\ntree \\(\\to\\) list, Eular Tour, add points\ninitialization, upward point -1, downward point +1\nlist scan\nlist \\(\\to\\) tree\n\nadjacency list"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-01.html",
    "href": "posts/parallel-algorithm/PA-01.html",
    "title": "parallel algorithm course 01",
    "section": "",
    "text": "gcc -fopenmp\nomp_num_thread(int): request\nmulti-data\nomp_get_thread_num, get id\n\nSMP: equal-time access cost, in theory\nNUMA: different .., practically\n\nFalse Sharing\ncache line\ntwo processors may have access to the same region, repeat many useless write back\n\nSynchronization, to avoid data racing, false sharing (avoid global array) d barrier\n#pragma omp barrier\ncritical\nonly one thread can enter (often cost cheap), mutual exclusion, avoid data racing\n(software support)\n#pragma omp critical\natomic\nonly support (hardware support)\n\nx binopr= expr\nx++, ++x, x‚Äì, ‚Äìx\n\n#pragma omp atomic"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html",
    "href": "posts/parallel-algorithm/pa-06.html",
    "title": "parallel algorithm course 06",
    "section": "",
    "text": "reiview\n\\(T_1(n)=O(n)\\)\n\\(T_\\infty(n)=O(log^k(n))\\)\nmodel: shared memory model"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#core-alg",
    "href": "posts/parallel-algorithm/pa-06.html#core-alg",
    "title": "parallel algorithm course 06",
    "section": "core alg",
    "text": "core alg\n\nmap \\(\\text{n}\\to \\text{n}\\)\nreduce \\(\\text{n} \\to 1\\)\nscan: compact (based on scan)\nlist ranking\nsort alg\nradix sort, sample sort"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#goal",
    "href": "posts/parallel-algorithm/pa-06.html#goal",
    "title": "parallel algorithm course 06",
    "section": "goal",
    "text": "goal\n\ncomplexity optimal\ndependency ‚Äì\nsymetric ‚Äì (symetry: hard to find independency) using rand (find independent set) or odd-even"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#thinking",
    "href": "posts/parallel-algorithm/pa-06.html#thinking",
    "title": "parallel algorithm course 06",
    "section": "thinking",
    "text": "thinking\n\ncase to general\nforward-backward\nbit-wise thinking\nmatrix thinking"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#pattern",
    "href": "posts/parallel-algorithm/pa-06.html#pattern",
    "title": "parallel algorithm course 06",
    "section": "pattern",
    "text": "pattern\n\ndivide and conquer\nindependent set\nrandom\njump"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-06.html#openmp",
    "href": "posts/parallel-algorithm/pa-06.html#openmp",
    "title": "parallel algorithm course 06",
    "section": "OpenMP",
    "text": "OpenMP\n\npipeline: serial \\(\\to\\) independent \\(\\to\\) group \\(\\to\\) parallel\ndata racing, using syncronization\nPseudo random number generator"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html",
    "href": "posts/parallel-algorithm/PA-08.html",
    "title": "parallel algorithm course 08",
    "section": "",
    "text": "openACC\nGPU not sharable\nopenacc.org\nopenacc best programming\nthree levels\ndata"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html#optimize",
    "href": "posts/parallel-algorithm/PA-08.html#optimize",
    "title": "parallel algorithm course 08",
    "section": "optimize",
    "text": "optimize\nUnified virtual memory, auto, slow\nremove the ‚Äúmanaged‚Äù suboption to the -ta compiler\ndata clauses\nallocate with copy\n\ncopyin {list}: cpu \\(\\to\\) gpu\ncopyout {list}: gpu \\(\\to\\) cpu\ncopy {list}\n\nallocate without copy\n\ncreate {list}\ndelete {list}\n\nonly copy\n\npresent {list}: only copy if present when copyin\n\npragma\n#pragam acc data copyin (a[0:nelem]) copyout(b[s/4:3*s/4])\nrun functions in each devices, and do copyin, copyout in the functions\nslow\nwe can copyin and copyout together\n\ndata\n\n#pragma acc data\n{\n    #pragma acc parallel loop\n    ...\n    #pragma acc parallel loop\n    ...\n}\ncopyin and copyout in each loops are done in one clause\n\nenter data: copyin, create\nexit data: copyout, delete, finalize(destroy a variable not regarding its reference)\nclass M{\n    M() {\n        v = new double[N];\n        #pragma acc enter data create(v)\n    }\n    ~M() {\n        #pragma acc exit data delete(v)\n        {\n            free(v);\n        }\n    }\n}\n\nrunning with explicit data management\ndata racing update between cpu and gpu\ndo_somthing_on_device();\n\n#pragma acc update host(a)\n\ndo_something_on_host();\n\n#pragma acc update device(a)\n\nreduce IO between cpu and GPU"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-08.html#optimize-loops",
    "href": "posts/parallel-algorithm/PA-08.html#optimize-loops",
    "title": "parallel algorithm course 08",
    "section": "optimize loops",
    "text": "optimize loops\noptimize matvec loops\ndefault iteration is 128 for GPU\nthree level\nGang, workers, vectors\none SMP \\(\\to\\) 64 cores, 16 warps(1 warp = 32 threads), at most 2048 threads\n\nset vector size as 32 k\n\nworker = warp \n#pragma acc parallel vector_length(32)\n#pragma acc loop gang worker\nfor (int i = 0; i < n; i++)\n    #pragma acc loop vector // vector loop\n    for (int j = 0; j < i; j++)\nvector loop: has to specify vector_length or num_workers\ncollapse clause\n#pragma acc parallel loop collapse(2)\nfor (i)\n    for (j)\nequiv\n#pragma acc parallel loop\nfor (ij)\n\nthe tile clause\noperate on smaller blocks of the operation to exploit data locality\n\nstride-1 memory access\ncontinus access\na[0][i][j]"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-10.html",
    "href": "posts/parallel-algorithm/pa-10.html",
    "title": "parallel algorithm course 10",
    "section": "",
    "text": "final exam\nreview\n\nfundamental\n\n\nDAG\nwork-span analysis\n\nideal: \\(T_1(n)=T_s(n)\\), \\(T_\\infty(n)=\\Theta(\\log^k(n))\\)\n\nonly consider shared data memory model\n\nalgorithmÔºö\n\nmap: n->n\n\nassociative\n\nreduce:n -> 1\nscan: n -> n\n\ncompact: n -> m\n\nlist ranking: n -> n, input becomes linked list, GRAPH, BFS, DFS, find independent\nsorting, sample sort\n\ngoal:\n\noptimal complexity\nindependent ++, sync ‚Äì, don‚Äôt use sync too much, has cost, e.g.¬†print message during parallel\ne.g.¬†histgram, lock in each bucket\nsymetrics ‚Äì, repeat occurance, break symetric, use randomize\n\nthinking:\n\ncase to general\nforward - backward, cover gap between input and output\nbit-wise, radix sort\nmatrix-wise, compute fibonacci sequence, matrix operation\n\npattern\n\ndivide & conquer\nindep set\nrandom\njump\nEulur tour\nBag of pennants (conform divide and conquer patterm)\n\n\nopenacc\nprofile driven programming\n\nanalyse\nparallel\noptimize\n\nincremental programming\nCPU -> GPU -> Unified memory -> data parallel -> loop -> blocking\nblocking:\n\nIO and compute parallel\nmulti-device\n\n\nAnalyse\n\nobserve profile\nbottle-neck, for loop\nimprove \\(\\frac{\\text{compute}}{\\text{mem}}\\) access ratio\n\nParallel\n\nalgorithm\n\nOptimize\n\ndata movement, manual management\nlarge matrix, sometimes we don‚Äôt need load all the elements\nloop mapping: tell compile how loop maps the level of parallel, e.g.¬†vetor length\nvector 32*n -> n warps\nblocking, ÊµÅÊ∞¥Á∫øÂπ∂Ë°å\n\ninput compute output\ncompute -> futher split -> multi device\n\ntile <- GPU 2-level cache usage\ncollapse\nmemory access pattern, keep continuous access\n\nGPU hardware\n\n\n\nSMP\n\n\n\n\nÁÆÄÁ≠îÈ¢ò X 2\nÁÆóÊ≥ïÈ¢ò\n\nÊúü‰∏≠Ââç X 1\nÊúü‰∏≠Âêé X 1\n\nÁºñÁ®ãÈ¢ò X 1"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html",
    "href": "posts/parallel-algorithm/PA-03.html",
    "title": "parallel algorithm course 03",
    "section": "",
    "text": "recursion\nscan\nmap, reduce, scan"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html#compact-alg",
    "href": "posts/parallel-algorithm/PA-03.html#compact-alg",
    "title": "parallel algorithm course 03",
    "section": "compact alg",
    "text": "compact alg\npick up using an indication arr\nA = 0 1 2 3 4 5 6 F = 0 1 1 0 0 1 0\noutput = 1 2 5\nkey to do parallel, acknowledge the target position in advance\nscan F\nidx = 0 1 2 2 2 3 3"
  },
  {
    "objectID": "posts/parallel-algorithm/PA-03.html#fragment-sum",
    "href": "posts/parallel-algorithm/PA-03.html#fragment-sum",
    "title": "parallel algorithm course 03",
    "section": "fragment sum",
    "text": "fragment sum\n\napplication\n\nF zero flag put in the front, one flag in the back\ntwo stage compact, \\(F^{-1}\\) first, then use \\(F\\)\n\nscatter\n\n\n\nwhat is scatter\n\n\nmake it into a large sparse matrix and then reduce sum\n\nbinary base sort\nRadix sort.\nfor from lowbit to highbit\n\nscan\n\\(T_1(n,b)=O(bn)\\)\n\\(T_\\infty(n,b)=(blogn)\\)\nif from highbit to lowbit, need fragment scan\nbit-wise thinking."
  },
  {
    "objectID": "posts/parallel-algorithm/pa-05.html",
    "href": "posts/parallel-algorithm/pa-05.html",
    "title": "parallel algorithm course 05",
    "section": "",
    "text": "omp task\narray pool\nuse array to simulate linked list\n\n\nfind prev using next\n\n\n\nfind index\nprev\njump\nrepeat:\npar-for i from 0 to n-1:\n    if NULL: continue\n    if prev[i] != empty:\n        rank[i] += rank[prev[i]] # initial all 1 except head is 0\n        prev[i] = prev[prev[i]] # use 2 copies to avoid data racing\n\\(T_1(n)=O(n\\cdot log(n))\\)\n\\(T_\\infty(n)=O(log(n))\\)\nWyllie‚Äôs Alg: jump\n\n\n\nsimilar: binary lifting"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-05.html#improve-wyllies-alg",
    "href": "posts/parallel-algorithm/pa-05.html#improve-wyllies-alg",
    "title": "parallel algorithm course 05",
    "section": "improve Wyllie‚Äôs Alg",
    "text": "improve Wyllie‚Äôs Alg\n\\(O(n\\cdot log(n))\\rightarrow O(n)\\)\n\nstep 1: shrink \\(n\\rightarrow m\\)\nstep 2: call Wyllie‚Äôs Alg \\(O(m\\cdot log(m))\\)\nstep 3: restore \\(m\\rightarrow n\\)\n\ntarget \\(m\\rightarrow \\frac{n}{log(n)}\\)\nindependent set \\(\\forall i \\in I,N(i) \\notin I\\)\nsymetry break\n\nrandom flip coin, keep or remove\nresolve confict\n\n# producing independent set\n\npar-for i=1..n\n    F[i] = RND(0 or 1)\npar-for i=1..n\n    if F[i]=F[N[i]]=1: # data racing. need 2 copies\n        F[i] = 0\n\\(T_1(n)=O(n)\\)\n\\(T_\\infty(n)=O(1)\\)\n\\(\\mathbb{E}|I|=\\frac{n}{4}\\)\n\\([n]/I\\rightarrow \\frac{3}{4}n\\) repeat \\(loglog(n)\\) times \\(\\frac{n}{log(n)}\\)\n\n\ninitialized with 1 except head\nrepeatedly remove an independent set \\(S\\) til #rest=\\(\\frac{n}{logn}\\) (change next[prev[i]] to next[i], add counter[i] to counter[next[i]] forall \\(i\\in S\\))\ncall Wyllie‚Äôs Alg\nreverse\n\n\nquick sort: choose 1 pivot, sample 1\nv.s.\nsample sort: randomly choose \\(p-1\\) pivots, sample \\(k\\cdot p+1\\)\ncompact operator"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html",
    "href": "posts/parallel-algorithm/pa-09.html",
    "title": "parallel algorithm course 09",
    "section": "",
    "text": "array\nlinked list\ntree (to list)\ngraph (today)\n\nparallel BFS\n\nBFS(G[V,E],S)\n\nD[V] = inf\nD[S] = 0\n\nF = {s}\n\nwhile F not empty do {\n    v = pop(F)\n    for (v, w) in E do {\n        if D[w] = inf {\n            D[w] = D[v] + 1\n            push(F, w)\n        }\n    }\n}\nkey idea: layering \\(F_l\\)\nneed a data structure:\n\nallow repetitive occurance of elements\nunordered\nfast search union and split"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html#bag-of-pennats",
    "href": "posts/parallel-algorithm/pa-09.html#bag-of-pennats",
    "title": "parallel algorithm course 09",
    "section": "Bag of pennats",
    "text": "Bag of pennats\n\nA pennant: \\(2^k\\) nodes\nadd an extra root to a balance binary tree\nunion, split\na bag of pennants can be mapped to a binary number\ninsert -> add one (\\(O(\\log n)\\))\ncombine two bags of pennants -> binary number adding (\\(O(\\log n)\\))\nsplit -> require balance (\\(O(\\log n)\\))\n\n\n\n\nbag of pennant\n\n\n// calculate F(l+1) given F(l)\ninitialize(F(l+1), an empty pennant)\n\nfun process-level (G, F(l), F(l+1), D)\nif |F(l)| > threshold {\n    Fa, Fb = split(F(l))\n    spam {\n        process-level (G, Fa, F(l+1), D)\n        process-level (G, Fb, F(l+1), D)\n    }   \n    sync\n} else {\n    foreach v in F(l)\n    par-for (v, w) in E do {\n        if D[w] = inf {\n            D[w] = D[v] + 1\n            insert(F(l+1), w)\n        }\n    }\n}"
  },
  {
    "objectID": "posts/parallel-algorithm/pa-09.html#extend",
    "href": "posts/parallel-algorithm/pa-09.html#extend",
    "title": "parallel algorithm course 09",
    "section": "extend",
    "text": "extend\nmatrix A, B, C\n\\(A\\times B=C\\)\ntile\nconsider \\(A^k\\)\nsave \\(A\\), \\(A^T\\)"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html",
    "href": "posts/parallel-algorithm/solution.html",
    "title": "midterm exam",
    "section": "",
    "text": "work complexity ÊòØÂΩìÂ§ÑÁêÜÂô®‰∏™Êï∞‰∏∫ 1 Êó∂ÔºåÂÆåÊàêÁÆóÊ≥ïÊâÄÈúÄÁöÑÂü∫Á°ÄÊìç‰ΩúÁöÑ‰∏™Êï∞„ÄÇspan complexity ÊòØÂú®Êï∞ÊçÆ‰æùËµñ DAG Âõæ‰∏≠ÁöÑÊúÄÈïøË∑ØÂæÑÁöÑÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºåÂç≥ÂΩìÂ§ÑÁêÜÂô®‰∏™Êï∞‰∏∫ \\(\\infty\\) Êó∂ÔºåÂü∫Á°ÄÊìç‰ΩúÁöÑ‰∏™Êï∞„ÄÇ\n\n\n\nBrent‚Äôs Theorem:\n\\[\n\\max\\left\\{T_\\infty,\\frac{T_1}{p}\\right\\} \\le T_p \\le \\frac{T_1-T_\\infty}{p} + T_\\infty.\n\\]\nProof: Â∑¶‰æß‰∏çÁ≠âÂºèÊòæÁÑ∂„ÄÇ\nÂØπ‰∫éÂè≥‰æß‰∏çÁ≠âÂºèÔºå\nÂØπ DAG ÂõæËøõË°åÂàÜÂ±ÇÔºåËÆ∞Á¨¨ \\(i\\) Â±ÇÁöÑÊìç‰ΩúÊï∞‰∏∫ \\(m_i\\)ÔºåÂÖ± \\(n\\) Â±ÇÔºåÂàôÊúâ \\[\nT_1=\\sum_{i=1}^n m_i\n\\]\nÂØπÁ¨¨ \\(i\\) Â±ÇËøõË°åÂπ∂Ë°åÔºåÁî±‰∫éËøô \\(m_i\\) ‰∏™Êìç‰ΩúÁã¨Á´ãÔºåÊâÄ‰ª•\n\\[\nT_p^i=\\left \\lceil \\frac{m_i}{p} \\right \\rceil \\le \\frac{m_i-1}{p} + 1.\n\\]\nÊïÖ\n\\[\nT_p=\\sum_{i=1}^{n}T_p^i\\le \\sum_{i=1}^{n}\\left ( \\frac{m_i-1}{p}+1\\right)=\\frac{T_1-T_\\infty}{p} + T_\\infty\\blacksquare\n\\]\n\n\n\n\nÂèØÁªìÂêàÊÄßÂç≥Ôºå\\(\\forall a,b,c\\)ÔºåÊúâ \\(\\text{oper}(\\text{oper}(a,b),c)=\\text{oper}(a, \\text{oper}(b,c))\\)„ÄÇ\nÁî±‰∫éÂπ∂Ë°åÁ®ãÂ∫èÂú® scheduling Êó∂ÔºåÊØè‰∏™ task ÁöÑÂºÄÂßãÊó∂Èó¥ÂíåËøêË°åÊó∂Èó¥Êàë‰ª¨Êó†Ê≥ïÈ¢ÑÁü•ÔºåÂØπ‰∫é‰∏ÄÁ≥ªÂàóÁöÑÊìç‰ΩúÊó†Ê≥ï‰øùËØÅËøêÁÆóÁöÑÈ°∫Â∫èÔºå‰ªéËÄå‰øùËØÅÊ≠£Á°ÆÊÄß„ÄÇËã•Êìç‰ΩúÊª°Ë∂≥ÂèØÁªìÂêàÊÄßÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî® merge Á≠âÁÆóÊ≥ï‰øùËØÅÊìç‰ΩúÊï∞È°∫Â∫èÁöÑÂêåÊó∂ËøõË°åÂπ∂Ë°åÂ§ÑÁêÜÔºõËã•Êìç‰ΩúÂèàÊª°Ë∂≥ÂèØ‰∫§Êç¢ÊÄßÔºåÂàôÂèØ‰ª•ÁÆÄÂçïÁöÑ‰ΩøÁî® par-for Á≠âËøõË°åÂπ∂Ë°åÔºå‰∏îËÉΩ‰øùËØÅÊ≠£Á°ÆÊÄß„ÄÇ\n\n\n\n\ndifference\n\nscan Êìç‰ΩúÁöÑÂØπË±°ÊòØ‰∏Ä‰∏™Êï∞ÁªÑÔºåÊØè‰∏Ä‰∏™‰ΩçÁΩÆÁöÑÁ≠îÊ°àÊâÄÂØπÂ∫îÁöÑ‰æùËµñÂÖÉÁ¥†ÊòØ predefined„ÄÇlist ranking Êìç‰ΩúÂØπË±°ÊòØ linked list Êàñ array poolÔºåÊØè‰∏™‰ΩçÁΩÆÁöÑÁ≠îÊ°àÊâÄ‰æùËµñÁöÑÂÖÉÁ¥†ÊòØÊú™Áü•ÁöÑÔºåÂøÖÈ°ªÁªèËøáÊüêÁßç‰∏≤Ë°åÁöÑÊü•ÊâæÊù•ÂæóÂà∞ÂØπÂ∫îÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ\nscan ËÆ°ÁÆóÁöÑÊòØ \\(a_i=\\bigoplus_{k=1}^i\\text{arr}_k\\)Ôºålist ranking ËÆ°ÁÆóÁöÑÊòØÊØè‰∏™ element ÁöÑ rankingÔºåÂÖ∂‰∏≠ \\(\\bigoplus\\) ÊòØÂèØÁªìÂêà„ÄÅÂèØ‰∫§Êç¢ÁöÑ‰∫åÂÖÉËøêÁÆó„ÄÇ\nlist ranking ‰ΩøÁî® jump ÁöÑÊÄùÊÉ≥Êù•Ëß£ÂÜ≥ÔºåËÄå scan Ê≤°Êúâ„ÄÇ\n\nin common\n\n‰∏§ÁßçÁÆóÊ≥ïËß£ÂÜ≥ÁöÑÈóÆÈ¢òÈÉΩÊòØ \\(n\\to n\\)„ÄÇ\n\n\n\n\n\ndifference\n\nquicksort Âè™ËøõË°å‰∏ÄÊ¨°ÈááÊ†∑ÔºåÈÄâÊã©‰∏Ä‰∏™ pivot elementÔºåÂ∞ÜÊï∞ÁªÑÂàÜÊàê‰∏§ÊÆµÔºåËÄå samplesort ‰ºöËøõË°å \\(k\\cdot (p-1)\\) Ê¨°ÈááÊ†∑ÔºåÈÄâÂá∫ p-1 ‰∏™ pivot elementÔºåÂ∞ÜÊï∞ÁªÑÂàÜÊàê p ÊÆµ„ÄÇ\n\nin common\n\nÈÉΩÊòØÂü∫‰∫é divide and conquer ÊÄùÊÉ≥ÔºåËß£ÂÜ≥ sorting ÈóÆÈ¢ò"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#openmp-1",
    "href": "posts/parallel-algorithm/solution.html#openmp-1",
    "title": "midterm exam",
    "section": "OpenMP 1",
    "text": "OpenMP 1\n\nQ6 False sharing and how to avoid\n\nÁî±‰∫é cache ÁöÑÂ≠òÂú®ÔºåÂú®‰∏Ä‰∏™ cpu ‰∏äÁöÑ cache ‰∏≠ÂèØËÉΩ‰ºöÊúâÂÖ∂‰ªñ cpu ÊâÄÂ§ÑÁêÜÁöÑ‰∏éÂΩìÂâç cpu Êó†ÂÖ≥ÁöÑÊï∞ÊçÆ (‰∏ÄËà¨ÊòØÊï∞ÁªÑÂÖÉÁ¥†)ÔºåÂΩìÈÇ£‰∏ÄÈÉ®ÂàÜÁöÑÊï∞ÊçÆÂèëÁîüÊõ¥ÊîπÊó∂ÔºåÂΩìÂâçÂ§ÑÁêÜÂô®ÁöÑ cache ÈúÄË¶ÅËøõË°åÂêåÊ≠•Ôºå‰ªéËÄåËøõË°å‰∏ÄÁ≥ªÂàóÁöÑÊ≤°ÊúâÊÑè‰πâÁöÑ IO Êìç‰ΩúÔºå‰ªéËÄå‰∏•ÈáçÂΩ±ÂìçÂπ∂Ë°åÊÄßËÉΩ„ÄÇ\nÈÅøÂÖç false sharing\n\nÂØπ‰∫éÊï∞ÁªÑÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÂ¢ûÂä† paddingÔºåÂº∫Âà∂Â∞Ü‰∏çÂêå cpu cache ÈáåÁöÑÂÖÉÁ¥†ÈöîÁ¶ªÂºÄ„ÄÇ\nÈÅøÂÖç‰ΩøÁî®ÂÖ®Â±ÄÊï∞ÁªÑÔºåÂØπÊØè‰∏™ task ‰ΩøÁî® private ÁöÑÊï∞ÁªÑÔºå‰ΩøÁî® synchronization pragma Êù•ËøõË°åÂêåÊ≠•„ÄÇ\n\n\n\n\nQ7 Why do synchronizations slow down your program\n\nËã•ÊúâÂæàÂ§öÁ∫øÁ®ãÂêåÊó∂ÈúÄË¶ÅËøõË°åÂêåÊ≠•Êìç‰ΩúÔºåËøõÂÖ•ÊéíÈòüÁ≠âÂæÖÁä∂ÊÄÅÔºåÈÇ£‰πàÊ≠§Êó∂Âπ∂Ë°åÈÄÄÂåñÊàê‰∫Ü‰∏≤Ë°åÔºå‰ºöÈÄ†ÊàêÊÄßËÉΩÁöÑ‰∏ãÈôç„ÄÇ\nÂêåÊ≠•Êìç‰ΩúÊú¨Ë∫´‰ºöÊúâ‰∏ÄÂÆöÁöÑÂºÄÈîÄÔºåÂ¶ÇÊéíÈòüË∞ÉÂ∫¶„ÄÅÂ§ÑÁêÜÂô®‰πãÈó¥ÁöÑÈÄö‰ø°Á≠â„ÄÇ\n\n\n\nQ8 When should we use critical, atomic or lock pragma\n\ncritical: ÂΩìÈúÄË¶ÅÈÅøÂÖç data racingÔºåÂè™ÂÖÅËÆ∏‰∏Ä‰∏™Á∫øÁ®ãËøõÂÖ•ÔºåËÄå‰∏îÂÖ≥ÈîÆÂå∫ËÆ°ÁÆóÁöÑÂºÄÈîÄ‰∏çÊòØÈùûÂ∏∏Â§ßÁöÑÊó∂ÂÄô‰ΩøÁî®„ÄÇ\natomic: Âè™ËÉΩÂú®‰∏Ä‰∫õÁÆÄÂçïÁöÑËøêÁÆó‰∏ä‰ΩøÁî®(ÈúÄË¶ÅÁ°¨‰ª∂ÊîØÊåÅ)ÔºåÂ¶Ç a++ Á≠â„ÄÇ\nlock: ÂΩìÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑÂêåÊ≠•ÊàñÊúâÂ§çÊùÇÁöÑÈîÅÁöÑÂÖ≥Á≥ªÊó∂‰ΩøÁî®„ÄÇ\n\n\n\nQ9 Give four reasons why the iterations are not in a static order for a parallel for loop\n\nparallel for ÁöÑ schedule ÂèÇÊï∞ÈªòËÆ§ÊòØ staticÔºåÂç≥Âú®Âæ™ÁéØÂºÄÂßãÂâç‰∏∫ÊØè‰∏™ thread ÂàÜÈÖçÂ§ßËá¥Á≠âÈáè‰∏™ iterations1„ÄÇÂõ†Ê≠§ÔºåÁî±‰∫é‰∏çÂêå thread ÊâßË°åÁöÑ iterations chuncks ‰πãÈó¥Âú®ËøêË°åÊó∂ÊòØÁã¨Á´ãÁöÑÔºåÂÆÉ‰ª¨ÊâßË°åÁöÑÈ°∫Â∫èÊòØ‰∏çÁ°ÆÂÆöÁöÑ„ÄÇ\nÁ®ãÂ∫èÁöÑËøêË°å‰ºöÂõ†‰∏∫ cpu ÁöÑË∞ÉÂ∫¶Á≠âÂéüÂõ†ÔºåÂØºËá¥ËøêË°åÁöÑÊó∂Èó¥‰∏çÂõ∫ÂÆö„ÄÇ‰ªéËÄåÂΩ±ÂìçÊâßË°åÁöÑÈ°∫Â∫è„ÄÇ\nËÆ°ÁÆóÊú∫Âπ∂‰∏çËÉΩ‰øùËØÅÊèê‰æõÂõ∫ÂÆöÊï∞ÈáèÁöÑ cpuÔºåÂõ†Ê≠§ OpenMP ÂØπÂπ∂Ë°å tasks ÁöÑË∞ÉÂ∫¶ÂíåÁÆ°ÁêÜ‰πü‰∏çÂõ∫ÂÆö„ÄÇ\nÁî±‰∫é OpenMP ÂíåÊìç‰ΩúÁ≥ªÁªüÁöÑË∞ÉÂ∫¶Ôºåthreads ÁöÑÊâßË°åÈ°∫Â∫èÂπ∂‰∏ç‰∏ÄÂÆöÊòØ‰ªñ‰ª¨ÁöÑÂàõÂª∫È°∫Â∫èÔºå‰∏î‰∏çÂõ∫ÂÆö„ÄÇ\n\n1¬†Openmp.org. [Online]. Available: https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-1.pdf. [Accessed: 22-Apr-2022].\n\n\n\n\n\nsolution\n\n\n\n(Compile, Run) \\(\\times\\) (Hardware, Software)\n\n\n\n\nQ10 Safe way to generate random numbers in parallel\n‰ΩøÁî®Âπ∂Ë°åÁâà LCG ÁÆóÊ≥ïÁîüÊàêÈöèÊú∫Êï∞„ÄÇ\n\n// Âéü LCG ÁÆóÊ≥ï\nint random() {\n    random_next = (MULTIPLIER * random_last + ADDEND) % PMOD;\n    random_last = random_next;\n\n    return random_next;\n}\n\nÂú®Âéü LCG ÁÆóÊ≥ïÂü∫Á°Ä‰∏äÔºå‰ΩøÁî® Leap Frog Method„ÄÇËÆ∞Áî± LCG ÁÆóÊ≥ïÁîüÊàêÁöÑÈöèÊú∫Êï∞Â∫èÂàó‰∏∫ \\(a_0,a_1,\\dots\\)ÔºåÊúâ k ‰∏™ thread„ÄÇÂèñ \\(a_0,a_1,\\dots,a_{k-1}\\) ‰Ωú‰∏∫ÊØè‰∏™ thread ÈöèÊú∫Êï∞Â∫èÂàóÁöÑÁßçÂ≠êÔºåÂπ∂‰øÆÊîπ MULTIPLIER Âíå ADDENDÔºå‰ΩøÂæó \\(\\text{next}(a_i)=a_{i+k}\\)ÔºåÂç≥ÊØè‰∏™ thread \\(t\\) ÊâÄ‰ΩøÁî®ÁöÑÈöèÊú∫Êï∞Â∫èÂàó‰∏∫ \\(a_{t::k}\\)„ÄÇ"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-2-comment-deletion",
    "href": "posts/parallel-algorithm/solution.html#algorithm-2-comment-deletion",
    "title": "midterm exam",
    "section": "Algorithm 2: comment deletion",
    "text": "Algorithm 2: comment deletion\nsolution: scan, fragsum"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-3-missing-element",
    "href": "posts/parallel-algorithm/solution.html#algorithm-3-missing-element",
    "title": "midterm exam",
    "section": "Algorithm 3: missing element",
    "text": "Algorithm 3: missing element\nsolution: xor"
  },
  {
    "objectID": "posts/parallel-algorithm/solution.html#algorithm-4-knn",
    "href": "posts/parallel-algorithm/solution.html#algorithm-4-knn",
    "title": "midterm exam",
    "section": "Algorithm 4: KNN",
    "text": "Algorithm 4: KNN\nsolution:\nfor i: 0 to N\n    for j: i+1 to N\n        d[i,j]=dis[A[i],A[j]] // heap[i].add, heap[j].add, lock\ndesigned schedule to balance work flow in each thread\nlock"
  },
  {
    "objectID": "posts/test/example.html",
    "href": "posts/test/example.html",
    "title": "my first post",
    "section": "",
    "text": "This is a test for the \\(\\LaTeX\\) rendering.11¬†this is a note\nIt successes with a little effort !!!\nsee Section¬†1. and Equation¬†1."
  },
  {
    "objectID": "posts/test/example.html#sec-two",
    "href": "posts/test/example.html#sec-two",
    "title": "my first post",
    "section": "equations and callouts",
    "text": "equations and callouts\nnothing\nEinstein‚Äôs theory of special relatively that expresses the equivalence of mass and energy:\n\\[\nE = mc^{2}\n\\tag{1}\\]\n\n\n\n\n\n\nmy note\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important."
  },
  {
    "objectID": "posts/test/example.html#cite-at-the-margin",
    "href": "posts/test/example.html#cite-at-the-margin",
    "title": "my first post",
    "section": "cite at the margin",
    "text": "cite at the margin\nColorbars indicate the quantitative extent of image data. Placing in a figure is non-trivial because room needs to be made for them. The simplest case is just attaching a colorbar to each axes:2.2¬†See the Matplotlib Gallery to explore colorbars further"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi,\nI‚Äôm an undergraduate student majoring in computer science\ninterested in machine learning, rec sys, NLP and fancy stuff.\ncontact me at furyton AT outlook DOT com üôÉ"
  }
]